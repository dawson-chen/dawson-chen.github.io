+++
title = 'DeepSeek-V2技术解读'
date = 2024-0509T23:40:02+08:00
draft = false
math = true
busuanzi = true
+++

DeepSeek又带来了新的MoE模型DeepSeek-V2，总参数量2360亿，激活参数210亿，虽然离GPT4水平还差一点，但也可以说是目前开源的最强MoE模型了。并且秉承着一贯的开源精神，同时发布的技术报告也是干货满满，所以今天就来重点做一下技术方面的解读。

## 认识幻方

虽说是技术解读，但特别想聊一下幻方这家公司，也就是DeepSeek的母公司。

**初识**

第一次听到DeepSeek这家公司是我在了解scaling law的过程中，这个概念于2020年被openai提出，2022年 DeepMind Chinchilla开启了最佳实践，一直指引着大模型的发展。而Deepseek是我认为国内一众追随者中做的最认真的。在看完Deepseek LLM大模型的论文后，我就感觉到这是一家少有的非常有技术追求的公司。

出于好奇，我又去网上翻了一下这个公司的信息。2023年成立，由幻方量化独立出来的公司，专注于人工智能基础研究。炸的一听，一家量化公司出来做大模型，感觉有点不可思议，看不透这背后有什么关联。因为大模型是从NLP技术里衍生出来的产物，而量化与NLP又是毫不相干的2个领域。但当我进一步了解幻方的历史，更重要的是其创始人吴文锋的经历，又突然间释怀并多出一丝钦佩。吴文峰 2008年在浙大读人工智能专业，毕业后怀揣着人工智能改变世界的念头，开始在没有任何经验的量化领域创业。最终成立了幻方，经历了6年的发展就成为量化界的龙头企业。我对量化这个概念并不了解，但你还能想到比这更cool的创业故事吗，尤其对于那些相信技术改变世界的人来说。

更难得可贵的是，幻方始终坚持在AI上面投入，在2020年受到openai启发开始默默囤积算力，这在当时国内的科技圈里也是少之又少的远见。十年磨一剑，不忘初心，始终坚信，永远向前，这或许是幻方能成功的原因。所以当他们后来成立DeepSeek这家独立的公司来投入生成式人工智能，就是理所应当的事情了，因为这家公司的基因里始终流淌着技术和AI的追求。

**感想**

大模型引领人工智能的时代，一共有3块基石：算力，数据，人才。前面2者只是硬性条件，而真正决胜的关键在于人才，更是推进AGI进程的核心动力。这种人才究竟应该具备哪些特征呢？把重点放在人上面，我们可以找到很多技能和品质上的条件，但拥有了足够的人才，就一定可以在大模型赛道里取得领先吗？战略规划和人才管理可能更为重要，这就要取决于领导者的意愿或者说这个公司的基因，更直白的讲就是要看领导者的意志与决心。

国内大模型的发展，经历了去年百模大战，几百亿的资金投入，目前第一梯度已经接近了GPT4的水平。而2024年又被认为是AI应用的元年，各家大厂包括创业公司都开始积极的寻找应用的场景，或者与自身业务开始结合。这或许是必须的一个步骤，但从中或多或少可以嗅到投资人耐心不足的气味，而未来真正的杀手锏应用会是什么样，没有人知道。越是这个时候，那种耐下性子继续做着基础层面创新的精神，才显得难能可贵，或许也是更快接近AGI的方式。

这是我佩服幻方的一点，就像他们的大模型的名字DeepSeek深度求索一样，他们做这件事情的出发点就只是满足好奇心，恰好他们有足够的钱去投入。

## DeepSeek MoE模型

应该是在2023年12月，Mixtral刚带起一波MoE的热度之后，DeepSeek发布了他们的MoE论文和模型。起初我也以为这是一篇跟风之作，但深入研究后发现里面充满了各种干货，绝对是长时间的技术积累和沉淀的结果。

DeepSeek-V2在MoE技术上与去年12月发布的论文基本保持一致，里面最重要的是2点：fine-grained专家、共享专家。我会跳过细节，尝试去分析他们这么做的背后动机。

**Fine-grained专家**

最开始看到DeepSeek MoE的论文，就惊讶于他们的实验结果出奇的好，同样激活参数和总参数下远超正常MoE的收益。这其实主要归功于Fine-grained专家这个技术，论文里说这样可以使每个专家更聚焦于特殊的领域知识，从而带来更灵活的专家组合。



![专家激活分布](https://raw.githubusercontent.com/dawson-chen/picgo-repo/master/image-20240509074932110.png)

事实上就像mixtral这种图中展示的一样，仔细看会发现每个expert关注的token并非某个领域，而更像是某个语法层面的角色。在正常的Dense网络中，所有的知识都被存放在FFN的隐藏层空间里，不同类型的知识存在于不同的维度激活组合。而MoE中相当于把语法层面上相似的知识单独放到一个空间里，从而带来更好的区分度。

另外，从scaling law的角度去解释这样做的合理性，MoE的scaling law公式如下：

$$
log\ L(N,E)\triangleq -0.082\ log\ N - 0.108\ log\ E + 0.009\ log\ N\ log\ E + 1.104 \tag{5}
$$
解释一下其中使用到的变量：$E$ 表示expert的数量，$N$ 表示对应基底模型的参数量。

带入N=21B、E=8（DeepSeek-V2配置，不考虑fine-grained），观察loss相对于N和E的导数会发现，N的导数是-0.074，E的导数是-0.025。增加N的收益显然是更大的，但增加N会增加训练和推理成本，而fine-grained方法相当于只增加E，同时保证N和总参数量不变。

**shared expert**

MoE在刚开始出现的时候，人们认为topk>1的时候才能更好的学习到路由。而既然总是要激活多个专家，那么干脆把部分专家变成固定激活，从而学到通用的一些知识。这是shared expert的出发点，但这样做其实还有2点好处，第一是高效计算，第二是upcycling。

MoE模型在训练的时候会花费大量的时候用来做通讯，因为expert会分散到不同的设备上从而降低巨大的总参数量带来的显存消耗。一种解决思路是：在通讯核处在工作状态的时候，同时用计算核进行后续的运算，从而影藏掉部分通讯时间。shared expert的计算与MoE通讯是不依赖的，因此可以使用通讯隐藏从而比普通的MoE结构计算更高效。

shared expert在实现的时候，往往是将多个共享的expert参数合并成一个大的MLP计算。在upcycling的时候，会用Dense网络的参数初始化MoE，考虑到fine-grained专家的MLP层都比较小，所以每个expert都只能用部分MLP的参数。理想的切分方法是把Dense的不同类型信息切分到不同的专家中，以符合expert自身的需要。但实际上找到这种方法是困难的，shared expert的存在使得Dense的参数可以不切分加载进去，从而更大的保留了Dense原有的能力。至于为什么做upcycling而不是直接从头训练，很简单，因为MoE训练速度太慢。

**Device-Limited Routing**

考虑到MoE模型训练时对通讯能力的依赖，所以实践时的压力都给到了硬件和框架层面。DeepSeek V2中提出了用Device-Limited Routing来降低总的通讯压力，但是具体的实现细节我还没有完全想明白，我也提了一个issue给官方，希望后续能把这点搞清楚。

## MLA: Multi-Head Latent Attention

论文中的示意图如下：

![image-20240509083351982](https://raw.githubusercontent.com/dawson-chen/picgo-repo/master/image-20240509083351982.png)

总的理解是：MLA是一种加强版的MQA，能够实现相同的推理速度，同时带来更好的performance。下面简单介绍一下是如何做的：

1. 在MHA中，KV被映射到了和Q相同的维度里，所以在推理时需要保存大量的KV。MLA中将KV映射到较小的维度里，再映射回Q相同的维度。而因为在attention计算过程中，QK以及OV其实分别属于一个影藏的矩阵$W_{QK}$和$W_{OV}$，所以向上的映射矩阵可以被吸收进$W_Q$和$W_O$里，保证了推理过程中只需要保存较小维度的KV值。
2. 因为RoPE会给每个不同位置的token的QK向量上面施加不同的矩阵变换，这这会打乱MLA中矩阵融合的操作，DeepSeek-V2的做法是单独在一个较小维度的QK上面做RoPE。相当于在MLA里保留了一部分MHA的计算，在较小的维度里做RoPE是否对效果有影响，这点还需要进一步验证。

## 总结

以上就是对DeepSeek-V2的解读，确实是一份干货满满的技术报告，看的出来DeepSeek在用创新的方式解决问题。在非研究性机构里做创新最大的难度是需要结合场景，所以能够使用到的技术会被限制，并且需要考虑的因素也比较多。**创新往往都是自己产生的，不是刻意安排的，更不是教出来的**，另外还需要一个宽松的环境，大胆去做没有目的性的尝试。创新是高风险 昂贵的 低效的，但是很高兴看到有这样一个公司还保留着这种硬核的技术风格。


