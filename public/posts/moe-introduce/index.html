<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Moe(Mixtrue of Experts)技术调研 | DawsonChen&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="陈道一，12/14/2023
MoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。
第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。
为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。
而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。
MoE为什么会起作用？ scaling law层面的解释
Scaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。
MoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。
典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：
模型结构层面的猜想
观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。
观察2：模型越靠后的层学习越不够充分。
基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。
所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。
[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3
Expert真的是专家吗？ 考虑下面3个MoE路由配置的实验：
将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1&gt;2&gt;3，但真实的实验结论是2&gt;3&raquo;1（23.22&gt;23.27&raquo;23.99）。
对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。
如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；
以Mixtral-7B*8 MoE为例：
dim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7.">
<meta name="author" content="Dawson Chen">
<link rel="canonical" href="https://dawson-chen.github.io/posts/moe-introduce/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dawson-chen.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dawson-chen.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dawson-chen.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dawson-chen.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://dawson-chen.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://dawson-chen.github.io/posts/moe-introduce/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            
            throwOnError: false
        });
    });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>

const config = {
    startOnLoad:true,
    theme: 'forest',
    themeVariables: {
        lineColor: "#fafafa"    
    },
    flowchart: {
        useMaxWidth:false,
        htmlLabels:true
        }
};
mermaid.initialize(config);


window.onload = () => {
    window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid'));
}
</script><meta property="og:title" content="Moe(Mixtrue of Experts)技术调研" />
<meta property="og:description" content="陈道一，12/14/2023
MoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。
第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。
为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。
而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。
MoE为什么会起作用？ scaling law层面的解释
Scaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。
MoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。
典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：
模型结构层面的猜想
观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。
观察2：模型越靠后的层学习越不够充分。
基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。
所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。
[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3
Expert真的是专家吗？ 考虑下面3个MoE路由配置的实验：
将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1&gt;2&gt;3，但真实的实验结论是2&gt;3&raquo;1（23.22&gt;23.27&raquo;23.99）。
对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。
如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；
以Mixtral-7B*8 MoE为例：
dim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dawson-chen.github.io/posts/moe-introduce/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-20T23:40:17+08:00" />
<meta property="article:modified_time" content="2023-12-20T23:40:17+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Moe(Mixtrue of Experts)技术调研"/>
<meta name="twitter:description" content="陈道一，12/14/2023
MoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。
第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。
为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。
而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。
MoE为什么会起作用？ scaling law层面的解释
Scaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。
MoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。
典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：
模型结构层面的猜想
观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。
观察2：模型越靠后的层学习越不够充分。
基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。
所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。
[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3
Expert真的是专家吗？ 考虑下面3个MoE路由配置的实验：
将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1&gt;2&gt;3，但真实的实验结论是2&gt;3&raquo;1（23.22&gt;23.27&raquo;23.99）。
对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。
如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；
以Mixtral-7B*8 MoE为例：
dim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dawson-chen.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Moe(Mixtrue of Experts)技术调研",
      "item": "https://dawson-chen.github.io/posts/moe-introduce/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Moe(Mixtrue of Experts)技术调研",
  "name": "Moe(Mixtrue of Experts)技术调研",
  "description": "陈道一，12/14/2023\nMoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\n第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。\n为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。\nMoE为什么会起作用？ scaling law层面的解释\nScaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。\nMoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。\n典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：\n模型结构层面的猜想\n观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。\n观察2：模型越靠后的层学习越不够充分。\n基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。\n所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。\n[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3\nExpert真的是专家吗？ 考虑下面3个MoE路由配置的实验：\n将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1\u0026gt;2\u0026gt;3，但真实的实验结论是2\u0026gt;3\u0026raquo;1（23.22\u0026gt;23.27\u0026raquo;23.99）。\n对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。\n如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；\n以Mixtral-7B*8 MoE为例：\ndim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7.",
  "keywords": [
    
  ],
  "articleBody": " 陈道一，12/14/2023\nMoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\n第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。\n为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。\nMoE为什么会起作用？ scaling law层面的解释\nScaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。\nMoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。\n典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：\n模型结构层面的猜想\n观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。\n观察2：模型越靠后的层学习越不够充分。\n基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。\n所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。\n[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3\nExpert真的是专家吗？ 考虑下面3个MoE路由配置的实验：\n将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1\u003e2\u003e3，但真实的实验结论是2\u003e3»1（23.22\u003e23.27»23.99）。\n对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。\n如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；\n以Mixtral-7B*8 MoE为例：\ndim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7.2b 计算过程如下：\ntransformers: FFNs: 176,160,768 * 8 = 45,097,156,608 gate: 4096 * 8 = 1,048,576 MA: 4096 * (128 * 48) + 4096 * 4096= 41,943,040 LN: 4096 * 2 = 8,192 total: (176,160,768 * 8 + 41,943,040 + 8,192 + 4096 * 8) * 32 = 46,440,644,608 others: embed \u0026 output_w: 262,144,000 total: 46,440,644,608 + 262,144,000 = 46,702,788,608 = 46B active params: (176,160,768 * 2 + 41,943,040 + 8,192 + 4096 * 8) * 32 + 262,144,000 = 12,879,921,152 = 12.8B 模型训练的并行方式分为3种，DP(data parallel) / TP(tensor parallel) / PP(pipline parallel)，MoE模型在训练时可以同时使用这3种模型外，还可以加入EP(expert parallel)方式。EP的精髓就是多个模型中共享expert，2点理解：\n因为MoE每次前向中只用到一小部分expert，如果每个模型保留完整的expert，一定会导致大多数expert空闲的情况； 如果DP是8，EP是2，那么2个模型共用一套完整的experts； 训练并行设定：TP2 DP8 EP8 （megatron方案），需要显存如下：\nmodule MoE参数/单卡 Dense参数/单卡 Emb and Output $h * vocab *2 /TP$=262 144 $h * vocab *2 /TP$=262 144 experts $hffn3 * num_experts * n_layers / TP / EP$=88 080 384 $hffn3 * n_layers / TP$=88 080 384 gate $h*num_experts * n_layers$=1 048 576 / GQA $hhdim(nhead+n_kv_heads) * 2 * n_layers / TP$=671 088 640 $hhdim(nhead+n_kv_heads) * 2 * n_layers / TP$=671 088 640 LN $h*2 * n_layers$=262 144 $h*2 * n_layers$=262 144 total 3,622,043,648 3,620,995,072 推理所需显存/单卡 7,244,087,296 7,241,990,144 训练所需显存/单卡 57,952,698,368 57,935,921,152 通过加入EP的方式，在7B的模型大小下，MoE训练所需的显存于正常7B相差不大，结论成立。\n注：实际计算中，MoE 的激活值会相比原有增大 EP 倍，和训练长度有关，以实际为准。\n结论2：expert数量越多，训练时所需的最小设备数越多；\n不超过7B的情况下，最小设备数的计算的逻辑如下：\n确定了训练使用设备的GPU显存大小，以及基底模型规模； 比如：80G，7B 确定可以使用的最小TP(tensor parallel)数； 因为每张卡上可以训练的参数量差不多是3-4B，因此TP=2的时候满足要求，每张卡3.5B； EP(expert parallel)使用最大，以节约内存； EP = num_experts 因为expert在DP中共享，所以DP必须是EP的整数倍； DP=EP 最终，最小设备数为：DP * TP。 在7B情况下，num_experts=8 最小设备数为16、num_experts=16 最小设备数为32。\n在Mixtral-7Bx8 MoE的例子中，训练时可以跑起来的最小硬件要求：A100 80G x 8张 x 2台。\n注1：超过7B时，必须要使用PP才能训练。\n注2：模型较大的情况下，还需要考虑稳定性其他因素，最优的并行组合需要实测后才能得出；\n训练成本 刨除显存以及设备数量这2点因素，训练成本以及速度计算如下：\n差异主要来自推理过程中活跃参数的数量差异； MoE 7b*8 的活跃参数是12b，所需算力相当于训练12b模型所需的算力； 推理成本 以Mixtral-7Bx8为例，整体参数量为46b，活跃参数是12b，但是在并行条件下推理速度接近于7.2b基底模型。因为虽然模型计算量翻倍了，但是模型容量主要是由宽度带来的，wide model更利于并行计算。\n但是由于计算量增加，所以在固定设备数量的前提下，对外服务的最大吞吐量会减半。也就是说：\nMoE模型会降低推理系统的最大吞吐量，但是时延变化不大； 吞吐量降低多少主要与活跃参数相关。 Mixtral-7Bx8MoE的收益和成本？ 成本\n成本主要取决于推理过程种的活跃参数，Mistral 7Bx8的MoE模型每次推理使用2个expert（top2），所以训练成本大概相当于7B模型的2倍，既训练一个12B模型所需的算力资源。\n收益\nModel active params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K LLaMA 2 7B 7b 44.4 77.1 69.5 77.9 68.7 43.2 17.5 56.6 11.6 26.1 3.9 16.0 LLaMA 2 13B 13b 55.6 80.7 72.9 80.8 75.2 48.8 16.7 64.0 18.9 58.4 6.0 34.3 LLaMA 2 33B 33b 56.8 83.7 76.2 82.2 79.6 54.4 24.1 68.5 25.0 40.9 8.4 44.1 LLaMA 2 70B 70B 69.9 85.4 80.4 82.6 79.9 56.5 25.4 73.0 29.3 49.8 13.8 69.6 Mixtral 7B 7B 62.5 81.0 81.0 82.2 80.5 54.9 23.2 62.5 26.2 50.2 12.7 50.0 Mistral 8x7B 12B 70.6 84.4 84.4 83.6 83.1 59.7 30.6 71.5 40.2 60.7 28.4 74.4 Mixtral 7B -\u003e MoE 7Bx8，上涨15.6%； LLaMA 7B -\u003e 13B，上涨19.4%； 结论：考虑到分数越高越难提升，mixtral的MoE 7Bx8的效果收益可以认为与直接训练13B的模型相仿。\n数据来自：Mixtral of experts | Mistral AI | Open source models\nMoE应用场景与对应的收益成本 *基础要求：建议的模型方案，所消耗的算力与基底模型保持不变。\n注：Switch-transformers, HashMoE, DeepspeedMoE等论文中的方案都可以满足该要求。\n场景一：在模型规模扩大遇到瓶颈时，通过MoE继续提升模型效果，并且使用保持相同的算力资源。\n收益：\n10%以上的效果提升； 类比mixtral 7bx8 MoE提升15%，GPT4比GPT3提升 20+% 成本：\n没有明显的成本增加； 场景二：对于线上模型，可以替换为更小基底的MoE模型。\n收益：\n在相同的推理资源下，维持相同的最大吞吐量，并大大降低模型的时延； 小基底的MoE模型使用的训练资源更少。 成本：\n没有明显的成本增加，除了开发的复杂度变高； MoE有哪些重要的工作 时间 单位 模型名称 gating sparsity Arch Jun 2020 Google GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding 引用：543 learnable token-based top2 MA\u003eMoE\u003eMA\u003eFFN Jun 2021 Google Switch-transformers引用：1024 learnable token-based top1 MA\u003eMoE Jul 2021 FAIR Hash Layers For Large Sparse Models引用：95 determinative hash-based *top1（hash决定） MA\u003eMoE Apr 2022 Google Brain ST-MoE: DesigningStable and Transferable Sparse Expert Models引用：28 learnable token-based top2 [MA\u003eFFN]*3 \u003eMA\u003eMoE Aug 2022 Google GLaM: Efficient Scaling of Language Models with Mixture-of-Experts引用：240 learnable token-based top2 MA\u003eMoE\u003eMA\u003eFFN Oct 2022 Google Mixture-of-Experts with Expert Choice Routing引用：56 learnable expert-based top2 MA\u003eMoE Oct 2022 University of Texas Residual Mixture of Experts引用：19 core + residual Top2 MA\u003eRMoE MoE的scaling law： 来自 Deepmind, 2022, Unified Scaling Laws for Routed Language Models\n计算公式如下：\n需要注意的有以下几点：\nlog的基底为10；\n默认MoE的位置是 MA-\u003eFFN-\u003eMA-\u003eMoE；\n也就是隔一层放置一个，如果是每层放置，对应的expert数量要*2；\n对应的参数有3组，分别对应了3种路由算法：S-BASE、RL-R、HASH；\n其中Hash效果最差，与self-learning参数的路由最接近；\nmega-blocks对应的方案是S-BASE的方案，效果最好；所以如果只是使用的话注意选择合适的参数；\n一个scaling law的使用场景是计算有效参数，能够把MoE的模型参数对应到对应的Dense网络参数，计算方式如下：\n实测，Mixtral 7bx8对应的有效参数大小是12.2B，与前面从效果和算力层面的推测一致。\n难收敛如何解决 原因1：MoE刚开始训练时expert是随机初始化的，并且训练过程中expert的能力在不断变化，所以初期gating网络一直在拟合变化中的mapping关系，导致收敛时间边长；\n解决方法1：分阶段训练；Evo-MoE\na. 首先训练一个正常的网络；b. 通过随机mask将expert分化成不同的expert；c. 加入gating网络，从topk逐渐降低到top1；\n解决方法2：使用固定的映射作为route，hash-based routing。\n这种方式可以提前生成token对应的expert映射，所以不存在波动的问题。\n原因2：gating网络数值不稳定性；\ngating网络在更新时，输入的logits会增大。因为softmax的导数与输入成正比，所以输入越大会导致梯度越大，容易产生不稳定。\n解决方法：加入z-loss，限制输入gating网络的logits的大小。\n其他原因：\n乘性算子：RMS LN、GEGLU使用会增加不稳定性； 越大的模型越不稳定； 相关解决方法：\nfloat32去计算gating 网络中的softmax； 使用更小的初始化参数； expert负载均衡如何解决？ 问题提出：Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\nWe have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts；\n如果不做任何控制，gating网络会倾向于给某几个expert更高的权重。\n少量的负载不均衡不会影响模型效果，但是会影响推理和训练的速度。\n解决方法：\n附加损失auxiliary loss hash-based route 微调效果不稳定如何解决？ 问题原因：\nMoE模型比Dense模型更容易在少量数据上过拟合； MoE参数微调容易带来基础性能的丢失； 解决方法：\nexpert dropout调整到比正常参数高一些； 正常0.1，expert 0.4 只更新非expert参数往往能够得到和全量更新同样的效果。 附录 相关论文速览 Adaptive mixtures of local experts\nHinton, 1991 https://direct.mit.edu/neco/article/3/1/79-87/5560\n这篇论文第一次介绍moe的方法，其主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nGoogle, Jun 2022 http://arxiv.org/abs/2101.03961\n目的还是把模型做大，容量提升 但是不明显增加推理的计算量。论文主要围绕3个问题提出解决方案：复杂度、通讯消耗、训练稳定性。实验基于T5家族模型上进行，第一次证明了这类稀疏模型可以用低精度(BF16)训练。最终效果，同样的资源情况下训练速度提升4-7倍，在多语种场景下在全部101种语言上观察到提升。\nBalancing has been previously shown to be important for training MoE models!\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nGoogle, Aug 2022 http://arxiv.org/abs/2112.06905\n第一次将MoE应用到GPT3这种规格的模型上面，最终的效果也是在29个NLP任务中领先GPT3。最大的模型参数量是GPT3的7倍，但是训练消耗仅仅是1/3。有趣的是，他们用训练使用能量来衡量训练消耗。\n**PS：**虽然模型结构和GShard完全类似，但是应用规模比其他的工作要大，所以有较高的参考价值。\nGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nGoogle, 2021 https://arxiv.org/pdf/2006.16668.pdf\n第一次将MoE应用到transformer结构中，并且介绍了很多工程的细节，比如并行 以及 负载均衡。在工程方面，提出了一套API来处理MoE高效并行的实现，将模型结构和具体实现分离，并且在使用起来更符合语义层面的理解。值得深入研究，对工程实现方面的借鉴意义更大。\nBrainformers: Trading Simplicity for Efficiency\nDeepmind, 2023 http://arxiv.org/abs/2306.00008\n提出一种新的FFN、Attn、Gate模块的排列方式，比正常的attention排列得到的模型更高效（原文quality and efficiency）。与同是MoE模型的GLaM相比，提升2倍的收敛速度，降低5倍的单步训练时长。\n新的模型结构通过自动化程序挖掘出来；top-2 has demonstrated stronger empirical performance than top-1 gating；\nTo avoid causal leakage in decoding mode, we suggest normalizing along the expert dimension for both token-based routing and expert-based routing. 因为在训练的过程中，其实是用全文做的normalize，但是decode的时候看不到全文。\n问题：在Token-based Routing Versus Expert-based Routing这章节里，为什么要做normalize？\nST-MoE: Designing Stable and Transferable Sparse Expert Models\nGoogle Brain, Apr 2022 http://arxiv.org/abs/2202.08906\n主要针对MoE模型的训练稳定性，以及微调质量无法保证 这2个问题，提出了对应的解决方案，最终得到的模型叫做ST-MoE-32B。效果层面上，通过微调的MoE模型第一次在下游任务中得到SoTA。\n主要通过模型结构以及loss的改动，增加训练的稳定性。并且对训练稳定性进行了深入的研究，定量分析了一些模型改动对稳定性的影响。可能是对MoE训练稳定性问题研究最多的一篇论文。\nMixture-of-Experts with Expert Choice Routing\nGoogle, Oct 2022 https://arxiv.org/pdf/2202.09368.pdf\n文章的动机是解决不好的routing策略，导致expert训练不充分的问题。不同于正常的routing策略是token选择experts，本文中提出的策略是expert选择tokens。对比了Switch Transformer和GShard 2种方法，发现训练速度可以提高1倍，并且效果GLUE和SuperGLUE上表现更好。\n第一次提出expert-based routing的方式，后来在BrainFormer中有被用到。为什么这种方式会更好呢，背后的直觉是什么？\nFrom Sparse to Soft Mixtures of Experts\nGoogle Deepmind, Aug 2023 http://arxiv.org/abs/2308.00951\n视觉方向的一篇MoE论文，主要解决的问题是：训练不稳定、token dropping、experts扩展、微调效果。提出一种完全可导的稀疏MoE结构（==正常的MoE不是也可导吗？==）本质上，这篇文章也提出了一种新的Routing策略，对比了Token-based和Expert-based这2种routing策略，在推理速度、expert数量扩展上都有不同程度的优势。\nHash layers for large sparse models\nFacebook AI, Jul 2021 http://arxiv.org/abs/2106.04426\n通过对token Hash选择expert，省去了gating网络，优于SwitchTransformer和传统的Transformer模型。同时探索了不同的hash算法对应的效果，并且分析hashing-routing方法的有效性。\n感觉上Hash方法更接近于随机去选择expert，为什么还能起到效果，很奇怪？Hash如何与语义关联的？\n正常的gating网络和expert需要一起进行训练，而在刚开始的时候gating网络是随机的，expert之间也没有什么区别，那么随着expert不断训练它所包含的知识也在改变，而gating网络去学习这种变化的mapping关系，可能会导致模型最后效果很差。PS：本质上还是如何将expert训练的更有差异化的问题。基于这个问题，这篇文章提出的hashing至少在训练过程中会更稳定，确定了hash算法，那么token和expert之间的mapping关系就定下来了。PS:直觉上可以加快模型训练的速度。\n这篇论文实验非常详细，感觉对于routing策略来说是一个很强的baseline。结论说明一点，固定的routing方法能得到更好的训练结果\nGo Wider Instead of Deeper\nNational University of Singapore, Sep 2021 http://arxiv.org/abs/2107.11817\nMOE模型虽然保证计算量不增加太多，但是模型占用显存是显著增加了的。这篇论文中通过层间共享参数的机制，降低了显存的占用，并且得到的效果还不错（类似ALBERT结合MOE）。\n把ALBERT中层共享参数的方法与MoE模型，然后声称是Wider Net。然而，计算过程还是会有多层的推理，每一层的layer norm使用的是不同的参数。有点标题党的嫌疑，并且测试模型的规模都比较小，没有太多可以借鉴的地方。\nAdaptive mixtures of local experts论文阅读 BP提出自1986年，这篇文章写自1991年。\n主要想解决的问题是：当BP用来训练一个解决多个不同问题的网络时，因为任务之间的干涉导致模型难以收敛和泛化。虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n原来的公式：\n$$ \\mathbb{E}^c = {\\parallel \\vec{d^c}-\\sum_i{p_i^c \\vec{o_i^c}} \\parallel}^2 $$\n其中，$c$ 是case，$d$是目标输出，$p$ 是expert对应的gate weight，$o$ 是export对应的输出。\n分析：如果其中1个expert的输出（$o_i$）改变，导致expert所有输出的调和超过了最终的输出（$\\vec{d^c}$），那么所有expert梯度都会变为反方向。造成的结果就是，expert之间会互相干扰，导致收敛速度变慢，并且学习到的expert会倾向于合作的关系，共同作用于1个case并得到最终结果。\n那么有没有方法expert之间更倾向于竞争关系呢。\n这个问题有2篇相关的论文：\nLearning piecewise control strategies inamo dular connectionist architecture Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks ==TODO==：一种方法是给目标函数添加惩罚项，来奖励expert之间更多的竞争。\n如果把loss改成如下：\n$$ \\mathbb{E}^c = \\sum_i p_i^c{\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel}^2 $$\n最直观的变化是，每个expert的输出单独拿出来和目标进行比较，所以不管gate网络和其他网络的输出是什么，每个expert得到的梯度方向只取决于自身的输出和目标的差值。但如果其中某个expert输出对应的error变小，那么gate网络对应该expert的概率就会增加。这点可以通过梯度 $\\frac{\\partial \\mathbb{E}^c}{p_i}={\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel}^2$很明显的看到，$p_i$梯度大小取决于error平方的大小，梯度越大下降的越快。\n新的目标函数对应每个expert的梯度如下：\n$$ \\frac{\\partial \\mathbb{E}^c}{\\partial \\vec{o_i^c}}=-2p_i^c(\\vec{d^c}-\\vec{o_i^c}) $$\n此时会出现一个问题，当初期网络随机初始化后，可以认为每个expert对应的权重$p_i^c$是相同的，那么每个expert的梯度取决于error的大小，所以拟合最好的expert得到的梯度最小。为了解决这个问题，提出了新的目标函数如下：\n$$ \\mathbb{E}^c=-log\\sum_i{p_i^c e^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel^2}} $$\n此时每个expert输出对应的梯度为：\n$$ \\frac{\\partial \\mathbb{E}^c}{\\partial \\vec{o_i^c}}=-\\Big \\frac{p_i^c e ^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel^2}}{\\sum_j{p_j^c e ^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_j^c} \\parallel^2}}} \\Big $$\n梯度的前部分确保了表现好的expert能够得到更快的训练。\n",
  "wordCount" : "949",
  "inLanguage": "en",
  "datePublished": "2023-12-20T23:40:17+08:00",
  "dateModified": "2023-12-20T23:40:17+08:00",
  "author":{
    "@type": "Person",
    "name": "Dawson Chen"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dawson-chen.github.io/posts/moe-introduce/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DawsonChen's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dawson-chen.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dawson-chen.github.io/" accesskey="h" title="DawsonChen&#39;s Blog (Alt + H)">DawsonChen&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dawson-chen.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://dawson-chen.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://dawson-chen.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://dawson-chen.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://dawson-chen.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Moe(Mixtrue of Experts)技术调研
    </h1>
    <div class="post-meta"><span title='2023-12-20 23:40:17 +0800 CST'>December 20, 2023</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Dawson Chen

      <div  class="meta-item">&nbsp·&nbsp
        <span id="busuanzi_container_page_pv">Pageviews: <span id="busuanzi_value_page_pv"></span></span>
      </div>
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#moe%e7%9a%84%e5%8f%91%e6%98%8e%e7%9a%84%e5%8a%a8%e6%9c%ba%e6%98%af%e4%bb%80%e4%b9%88" aria-label="MoE的发明的动机是什么？">MoE的发明的动机是什么？</a></li>
                <li>
                    <a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%9c%a81991%e5%b9%b4%e6%8f%90%e5%87%ba%e7%9b%b4%e5%88%b0%e6%9c%80%e8%bf%91%e6%89%8d%e9%87%8d%e6%96%b0%e8%bf%9b%e5%85%a5%e8%a7%86%e9%87%8e" aria-label="为什么在1991年提出直到最近才重新进入视野？">为什么在1991年提出直到最近才重新进入视野？</a></li>
                <li>
                    <a href="#moe%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e8%b5%b7%e4%bd%9c%e7%94%a8" aria-label="MoE为什么会起作用？">MoE为什么会起作用？</a></li>
                <li>
                    <a href="#expert%e7%9c%9f%e7%9a%84%e6%98%af%e4%b8%93%e5%ae%b6%e5%90%97" aria-label="Expert真的是专家吗？">Expert真的是专家吗？</a></li>
                <li>
                    <a href="#%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e8%ae%ad%e7%bb%83%e5%92%8c%e6%8e%a8%e7%90%86%e6%88%90%e6%9c%ac" aria-label="如何计算训练和推理成本？">如何计算训练和推理成本？</a><ul>
                        
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e6%88%90%e6%9c%ac" aria-label="训练成本">训练成本</a></li>
                <li>
                    <a href="#%e6%8e%a8%e7%90%86%e6%88%90%e6%9c%ac" aria-label="推理成本">推理成本</a></li></ul>
                </li>
                <li>
                    <a href="#mixtral-7bx8moe%e7%9a%84%e6%94%b6%e7%9b%8a%e5%92%8c%e6%88%90%e6%9c%ac" aria-label="Mixtral-7Bx8MoE的收益和成本？">Mixtral-7Bx8MoE的收益和成本？</a><ul>
                        
                <li>
                    <a href="#moe%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e4%b8%8e%e5%af%b9%e5%ba%94%e7%9a%84%e6%94%b6%e7%9b%8a%e6%88%90%e6%9c%ac" aria-label="MoE应用场景与对应的收益成本">MoE应用场景与对应的收益成本</a></li></ul>
                </li>
                <li>
                    <a href="#moe%e6%9c%89%e5%93%aa%e4%ba%9b%e9%87%8d%e8%a6%81%e7%9a%84%e5%b7%a5%e4%bd%9c" aria-label="MoE有哪些重要的工作">MoE有哪些重要的工作</a></li>
                <li>
                    <a href="#moe%e7%9a%84scaling-law" aria-label="MoE的scaling law：">MoE的scaling law：</a></li>
                <li>
                    <a href="#%e9%9a%be%e6%94%b6%e6%95%9b%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3" aria-label="难收敛如何解决">难收敛如何解决</a></li>
                <li>
                    <a href="#expert%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3" aria-label="expert负载均衡如何解决？">expert负载均衡如何解决？</a></li>
                <li>
                    <a href="#%e5%be%ae%e8%b0%83%e6%95%88%e6%9e%9c%e4%b8%8d%e7%a8%b3%e5%ae%9a%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3" aria-label="微调效果不稳定如何解决？">微调效果不稳定如何解决？</a></li>
                <li>
                    <a href="#%e9%99%84%e5%bd%95" aria-label="附录">附录</a><ul>
                        
                <li>
                    <a href="#%e7%9b%b8%e5%85%b3%e8%ae%ba%e6%96%87%e9%80%9f%e8%a7%88" aria-label="相关论文速览">相关论文速览</a></li>
                <li>
                    <a href="#adaptive-mixtures-of-local-experts%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb" aria-label="Adaptive mixtures of local experts论文阅读">Adaptive mixtures of local experts论文阅读</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p><em><strong>陈道一</strong>，12/14/2023</em></p>
</blockquote>
<h2 id="moe的发明的动机是什么">MoE的发明的动机是什么？<a hidden class="anchor" aria-hidden="true" href="#moe的发明的动机是什么">#</a></h2>
<p>MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。</p>
<p>第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：<strong>保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果</strong>。</p>
<h2 id="为什么在1991年提出直到最近才重新进入视野">为什么在1991年提出直到最近才重新进入视野？<a hidden class="anchor" aria-hidden="true" href="#为什么在1991年提出直到最近才重新进入视野">#</a></h2>
<p>1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。</p>
<p>而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。</p>
<h2 id="moe为什么会起作用">MoE为什么会起作用？<a hidden class="anchor" aria-hidden="true" href="#moe为什么会起作用">#</a></h2>
<p><strong>scaling law层面的解释</strong></p>
<p>Scaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：<strong>在固定数据集大小下 尽量训练更大规模的模型</strong>。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。</p>
<p><font style="background-color:yellow">MoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。</font></p>
<p>典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：</p>
<p><img loading="lazy" src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c8a098752565457d997d31558b3f3f06~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1690&amp;h=770&amp;s=175146&amp;e=png&amp;b=fdf9f9" alt="image.png"  />
</p>
<p><strong>模型结构层面的猜想</strong></p>
<p>观察1：FFN可以理解为KV对，K代表了<strong>文本模式</strong>、V代表了<strong>文本分布</strong>^[1]^，越靠后的层学习到越复杂的模式。</p>
<p>观察2：模型越靠后的层学习越不够充分。</p>
<p>基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。</p>
<p>所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。</p>
<blockquote>
<p>[1] Transformer Feed-Forward Layers Are Key-Value Memories
[2] Deepspeed-MoE#PR-MoE
[3] Hash Layers For Large Sparse Models#Figure 3</p>
</blockquote>
<h2 id="expert真的是专家吗">Expert真的是专家吗？<a hidden class="anchor" aria-hidden="true" href="#expert真的是专家吗">#</a></h2>
<p>考虑下面3个MoE路由配置的实验：</p>
<ol>
<li>将语义相似的token路由到相同的expert上；</li>
<li>将语义相似的token路由到不同的expert上；</li>
<li>随机将token指定到一个固定的expert上；</li>
</ol>
<p>如果把expert理解成不同领域的专家，那么应该是1&gt;2&gt;3，但真实的实验结论是2&gt;3&raquo;1（23.22&gt;23.27&raquo;23.99）。</p>
<p>对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。</p>
<h2 id="如何计算训练和推理成本">如何计算训练和推理成本？<a hidden class="anchor" aria-hidden="true" href="#如何计算训练和推理成本">#</a></h2>
<p><strong>结论1</strong>：<em>MoE训练所需的显存与基底模型并没有明显差别；</em></p>
<p>以Mixtral-7B*8 MoE为例：</p>
<table>
<thead>
<tr>
<th>dim</th>
<th>n_layers</th>
<th>hidden_dim</th>
<th>n_heads</th>
<th>n_kv_heads</th>
<th>vocab_size</th>
<th>num_experts_per_tok</th>
<th>num_experts</th>
</tr>
</thead>
<tbody>
<tr>
<td>4096</td>
<td>32</td>
<td>14336</td>
<td>32</td>
<td>8</td>
<td>32000</td>
<td>2</td>
<td>8</td>
</tr>
</tbody>
</table>
<ul>
<li>整体参数量：46b</li>
<li>活跃参数量：12b</li>
<li>对应基底模型参数量：7.2b</li>
</ul>
<p>计算过程如下：</p>
<blockquote>
<ul>
<li>transformers:
<ul>
<li>FFNs: 176,160,768 * 8 = 45,097,156,608</li>
<li>gate: 4096 * 8 = 1,048,576</li>
<li>MA: 4096 * (128 * 48) + 4096 * 4096= 41,943,040</li>
<li>LN: 4096 * 2 = 8,192</li>
<li>total: (176,160,768 * 8 + 41,943,040 + 8,192 + 4096 * 8) * 32 = 46,440,644,608</li>
</ul>
</li>
<li>others:
<ul>
<li>embed &amp; output_w: 262,144,000</li>
</ul>
</li>
<li>total:
<ul>
<li>46,440,644,608 + 262,144,000 = 46,702,788,608 = 46B</li>
</ul>
</li>
<li>active params:
<ul>
<li>(176,160,768 * 2 + 41,943,040 + 8,192 + 4096 * 8) * 32 + 262,144,000 = 12,879,921,152 = 12.8B</li>
</ul>
</li>
</ul>
</blockquote>
<p>模型训练的并行方式分为3种，DP(data parallel) / TP(tensor parallel) / PP(pipline parallel)，MoE模型在训练时可以同时使用这3种模型外，还可以加入EP(expert parallel)方式。EP的精髓就是<strong>多个模型中共享expert</strong>，2点理解：</p>
<ul>
<li>因为MoE每次前向中只用到一小部分expert，如果每个模型保留完整的expert，一定会导致大多数expert空闲的情况；</li>
<li>如果DP是8，EP是2，那么2个模型共用一套完整的experts；</li>
</ul>
<p>训练并行设定：TP2 DP8 EP8 （megatron方案），需要显存如下：</p>
<table>
<thead>
<tr>
<th>module</th>
<th>MoE参数/单卡</th>
<th>Dense参数/单卡</th>
</tr>
</thead>
<tbody>
<tr>
<td>Emb and Output</td>
<td>$h * vocab *2 /TP$=262 144</td>
<td>$h * vocab *2 /TP$=262 144</td>
</tr>
<tr>
<td>experts</td>
<td>$h<em>ffn</em>3 * num_experts * n_layers / TP / EP$=88 080 384</td>
<td>$h<em>ffn</em>3 * n_layers / TP$=88 080 384</td>
</tr>
<tr>
<td>gate</td>
<td>$h*num_experts * n_layers$=1 048 576</td>
<td>/</td>
</tr>
<tr>
<td>GQA</td>
<td>$h<em>hdim</em>(nhead+n_kv_heads) * 2 *  n_layers / TP$=671 088 640</td>
<td>$h<em>hdim</em>(nhead+n_kv_heads) * 2 *  n_layers / TP$=671 088 640</td>
</tr>
<tr>
<td>LN</td>
<td>$h*2  * n_layers$=262 144</td>
<td>$h*2  * n_layers$=262 144</td>
</tr>
<tr>
<td><strong>total</strong></td>
<td><strong>3,622,043,648</strong></td>
<td><strong>3,620,995,072</strong></td>
</tr>
<tr>
<td><strong>推理所需显存/单卡</strong></td>
<td><strong>7,244,087,296</strong></td>
<td><strong>7,241,990,144</strong></td>
</tr>
<tr>
<td><strong>训练所需显存/单卡</strong></td>
<td><strong>57,952,698,368</strong></td>
<td><strong>57,935,921,152</strong></td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>通过加入EP的方式，在7B的模型大小下，MoE训练所需的显存于正常7B相差不大，结论成立。</p>
<p>注：实际计算中，MoE 的激活值会相比原有增大 EP 倍，和训练长度有关，以实际为准。</p>
</li>
</ul>
<p><strong>结论2</strong>：<em>expert数量越多，训练时所需的最小设备数越多；</em></p>
<p>不超过7B的情况下，最小设备数的计算的逻辑如下：</p>
<ol>
<li>确定了训练使用设备的GPU显存大小，以及基底模型规模；
比如：80G，7B</li>
<li>确定可以使用的最小TP(tensor parallel)数；
因为每张卡上可以训练的参数量差不多是3-4B，因此TP=2的时候满足要求，每张卡3.5B；</li>
<li>EP(expert parallel)使用最大，以节约内存；
EP = num_experts</li>
<li>因为expert在DP中共享，所以DP必须是EP的整数倍；
DP=EP</li>
<li>最终，最小设备数为：DP * TP。</li>
</ol>
<p>在7B情况下，num_experts=8 最小设备数为16、num_experts=16 最小设备数为32。</p>
<p>在Mixtral-7Bx8 MoE的例子中，训练时可以跑起来的最小硬件要求：A100 80G x 8张 x 2台。</p>
<blockquote>
<p>注1：超过7B时，必须要使用PP才能训练。</p>
<p>注2：模型较大的情况下，还需要考虑稳定性其他因素，最优的并行组合需要实测后才能得出；</p>
</blockquote>
<h3 id="训练成本">训练成本<a hidden class="anchor" aria-hidden="true" href="#训练成本">#</a></h3>
<p>刨除显存以及设备数量这2点因素，训练成本以及速度计算如下：</p>
<ul>
<li>差异主要来自推理过程中活跃参数的数量差异；</li>
<li>MoE 7b*8 的活跃参数是12b，所需算力相当于训练12b模型所需的算力；</li>
</ul>
<h3 id="推理成本">推理成本<a hidden class="anchor" aria-hidden="true" href="#推理成本">#</a></h3>
<p>以Mixtral-7Bx8为例，整体参数量为46b，活跃参数是12b，但是在并行条件下推理速度接近于7.2b基底模型。因为虽然模型计算量翻倍了，但是模型容量主要是由宽度带来的，wide model更利于并行计算。</p>
<p>但是由于计算量增加，所以在固定设备数量的前提下，对外服务的最大吞吐量会减半。也就是说：</p>
<ul>
<li>MoE模型会降低推理系统的<strong>最大吞吐量</strong>，但是<strong>时延变化不大</strong>；</li>
<li>吞吐量降低多少主要与活跃参数相关。</li>
</ul>
<h2 id="mixtral-7bx8moe的收益和成本">Mixtral-7Bx8MoE的收益和成本？<a hidden class="anchor" aria-hidden="true" href="#mixtral-7bx8moe的收益和成本">#</a></h2>
<p><strong>成本</strong></p>
<p>成本主要取决于推理过程种的活跃参数，Mistral 7Bx8的MoE模型每次推理使用2个expert（top2），所以训练成本大概相当于7B模型的2倍，既训练一个12B模型所需的算力资源。</p>
<p><strong>收益</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>active params</th>
<th>MMLU</th>
<th>HellaS</th>
<th>WinoG</th>
<th>PIQA</th>
<th>Arc-e</th>
<th>Arc-c</th>
<th>NQ</th>
<th>TriQA</th>
<th>HumanE</th>
<th>MBPP</th>
<th>Math</th>
<th>GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA 2 7B</td>
<td>7b</td>
<td>44.4</td>
<td>77.1</td>
<td>69.5</td>
<td>77.9</td>
<td>68.7</td>
<td>43.2</td>
<td>17.5</td>
<td>56.6</td>
<td>11.6</td>
<td>26.1</td>
<td>3.9</td>
<td>16.0</td>
</tr>
<tr>
<td>LLaMA 2 13B</td>
<td>13b</td>
<td>55.6</td>
<td>80.7</td>
<td>72.9</td>
<td>80.8</td>
<td>75.2</td>
<td>48.8</td>
<td>16.7</td>
<td>64.0</td>
<td>18.9</td>
<td>58.4</td>
<td>6.0</td>
<td>34.3</td>
</tr>
<tr>
<td>LLaMA 2 33B</td>
<td>33b</td>
<td>56.8</td>
<td>83.7</td>
<td>76.2</td>
<td>82.2</td>
<td>79.6</td>
<td>54.4</td>
<td>24.1</td>
<td>68.5</td>
<td>25.0</td>
<td>40.9</td>
<td>8.4</td>
<td>44.1</td>
</tr>
<tr>
<td>LLaMA 2 70B</td>
<td>70B</td>
<td>69.9</td>
<td>85.4</td>
<td>80.4</td>
<td>82.6</td>
<td>79.9</td>
<td>56.5</td>
<td>25.4</td>
<td>73.0</td>
<td>29.3</td>
<td>49.8</td>
<td>13.8</td>
<td>69.6</td>
</tr>
<tr>
<td>Mixtral 7B</td>
<td>7B</td>
<td>62.5</td>
<td>81.0</td>
<td>81.0</td>
<td>82.2</td>
<td>80.5</td>
<td>54.9</td>
<td>23.2</td>
<td>62.5</td>
<td>26.2</td>
<td>50.2</td>
<td>12.7</td>
<td>50.0</td>
</tr>
<tr>
<td>Mistral 8x7B</td>
<td>12B</td>
<td>70.6</td>
<td>84.4</td>
<td>84.4</td>
<td>83.6</td>
<td>83.1</td>
<td>59.7</td>
<td>30.6</td>
<td>71.5</td>
<td>40.2</td>
<td>60.7</td>
<td>28.4</td>
<td>74.4</td>
</tr>
</tbody>
</table>
<ul>
<li>Mixtral 7B -&gt; MoE 7Bx8，上涨15.6%；</li>
<li>LLaMA 7B -&gt; 13B，上涨19.4%；</li>
</ul>
<p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f9093af85c8b4b82851c009527dc137e~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1862&amp;h=710&amp;s=68245&amp;e=png&amp;b=fefefe" alt="image.png"  />
</p>
<p><strong>结论</strong>：考虑到分数越高越难提升，mixtral的MoE 7Bx8的效果收益可以认为与直接训练13B的模型相仿。</p>
<p><em>数据来自：<a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral of experts | Mistral AI | Open source models</a></em></p>
<h3 id="moe应用场景与对应的收益成本">MoE应用场景与对应的收益成本<a hidden class="anchor" aria-hidden="true" href="#moe应用场景与对应的收益成本">#</a></h3>
<p>*<strong>基础要求</strong>：建议的模型方案，所消耗的算力与基底模型保持不变。</p>
<blockquote>
<p>注：Switch-transformers, HashMoE, DeepspeedMoE等论文中的方案都可以满足该要求。</p>
</blockquote>
<p><strong>场景一</strong>：在模型规模扩大遇到瓶颈时，通过MoE继续提升模型效果，并且使用保持相同的算力资源。</p>
<p>收益：</p>
<ul>
<li>10%以上的效果提升；
类比mixtral 7bx8 MoE提升15%，GPT4比GPT3提升 20+%</li>
</ul>
<p>成本：</p>
<ul>
<li>没有明显的成本增加；</li>
</ul>
<p><strong>场景二</strong>：对于线上模型，可以替换为更小基底的MoE模型。</p>
<p>收益：</p>
<ul>
<li>在相同的推理资源下，维持相同的最大吞吐量，并大大降低模型的时延；</li>
<li>小基底的MoE模型使用的训练资源更少。</li>
</ul>
<p>成本：</p>
<ul>
<li>没有明显的成本增加，除了开发的复杂度变高；</li>
</ul>
<h2 id="moe有哪些重要的工作">MoE有哪些重要的工作<a hidden class="anchor" aria-hidden="true" href="#moe有哪些重要的工作">#</a></h2>
<table>
<thead>
<tr>
<th>时间</th>
<th>单位</th>
<th>模型名称</th>
<th>gating</th>
<th>sparsity</th>
<th>Arch</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>Jun 2020</strong></em></td>
<td>Google</td>
<td>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding 引用：543</td>
<td>learnable token-based</td>
<td>top2</td>
<td>MA&gt;MoE&gt;MA&gt;FFN</td>
</tr>
<tr>
<td><em><strong>Jun 2021</strong></em></td>
<td>Google</td>
<td>Switch-transformers引用：1024</td>
<td>learnable token-based</td>
<td>top1</td>
<td>MA&gt;MoE</td>
</tr>
<tr>
<td><em><strong>Jul 2021</strong></em></td>
<td>FAIR</td>
<td>Hash Layers For Large Sparse Models引用：95</td>
<td>determinative hash-based</td>
<td>*top1（hash决定）</td>
<td>MA&gt;MoE</td>
</tr>
<tr>
<td><em><strong>Apr 2022</strong></em></td>
<td>Google Brain</td>
<td>ST-MoE: Designing<strong>Stable</strong> and <strong>Transferable</strong> Sparse Expert Models引用：28</td>
<td>learnable token-based</td>
<td>top2</td>
<td>[MA&gt;FFN]*3 &gt;MA&gt;MoE</td>
</tr>
<tr>
<td><em><strong>Aug 2022</strong></em></td>
<td>Google</td>
<td>GLaM: Efficient Scaling of Language Models with Mixture-of-Experts引用：240</td>
<td>learnable token-based</td>
<td>top2</td>
<td>MA&gt;MoE&gt;MA&gt;FFN</td>
</tr>
<tr>
<td><em><strong>Oct 2022</strong></em></td>
<td>Google</td>
<td>Mixture-of-Experts with Expert Choice Routing引用：56</td>
<td>learnable expert-based</td>
<td>top2</td>
<td>MA&gt;MoE</td>
</tr>
<tr>
<td><em><strong>Oct 2022</strong></em></td>
<td>University of Texas</td>
<td>Residual Mixture of Experts引用：19</td>
<td>core + residual</td>
<td>Top2</td>
<td>MA&gt;RMoE</td>
</tr>
</tbody>
</table>
<h2 id="moe的scaling-law">MoE的scaling law：<a hidden class="anchor" aria-hidden="true" href="#moe的scaling-law">#</a></h2>
<p><img loading="lazy" src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a5956df9d9248f38a4e4f42a5e23823~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2296&amp;h=636&amp;s=324184&amp;e=png&amp;b=fdfdfd" alt="image.png"  />
<em>来自 Deepmind, 2022, Unified Scaling Laws for Routed Language Models</em></p>
<p>计算公式如下：</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2d71aa5de90c48c58561a9d94242b0ad~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2016&amp;h=274&amp;s=34023&amp;e=png&amp;b=ffffff" alt="image.png"  />
</p>
<p>需要注意的有以下几点：</p>
<ul>
<li>
<p>log的基底为10；</p>
</li>
<li>
<p>默认MoE的位置是 MA-&gt;FFN-&gt;MA-&gt;MoE；</p>
<p>也就是隔一层放置一个，如果是每层放置，对应的expert数量要*2；</p>
</li>
<li>
<p>对应的参数有3组，分别对应了3种路由算法：S-BASE、RL-R、HASH；</p>
<p>其中Hash效果最差，与self-learning参数的路由最接近；</p>
<p>mega-blocks对应的方案是S-BASE的方案，效果最好；所以如果只是使用的话注意选择合适的参数；</p>
<p><img loading="lazy" src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/45442a85e4c64ef8b5e5d3d921a19349~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1348&amp;h=332&amp;s=44723&amp;e=png&amp;b=ffffff" alt="image.png"  />
</p>
</li>
</ul>
<p>一个scaling law的使用场景是计算有效参数，能够把MoE的模型参数对应到对应的Dense网络参数，计算方式如下：</p>
<p><img loading="lazy" src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6d687957966e40bc8786dcc36b8c5da8~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2214&amp;h=248&amp;s=67878&amp;e=png&amp;b=fefefe" alt="image.png"  />
</p>
<p>实测，Mixtral 7bx8对应的有效参数大小是12.2B，与前面从效果和算力层面的推测一致。</p>
<h2 id="难收敛如何解决">难收敛如何解决<a hidden class="anchor" aria-hidden="true" href="#难收敛如何解决">#</a></h2>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/afca2a204e8c41ff96433a17e6e8a55e~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1912&amp;h=758&amp;s=179392&amp;e=png&amp;b=fefefe" alt="image.png"  />
</p>
<hr>
<p><strong>原因1</strong>：MoE刚开始训练时expert是随机初始化的，并且训练过程中expert的能力在不断变化，所以初期gating网络一直在拟合变化中的mapping关系，导致收敛时间边长；</p>
<p><strong>解决方法1</strong>：分阶段训练；Evo-MoE</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/60699f5ffc2d4e2d82ee2cd7f06daa6d~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1762&amp;h=648&amp;s=100340&amp;e=png&amp;b=fefdfd" alt="image.png"  />
</p>
<p>a. 首先训练一个正常的网络；b. 通过随机mask将expert分化成不同的expert；c. 加入gating网络，从topk逐渐降低到top1；</p>
<p><strong>解决方法2</strong>：使用固定的映射作为route，hash-based routing。</p>
<p>这种方式可以提前生成token对应的expert映射，所以不存在波动的问题。</p>
<hr>
<p><strong>原因2</strong>：gating网络数值不稳定性；</p>
<p>gating网络在更新时，输入的logits会增大。因为softmax的导数与输入成正比，所以输入越大会导致梯度越大，容易产生不稳定。</p>
<p><img loading="lazy" src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8099703945c0472d9f3b439d99c55716~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1876&amp;h=664&amp;s=150446&amp;e=png&amp;b=ffffff" alt="image.png"  />
</p>
<p><strong>解决方法</strong>：加入z-loss，限制输入gating网络的logits的大小。</p>
<p><img loading="lazy" src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8514255e98a84142b5a09f970824cfc9~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1924&amp;h=204&amp;s=18924&amp;e=png&amp;b=ffffff" alt="image.png"  />
</p>
<hr>
<p><strong>其他原因</strong>：</p>
<ul>
<li>乘性算子：RMS LN、GEGLU使用会增加不稳定性；</li>
<li>越大的模型越不稳定；</li>
</ul>
<p><strong>相关解决方法</strong>：</p>
<ul>
<li>float32去计算gating 网络中的softmax；</li>
<li>使用更小的初始化参数；</li>
</ul>
<h2 id="expert负载均衡如何解决">expert负载均衡如何解决？<a hidden class="anchor" aria-hidden="true" href="#expert负载均衡如何解决">#</a></h2>
<p><strong>问题提出</strong>：Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</p>
<blockquote>
<p>We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts；</p>
<p>如果不做任何控制，gating网络会倾向于给某几个expert更高的权重。</p>
</blockquote>
<p>少量的负载不均衡不会影响模型效果，但是会影响推理和训练的速度。</p>
<p><strong>解决方法</strong>：</p>
<ul>
<li>附加损失auxiliary loss</li>
<li>hash-based route</li>
</ul>
<h2 id="微调效果不稳定如何解决">微调效果不稳定如何解决？<a hidden class="anchor" aria-hidden="true" href="#微调效果不稳定如何解决">#</a></h2>
<p><strong>问题原因</strong>：</p>
<ul>
<li>MoE模型比Dense模型更容易在少量数据上过拟合；</li>
<li>MoE参数微调容易带来基础性能的丢失；</li>
</ul>
<p><strong>解决方法</strong>：</p>
<ul>
<li>expert dropout调整到比正常参数高一些；
正常0.1，expert 0.4</li>
<li>只更新非expert参数往往能够得到和全量更新同样的效果。</li>
</ul>
<h2 id="附录">附录<a hidden class="anchor" aria-hidden="true" href="#附录">#</a></h2>
<h3 id="相关论文速览">相关论文速览<a hidden class="anchor" aria-hidden="true" href="#相关论文速览">#</a></h3>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Adaptive mixtures of local experts</font></strong></p>
<p><em><strong>Hinton, 1991</strong></em> <em><a href="https://direct.mit.edu/neco/article/3/1/79-87/5560">https://direct.mit.edu/neco/article/3/1/79-87/5560</a></em></p>
<p>这篇论文第一次介绍moe的方法，其主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</font></strong></p>
<p><em><strong>Google, Jun 2022</strong></em> <em><a href="http://arxiv.org/abs/2101.03961">http://arxiv.org/abs/2101.03961</a></em></p>
<p>目的还是把模型做大，容量提升 但是不明显增加推理的计算量。论文主要围绕3个问题提出解决方案：复杂度、<strong>通讯消耗</strong>、<strong>训练稳定性</strong>。实验基于T5家族模型上进行，第一次证明了这类稀疏模型可以用<strong>低精度</strong>(BF16)训练。最终效果，同样的资源情况下训练速度提升4-7倍，在多语种场景下在全部101种语言上观察到提升。</p>
<p>Balancing has been previously shown to be important for training MoE models!</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</font></strong></p>
<p><em><strong>Google, Aug 2022</strong></em> <em><a href="http://arxiv.org/abs/2112.06905">http://arxiv.org/abs/2112.06905</a></em></p>
<p>第一次将MoE应用到GPT3这种规格的模型上面，最终的效果也是在29个NLP任务中领先GPT3。最大的模型参数量是GPT3的7倍，但是训练消耗仅仅是1/3。有趣的是，他们用训练使用能量来衡量训练消耗。</p>
<p>**PS：**虽然模型结构和GShard完全类似，但是应用规模比其他的工作要大，所以有较高的参考价值。</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</font></strong></p>
<p><em><strong>Google, 2021</strong></em> <em><a href="https://arxiv.org/pdf/2006.16668.pdf">https://arxiv.org/pdf/2006.16668.pdf</a></em></p>
<p>第一次将MoE应用到transformer结构中，并且介绍了很多工程的细节，比如并行 以及 负载均衡。在工程方面，提出了一套API来处理MoE高效并行的实现，将模型结构和具体实现分离，并且在使用起来更符合语义层面的理解。值得深入研究，对工程实现方面的借鉴意义更大。</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Brainformers: Trading Simplicity for Efficiency</font></strong></p>
<p><em><strong>Deepmind, 2023</strong></em> <em><a href="http://arxiv.org/abs/2306.00008">http://arxiv.org/abs/2306.00008</a></em></p>
<p>提出一种新的FFN、Attn、Gate模块的排列方式，比正常的attention排列得到的模型更高效（原文quality and efficiency）。与同是MoE模型的GLaM相比，提升2倍的收敛速度，降低5倍的单步训练时长。</p>
<p>新的模型结构通过自动化程序挖掘出来；top-2 has demonstrated stronger empirical performance than top-1 gating；</p>
<blockquote>
<p>To avoid causal leakage in decoding mode, we suggest normalizing along the expert dimension for both token-based routing and expert-based routing. 因为在训练的过程中，其实是用全文做的normalize，但是decode的时候看不到全文。</p>
</blockquote>
<p>问题：在Token-based Routing Versus Expert-based Routing这章节里，为什么要做normalize？</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">ST-MoE: Designing Stable and Transferable Sparse Expert Models</font></strong></p>
<p><em><strong>Google Brain, Apr 2022</strong></em> <em><a href="http://arxiv.org/abs/2202.08906">http://arxiv.org/abs/2202.08906</a></em></p>
<p>主要针对MoE模型的训练稳定性，以及微调质量无法保证 这2个问题，提出了对应的解决方案，最终得到的模型叫做ST-MoE-32B。效果层面上，通过微调的MoE模型第一次在下游任务中得到SoTA。</p>
<p>主要通过模型结构以及loss的改动，增加训练的稳定性。并且对训练稳定性进行了深入的研究，定量分析了一些模型改动对稳定性的影响。可能是对MoE训练稳定性问题研究最多的一篇论文。</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Mixture-of-Experts with Expert Choice Routing</font></strong></p>
<p><em><strong>Google, Oct 2022</strong></em> <em><a href="https://arxiv.org/pdf/2202.09368.pdf">https://arxiv.org/pdf/2202.09368.pdf</a></em></p>
<p>文章的动机是解决不好的routing策略，导致expert训练不充分的问题。不同于正常的routing策略是token选择experts，本文中提出的策略是expert选择tokens。对比了Switch Transformer和GShard 2种方法，发现训练速度可以提高1倍，并且效果GLUE和SuperGLUE上表现更好。</p>
<p>第一次提出expert-based routing的方式，后来在BrainFormer中有被用到。为什么这种方式会更好呢，背后的直觉是什么？</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">From Sparse to Soft Mixtures of Experts</font></strong></p>
<p><em><strong>Google Deepmind, Aug 2023</strong></em> <em><a href="http://arxiv.org/abs/2308.00951">http://arxiv.org/abs/2308.00951</a></em></p>
<p>视觉方向的一篇MoE论文，主要解决的问题是：训练不稳定、token dropping、experts扩展、微调效果。提出一种完全可导的稀疏MoE结构（==正常的MoE不是也可导吗？==）本质上，这篇文章也提出了一种新的Routing策略，对比了Token-based和Expert-based这2种routing策略，在推理速度、expert数量扩展上都有不同程度的优势。</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Hash layers for large sparse models</font></strong></p>
<p><em><strong>Facebook AI, Jul 2021</strong></em> <em><a href="http://arxiv.org/abs/2106.04426">http://arxiv.org/abs/2106.04426</a></em></p>
<p>通过对token Hash选择expert，省去了gating网络，优于SwitchTransformer和传统的Transformer模型。同时探索了不同的hash算法对应的效果，并且分析hashing-routing方法的有效性。</p>
<blockquote>
<p>感觉上Hash方法更接近于随机去选择expert，为什么还能起到效果，很奇怪？Hash如何与语义关联的？</p>
</blockquote>
<p>正常的gating网络和expert需要一起进行训练，而在刚开始的时候gating网络是随机的，expert之间也没有什么区别，那么随着expert不断训练它所包含的知识也在改变，而gating网络去学习这种变化的mapping关系，可能会导致模型最后效果很差。PS：本质上还是如何将expert训练的更有差异化的问题。基于这个问题，这篇文章提出的hashing至少在训练过程中会更稳定，确定了hash算法，那么token和expert之间的mapping关系就定下来了。<em>PS:直觉上可以加快模型训练的速度。</em></p>
<p>这篇论文实验非常详细，感觉对于routing策略来说是一个很强的baseline。结论说明一点，固定的routing方法能得到更好的训练结果</p>
<p><strong><font style="color:royalblue;background-color:FloralWhite">Go Wider Instead of Deeper</font></strong></p>
<p><em><strong>National University of Singapore, Sep 2021</strong></em> <em><a href="http://arxiv.org/abs/2107.11817">http://arxiv.org/abs/2107.11817</a></em></p>
<p>MOE模型虽然保证计算量不增加太多，但是模型占用显存是显著增加了的。这篇论文中通过层间共享参数的机制，降低了显存的占用，并且得到的效果还不错（类似ALBERT结合MOE）。</p>
<p>把ALBERT中层共享参数的方法与MoE模型，然后声称是Wider Net。然而，计算过程还是会有多层的推理，每一层的layer norm使用的是不同的参数。有点标题党的嫌疑，并且测试模型的规模都比较小，没有太多可以借鉴的地方。</p>
<h3 id="adaptive-mixtures-of-local-experts论文阅读">Adaptive mixtures of local experts论文阅读<a hidden class="anchor" aria-hidden="true" href="#adaptive-mixtures-of-local-experts论文阅读">#</a></h3>
<blockquote>
<p>BP提出自1986年，这篇文章写自1991年。</p>
</blockquote>
<p>主要想解决的问题是：当BP用来训练一个解决多个不同问题的网络时，因为任务之间的干涉导致模型难以收敛和泛化。虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。</p>
<p>原来的公式：</p>
<p>$$
\mathbb{E}^c = {\parallel \vec{d^c}-\sum_i{p_i^c \vec{o_i^c}} \parallel}^2
$$</p>
<p>其中，$c$ 是case，$d$是目标输出，$p$ 是expert对应的gate weight，$o$ 是export对应的输出。</p>
<p>分析：如果其中1个expert的输出（$o_i$）改变，导致expert所有输出的调和超过了最终的输出（$\vec{d^c}$），那么所有expert梯度都会变为反方向。造成的结果就是，expert之间会互相干扰，导致收敛速度变慢，并且学习到的expert会倾向于合作的关系，共同作用于1个case并得到最终结果。</p>
<p>那么有没有方法expert之间更倾向于竞争关系呢。</p>
<blockquote>
<p>这个问题有2篇相关的论文：</p>
<ul>
<li>Learning piecewise control strategies inamo dular connectionist architecture</li>
<li>Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks</li>
</ul>
</blockquote>
<p>==TODO==：一种方法是给目标函数添加惩罚项，来奖励expert之间更多的竞争。</p>
<p>如果把loss改成如下：</p>
<p>$$
\mathbb{E}^c = \sum_i p_i^c{\parallel \vec{d^c}-\vec{o_i^c} \parallel}^2
$$</p>
<p>最直观的变化是，每个expert的输出单独拿出来和目标进行比较，所以不管gate网络和其他网络的输出是什么，每个expert得到的梯度方向只取决于自身的输出和目标的差值。但如果其中某个expert输出对应的error变小，那么gate网络对应该expert的概率就会增加。这点可以通过梯度 $\frac{\partial \mathbb{E}^c}{p_i}={\parallel \vec{d^c}-\vec{o_i^c} \parallel}^2$很明显的看到，$p_i$梯度大小取决于error平方的大小，梯度越大下降的越快。</p>
<p>新的目标函数对应每个expert的梯度如下：</p>
<p>$$
\frac{\partial \mathbb{E}^c}{\partial \vec{o_i^c}}=-2p_i^c(\vec{d^c}-\vec{o_i^c})
$$</p>
<p>此时会出现一个问题，当初期网络随机初始化后，可以认为每个expert对应的权重$p_i^c$是相同的，那么每个expert的梯度取决于error的大小，所以拟合最好的expert得到的梯度最小。为了解决这个问题，提出了新的目标函数如下：</p>
<p>$$
\mathbb{E}^c=-log\sum_i{p_i^c e^{-\frac12\parallel \vec{d^c}-\vec{o_i^c} \parallel^2}}
$$</p>
<p>此时每个expert输出对应的梯度为：</p>
<p>$$
\frac{\partial \mathbb{E}^c}{\partial \vec{o_i^c}}=-\Big<a href="%5Cvec%7Bd%5Ec%7D-%5Cvec%7Bo_j%5Ec%7D"> \frac{p_i^c e ^{-\frac12\parallel \vec{d^c}-\vec{o_i^c} \parallel^2}}{\sum_j{p_j^c e ^{-\frac12\parallel \vec{d^c}-\vec{o_j^c} \parallel^2}}} \Big</a>
$$</p>
<p>梯度的前部分确保了表现好的expert能够得到更快的训练。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://dawson-chen.github.io/posts/moe-scaling-law/">
    <span class="title">« Prev</span>
    <br>
    <span>Moe的Scaling Law</span>
  </a>
  <a class="next" href="https://dawson-chen.github.io/posts/ppo-practice/">
    <span class="title">Next »</span>
    <br>
    <span>PPO实践经验</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on x"
            href="https://x.com/intent/tweet/?text=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f&amp;title=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;summary=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;source=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f&title=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on whatsapp"
            href="https://api.whatsapp.com/send?text=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94%20-%20https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on telegram"
            href="https://telegram.me/share/url?text=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Moe(Mixtrue of Experts)技术调研 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Moe%28Mixtrue%20of%20Experts%29%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&u=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmoe-introduce%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://dawson-chen.github.io/">DawsonChen&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

        
    <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv">
        Page Views<span id="busuanzi_value_site_pv"></span>次
    </span>
    <span id="busuanzi_container_site_uv">
        Unique Visitors<span id="busuanzi_value_site_uv"></span>人次
    </span>
    </div></footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
