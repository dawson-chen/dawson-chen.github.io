<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>ChatGPT Plugins原理介绍和讨论 | DawsonChen&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="背景 让我们回顾以下过去的半个月里重要的AI发展。
事件 时间 介绍 公司 Visual ChatGPT 3-12 可以通过文本和图片聊天，甚至修改图片内容。 Microsoft GPT4发布 3-13 更大的ChatGPT模型，部分专业能力达到人类水平，可以接收图片输入。 OpenAI 365 Copilot 3-16 智能办公大杀器。 Microsoft 文心一言 3-16 中国版的ChatGPT Baidu ChatGPT plugin 3-23 可以使用工具的ChatGPT OpenAI HuggingGPT 3-30 可以使用HuggingFace中模型能力的ChatGPT Microsoft 很多评价说过去的几周是AI发展的Crazy Week，这种速度疯狂到甚至让人们开始担心AI会影响到社会和人类，并在公开信中呼吁暂停AI的研究。造成这种现象的原因可以理解为，一是基于ChatGPT的成功，二是行业内大量的关注。
个人认为，这其中ChatGPT plugin可以认为是对行业应用最有影响力的一个技术，也是继ChatGPT发布以来OpenAI发布的最重要的更新，可以简单的理解为OpenAI发布了对应ChatGPT的应用商店。对未来人工智能应用的形态也有一定启发，以前的AI模型的定位更多的是充当的一个单一的智能工具，具体到某个任务上，还需要人工协同才能完成；但是有了plugin这项技术，那么AI模型可以代替之前人工的部分，自主使用工具，从而端到端的完成某一项任务。这也是为什么一些基础的工作岗位很有可能会被新一代AI技术取代。
在网上已经有很多对ChatGPT plugin如何使用的介绍，但是比较少有对其实现原理进行解析的内容。这篇文章里我们主要分析一下它的原理，以及可能造成的影响。
必要性 首先说为什么语言模型要使用插件？随着语言模型的规模不断变大，各种涌现能力被相继发现，从而衍生出各种关于模型能力的研究。但谈到语言模型的应用，始终绕不开一个问题，就是模型无法获取外界的信息。也就是，一旦模型训练完成，后续的所有输出都来自于训练数据中学习到的知识。
大语言模型存在的问题可以总结为以下2点：
缺少最新数据的补充；
在不同的应用场景，对数据的需求也是不同的。在开放问答领域，可以是维基百科一类的数据。在特定业务领域，可能是公司内部的一些私人数据集。
缺少专业的能力；
大型语言模型对通用逻辑的理解是比较好的，比方说写一篇文章，与人聊天。但是涉及到特殊的专业，比方说作数学题、求公式的解，这类型问题对模型来说是有点难的。
虽然GPT4号称用了更大的模型，可以在一些专业领域得到类似于人类的效果甚至超越。但是从本质上来看，语言模型所采用的文字接龙训练方式，对于这类问题是非常不友好的。
或许随着模型变大，训练时间更长可以得到更好的效果，但是花费巨大训练出的GPT3在计算能力上远远达不到1970年代出现的计算器，本身就可以说明大模型技术是不足以解决专业推理问题的。
了解了以上模型存在的问题，就可以理解教模型使用插件的必要性了。PS：使用插件、使用工具，在不同的地方有不同的说法，但是是一件事情。
模型使用工具技术发展 在GPT3发布以后，就有一些AI模型使用插件的技术研究陆续出现，甚至有一些开源的框架在github上收获不错的关注。
想法的提出：MRKL System MRKL System（全名是Modular Reasoning, Knowledge and Language，论文链接，博客链接）由以色列的一家人工智能公司AI21推出，可以被认为是语言模型使用工具系统想法的提出者。虽然在此之前有WebGPT这类教模型使用浏览器的工作，但它是第一个提出将模型作为中枢，接入各种不同类型的插件来完成任务的工作。
从工作流程上来看，MRKL已经完全接近于ChatGPT plugin。MRKL认为这是一种跨越神经学派和符号学派的架构（neuro-symbolic architecture），各种插件可以被认为是符号系统，由神经学派的语言模型进行统一调用。
这篇论文中以使用计算器为例子，主要描述了如何将自然语言中的内容转换为API所需要的参数，文中提出语言模型few-shot在复杂的问题上性能有限，所以用Prompt tuning这种轻量化的微调技术提升转换的准确率。Prompt tuning技术是用特定训练好的非自然语言prompt来控制模型在特定任务中的生成表现，对应到MRKL中那就是每一个插件都需要训练一个特定的Prompt，虽然说有一定训练成本，但也算是一种比较好的解决思路。
可是文中对于最重要的问题：”怎么决定调用插件？“，这块的细节并没有太多的描述，也引出了关于大模型推理技术的发展。
Reasoning技术：ReACT 为了教会模型实用工具，一种方法是首先让模型具备推理的能力，从而能够模拟人使用工具的过程。应该说语言模型的训练方式和推理是不沾边的，但是语言模型的美妙之处就在于，当模型大小足够大的时候，它会诞生出很多出乎意料的能力，比方说推理能力。
大语言模型的推理能力通过Chain-of-thought体现出来，但是这种推理能力需要显式的Prompt进行引导。根据引导方式的不同产生出各种不同的技术，其本质上是对不同思维方式的模拟，这里我们只介绍比较典型的ReACT技术。
ReACT用强化学习的方式建模推理的过程，agent认为是一个可以使用各种工具的智能体，environment为所有可用插件构成的工具箱集合，action为可以使用的插件功能集合。而控制策略为语言模型中学习到的知识。一个典型的推理的流程如下图所示：">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/chatgpt-plugin/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/chatgpt-plugin/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            
            throwOnError: false
        });
    });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>

const config = {
    startOnLoad:true,
    theme: 'forest',
    themeVariables: {
        lineColor: "#fafafa"    
    },
    flowchart: {
        useMaxWidth:false,
        htmlLabels:true
        }
};
mermaid.initialize(config);


window.onload = () => {
    window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid'));
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="DawsonChen&#39;s Blog (Alt + H)">DawsonChen&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      ChatGPT Plugins原理介绍和讨论
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2023-04-07 20:43:43 +0800 CST'>April 7, 2023</span>&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/chatgpt-plugin.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h2 id="背景">背景<a hidden class="anchor" aria-hidden="true" href="#背景">#</a></h2>
<p>让我们回顾以下过去的半个月里重要的AI发展。</p>
<table>
<thead>
<tr>
<th>事件</th>
<th>时间</th>
<th>介绍</th>
<th>公司</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visual ChatGPT</td>
<td>3-12</td>
<td>可以通过文本和图片聊天，甚至修改图片内容。</td>
<td>Microsoft</td>
</tr>
<tr>
<td>GPT4发布</td>
<td>3-13</td>
<td>更大的ChatGPT模型，部分专业能力达到人类水平，可以接收图片输入。</td>
<td>OpenAI</td>
</tr>
<tr>
<td>365 Copilot</td>
<td>3-16</td>
<td>智能办公大杀器。</td>
<td>Microsoft</td>
</tr>
<tr>
<td>文心一言</td>
<td>3-16</td>
<td>中国版的ChatGPT</td>
<td>Baidu</td>
</tr>
<tr>
<td>ChatGPT plugin</td>
<td>3-23</td>
<td>可以使用工具的ChatGPT</td>
<td>OpenAI</td>
</tr>
<tr>
<td>HuggingGPT</td>
<td>3-30</td>
<td>可以使用HuggingFace中模型能力的ChatGPT</td>
<td>Microsoft</td>
</tr>
</tbody>
</table>
<p>很多评价说过去的几周是AI发展的Crazy Week，这种速度疯狂到甚至让人们开始担心AI会影响到社会和人类，并在公开信中呼吁暂停AI的研究。造成这种现象的原因可以理解为，一是基于ChatGPT的成功，二是行业内大量的关注。</p>
<p>个人认为，这其中ChatGPT plugin可以认为是对行业应用最有影响力的一个技术，也是继ChatGPT发布以来OpenAI发布的最重要的更新，可以简单的理解为OpenAI发布了对应ChatGPT的应用商店。对未来人工智能应用的形态也有一定启发，以前的AI模型的定位更多的是充当的一个单一的智能工具，具体到某个任务上，还需要人工协同才能完成；但是有了plugin这项技术，那么AI模型可以代替之前人工的部分，自主使用工具，从而端到端的完成某一项任务。这也是为什么一些基础的工作岗位很有可能会被新一代AI技术取代。</p>
<p>在网上已经有很多对ChatGPT plugin如何使用的介绍，但是比较少有对其实现原理进行解析的内容。这篇文章里我们主要分析一下它的原理，以及可能造成的影响。</p>
<h3 id="必要性">必要性<a hidden class="anchor" aria-hidden="true" href="#必要性">#</a></h3>
<p>首先说为什么语言模型要使用插件？随着语言模型的规模不断变大，各种涌现能力被相继发现，从而衍生出各种关于模型能力的研究。但谈到语言模型的应用，始终绕不开一个问题，就是模型无法获取外界的信息。也就是，一旦模型训练完成，后续的所有输出都来自于训练数据中学习到的知识。</p>
<p>大语言模型存在的问题可以总结为以下2点：</p>
<ul>
<li>
<p>缺少最新数据的补充；</p>
<p>在不同的应用场景，对数据的需求也是不同的。在开放问答领域，可以是维基百科一类的数据。在特定业务领域，可能是公司内部的一些私人数据集。</p>
</li>
<li>
<p>缺少专业的能力；</p>
<p>大型语言模型对通用逻辑的理解是比较好的，比方说写一篇文章，与人聊天。但是涉及到特殊的专业，比方说作数学题、求公式的解，这类型问题对模型来说是有点难的。</p>
<p>虽然GPT4号称用了更大的模型，可以在一些专业领域得到类似于人类的效果甚至超越。但是从本质上来看，语言模型所采用的文字接龙训练方式，对于这类问题是非常不友好的。</p>
<p>或许随着模型变大，训练时间更长可以得到更好的效果，但是花费巨大训练出的GPT3在计算能力上远远达不到1970年代出现的计算器，本身就可以说明大模型技术是不足以解决专业推理问题的。</p>
</li>
</ul>
<p>了解了以上模型存在的问题，就可以理解教模型使用插件的必要性了。PS：使用插件、使用工具，在不同的地方有不同的说法，但是是一件事情。</p>
<h2 id="模型使用工具技术发展">模型使用工具技术发展<a hidden class="anchor" aria-hidden="true" href="#模型使用工具技术发展">#</a></h2>
<p>在GPT3发布以后，就有一些AI模型使用插件的技术研究陆续出现，甚至有一些开源的框架在github上收获不错的关注。</p>
<h3 id="想法的提出mrkl-system"><strong>想法的提出：MRKL System</strong><a hidden class="anchor" aria-hidden="true" href="#想法的提出mrkl-system">#</a></h3>
<p>MRKL System（全名是Modular Reasoning, Knowledge and Language，论文<a href="https://arxiv.org/abs/2205.00445">链接</a>，博客<a href="https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system">链接</a>）由以色列的一家人工智能公司AI21推出，可以被认为是语言模型使用工具系统想法的提出者。虽然在此之前有WebGPT这类教模型使用浏览器的工作，但它是第一个提出将模型作为中枢，接入各种不同类型的插件来完成任务的工作。</p>
<p><img loading="lazy" src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c1c9b7cc490641a2a998832ffd0229ab~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<p>从工作流程上来看，MRKL已经完全接近于ChatGPT plugin。MRKL认为这是一种跨越神经学派和符号学派的架构（neuro-symbolic architecture），各种插件可以被认为是符号系统，由神经学派的语言模型进行统一调用。</p>
<p>这篇论文中以使用计算器为例子，主要描述了如何将自然语言中的内容转换为API所需要的参数，文中提出语言模型few-shot在复杂的问题上性能有限，所以用Prompt tuning这种轻量化的微调技术提升转换的准确率。Prompt tuning技术是用特定训练好的非自然语言prompt来控制模型在特定任务中的生成表现，对应到MRKL中那就是每一个插件都需要训练一个特定的Prompt，虽然说有一定训练成本，但也算是一种比较好的解决思路。</p>
<p>可是文中对于最重要的问题：”怎么决定调用插件？“，这块的细节并没有太多的描述，也引出了关于大模型推理技术的发展。</p>
<h3 id="reasoning技术react"><strong>Reasoning技术：ReACT</strong><a hidden class="anchor" aria-hidden="true" href="#reasoning技术react">#</a></h3>
<p>为了教会模型实用工具，一种方法是首先让模型具备推理的能力，从而能够模拟人使用工具的过程。应该说语言模型的训练方式和推理是不沾边的，但是语言模型的美妙之处就在于，当模型大小足够大的时候，它会诞生出很多出乎意料的能力，比方说推理能力。</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f9802e42df4b47b38052f77048cf4bc6~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<p>大语言模型的推理能力通过Chain-of-thought体现出来，但是这种推理能力需要显式的Prompt进行引导。根据引导方式的不同产生出各种不同的技术，其本质上是对不同思维方式的模拟，这里我们只介绍比较典型的ReACT技术。</p>
<p>ReACT用强化学习的方式建模推理的过程，agent认为是一个可以使用各种工具的智能体，environment为所有可用插件构成的工具箱集合，action为可以使用的插件功能集合。而控制策略为语言模型中学习到的知识。一个典型的推理的流程如下图所示：</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a73d01fcff34b82a86d03f2b71fb5c8~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<p>ReACT推理流程可以分为Thought→Action→Observation→Thought 这样的循环，具体如何实现在本文的后续内容中会进行分析。</p>
<h3 id="使用工具的语言模型toolformer"><strong>使用工具的语言模型：Toolformer</strong><a hidden class="anchor" aria-hidden="true" href="#使用工具的语言模型toolformer">#</a></h3>
<p>与利用推理能力使用工具的思路不同，Toolformer是在训练语言模型过程中，使模型学习在适当位置调用相关API，并用API结果辅助后续的文本生成。在Toolformer训练过程中，数据是<code>Pittsburgh is also known as [QA(What …?→ Steel City)] the Steel City.</code>这种格式，如果是人去标注数据，首先需要找到API的放置位置，判断标准是API结果对后续文本生成有帮助，并且上文中有API需要的参数；然后是将API的标识、输入、输出以<code>[QA(What …?→ Steel City)]</code>这种形式插入到训练文本中。</p>
<p>注意，模型训练仍然采用典型的文字接龙方式，所以对原本语言模型的能力并没有损失。论文中提出一种利用LLM去自动标注这种数据的方式，和远程监督类似，步骤如下图：</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40bb5084a59a43d987bc9cd9d5ec1853~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<h3 id="工具提出langchain"><strong>工具提出：LangChain</strong><a hidden class="anchor" aria-hidden="true" href="#工具提出langchain">#</a></h3>
<p>LangChain差不多是在2022年底提出的，那时候也是LLMs技术急剧发展的阶段。其核心是做一个基于LLMs的工具，基本上所有需要用LLMs实现的功能都可以在里面找到对应的工具。其中一个主要的能力，就是教会模型使用工具，并且接入方式和扩展性都非常好。除此之外还有很多好用的工具，比如：Prompt管理、Memory。名字中的Chain表示其核心设计思路是将不同的模块链接在一起。</p>
<p>详细的文档见<a href="https://github.com/hwchase17/langchain">链接</a>。</p>
<p><a href="https://github.com/hwchase17/langchain">https://github.com/hwchase17/langchain</a></p>
<p>LangChain中有很多有用的工具，包括各种搜索引擎Bing、Google、SerpAPI（google问答）、wiki等。还有一个更有趣的是Human as a tool插件，可以使语言模型必要时询问人类，从而模拟各种各样的功能。</p>
<p>在原理部分我们会介绍它的工作流程。</p>
<h2 id="chatgpt-plugin的原理">ChatGPT plugin的原理<a hidden class="anchor" aria-hidden="true" href="#chatgpt-plugin的原理">#</a></h2>
<p>ChatGPT plugin是作为一个产品发布的，并且功能还没有完全开放，因此其实现原理细节也不是很清楚。但是由于LangChain中已经实现了类似的功能，并且2者的发布时间比较相近，所以有理由相信2者在原理上是有相似的。</p>
<p>下面我们分2部分，首先分析LangChain中使用工具的原理；第二部分通过比较2者的区别，得出一些关于ChatGPT plugin原理的猜想。</p>
<p><img loading="lazy" src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ece2e5b64a4f40f8a2e4d538f37cdbfe~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<h3 id="langchain的工作流程">LangChain的工作流程<a hidden class="anchor" aria-hidden="true" href="#langchain的工作流程">#</a></h3>
<p>首先让我们看一个例子：</p>
<pre tabindex="0"><code>from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
tools = load_tools([&#34;serpapi&#34;, &#34;llm-math&#34;], llm=llm)
agent = initialize_agent(tools, llm, agent=&#34;zero-shot-react-description&#34;, verbose=True)

response = agent.run(&#34;Who is Leo DiCaprio&#39;s girlfriend? What is her current age raised to the 0.43 power?&#34;)
</code></pre><p>代码里涉及到的关键概念：</p>
<ul>
<li>
<p>llm: BaseLLM</p>
<p>LangChain中对一系列开源模型接口的封装，其主要作用是统一不同的模型API，使接口更易使用，包括像提供缓存等一些基础功能。</p>
</li>
<li>
<p>tools: List[BaseTool]</p>
<p>对工具的封装，LangChain中对工具的封装是比较简单的，因此保证了比较高的自由度，唯一的要求是输入输出必须是文字形式，<code>def run(self, tool_input: str) -&gt; str</code>。</p>
<p>自定义Tool只需要3个参数:</p>
<ul>
<li>name：工具的标识名称；</li>
<li>description: 工具的自然语言描述；</li>
<li>func: 功能执行函数，输入输出都为单个的文本。</li>
</ul>
</li>
<li>
<p>agent: Agent</p>
<p>内部使用了一个LLM决定使用什么工具，LangChain中agent的实现有2种，一种是ReACT类型，一种是self-Ask类型，因为后者只能使用qa类型的工具，如果任务涉及不同类型的工具，最好用ReACT类型。其中比较常用的是<code>zero-shot-react-description</code>，其中zero-shot表示推理引导Prompt里不包括示例，description表示LLM在决定调用什么工具的信息都来自于工具的description字段。</p>
<p>注意，针对特定的任务可以设计针对性的few-shot提升agent的效果。</p>
</li>
</ul>
<p>介绍完上面的概念，让我们看这个例子是怎么工作的。首先根据提供的工具，agent会生成引导Prompt，对于上面的例子，prompt是下面的样子：</p>
<p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a1234d5ea48c47838dd4dc8d8477ccca~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<p>其中<code>{input}</code>为用户Query的占位符号，<code>{agent_scratchpad}</code>为模型生成填充的位置。下面说明一个循环Thought→Action→Observation→Thought的详细步骤：</p>
<ul>
<li>
<p>生成Thought，对应的Prompt（只显示Begin!之后的部分）：</p>
<blockquote>
<p>Question: Who is Leo DiCaprio&rsquo;s girlfriend? What is her current age raised to the 0.43 power?</p>
<p>Thought:</p>
</blockquote>
</li>
<li>
<p>LLM输出：</p>
<blockquote>
<p>I need to find out who Leo DiCaprio&rsquo;s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: &ldquo;Leo DiCaprio girlfriend&rdquo;</p>
<p>Observation:</p>
</blockquote>
<p>其中<code>Observation:</code> 为语言模型生成的终止符。</p>
</li>
<li>
<p>根据模型选择的Action，调用Search[ &ldquo;Leo DiCaprio girlfriend&rdquo;]得到结果：</p>
<blockquote>
<p>Leonardo DiCaprio has split from girlfriend Camila Morrone. Getty. The Titanic actor hasn&rsquo;t been in a relationship with a woman over the age of &hellip;</p>
</blockquote>
</li>
<li>
<p>第二次生成Thought，对应的Prompt如下：</p>
<blockquote>
<p>Question: Who is Leo DiCaprio&rsquo;s girlfriend? What is her current age raised to the 0.43 power?Thought: I need to find out who Leo DiCaprio&rsquo;s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: &ldquo;Leo DiCaprio girlfriend&rdquo; Observation: Leonardo DiCaprio has split from girlfriend Camila Morrone. Getty. The Titanic actor hasn&rsquo;t been in a relationship with a woman over the age of &hellip; Thought:</p>
</blockquote>
</li>
<li>
<p>继续这个循环直到输出最终结果，或者超过最大循环次数。</p>
</li>
</ul>
<p>最后，完整的推理过程如下：</p>
<p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/403cd9564d10443cb850f3ef086a307b~tplv-k3u1fbpfcp-watermark.image?" alt="image.png"  />
</p>
<h3 id="chatgpt-plugin的原理猜想未完">ChatGPT plugin的原理猜想（未完）<a hidden class="anchor" aria-hidden="true" href="#chatgpt-plugin的原理猜想未完">#</a></h3>
<p>根据OpenAI官方的介绍，ChatGPT plugin在设计上要比LangChain精细的多，主要体现：</p>
<ul>
<li>每个插件可以有多个API接口；</li>
<li>接口可以定义参数类型和格式；</li>
<li>描述的长度更大；</li>
</ul>
<p>并且按照描述，对于自定义的新插件使用起来也是zero-shot的方式，所以其实现难度要更高。根据一些相关文献，可以猜想出以下可能的实现方式：</p>
<p>待补充…</p>
<h2 id="讨论">讨论<a hidden class="anchor" aria-hidden="true" href="#讨论">#</a></h2>
<h3 id="应用场景">应用场景<a hidden class="anchor" aria-hidden="true" href="#应用场景">#</a></h3>
<p>手机行业</p>
<p>机器人行业</p>
<h3 id="对劳动力的影响">对劳动力的影响<a hidden class="anchor" aria-hidden="true" href="#对劳动力的影响">#</a></h3>
<p>Zippia. &ldquo;23+ Artificial Intelligence And Job Loss Statistics [2023]: How Job Automation Impacts the Workforce&rdquo; <a href="http://zippia.com/">Zippia.com</a>. Feb. 7, 2023, <a href="https://www.zippia.com/advice/ai-job-loss-statistics/">https://www.zippia.com/advice/ai-job-loss-statistics/</a></p>
<p>这段内容摘抄自上面这篇博客，作者是一家求职网站的创始人。文中用到的并非严格的统计方法，因此数字有夸大的嫌疑，这里过滤掉一些数字表示的结论，总结出一些未来可能的趋势，供大家参考。</p>
<ul>
<li>被AI技术淘汰掉旧的劳动力，不太可能找到更高薪的工作；</li>
<li>从长远来看，AI技术发展对全球经济是有促进作用的。但如果造成大规模的失业潮，就另当别论；</li>
<li>大多数公司会使用不同程度的AI技术来提升效率，也会是AI技术发展最直接的受益者；</li>
<li>当AI技术、机器人替代掉大多数的工作，很多人会没有工作，从而需要政府救济维持生活；</li>
<li>最有可能被取代的工作类型：客服、会计、前台接待、制造类、零售接待、数据分析 等；</li>
<li>最难被取代的工作类型：HR、作家、律师、管理者、科学家、人文类 等；</li>
</ul>
<h2 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h2>
<ol>
<li><a href="https://python.langchain.com/en/latest/">Welcome to LangChain — 🦜🔗 LangChain 0.0.132</a></li>
<li>MRKL System <a href="https://arxiv.org/pdf/2205.00445.pdf">2205.00445.pdf (arxiv.org)</a></li>
<li>REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS</li>
<li>Toolformer：<a href="https://arxiv.org/pdf/2302.04761.pdf">*</a><a href="https://arxiv.org/pdf/2302.04761.pdf*">https://arxiv.org/pdf/2302.04761.pdf*</a></li>
<li>ReACT：<a href="https://arxiv.org/pdf/2210.03629.pdf">https://arxiv.org/pdf/2210.03629.pdf</a></li>
<li><a href="https://www.youtube.com/@aiadvantage">YouTube@aiadvantage</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">DawsonChen&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
