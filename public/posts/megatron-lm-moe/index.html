<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Megatron Lm Moe | DawsonChen&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。
最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue&hellip; i&rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。
但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。
MoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：
%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bumpX&#39; } } }%% graph LR x[&#34;X(n-1)&#34;] --&gt; p1[&#34;.&#34;] --&gt; input_ln[&#34;Layer Norm&#34;] input_ln --&gt; attn[&#34;Self Attention&#34;] attn --&gt; plus1((&#43;)) p1 --&gt; plus1 plus1 --&gt; p2[&#34;.&#34;] --&gt; attn_ln[&#34;Layer Norm&#34;] subgraph MoE Layer expert1[&#34;Expert 1&#34;] expert2[&#34;Expert 2&#34;] expert_dot[&#34;...&#34;] expertk[&#34;Expert K&#34;] end attn_ln -.-&gt; expert2[&#34;Expert 2&#34;] attn_ln --&gt; expert1[&#34;Expert 1&#34;] attn_ln -.">
<meta name="author" content="Me">
<link rel="canonical" href="https://dawson-chen.github.io/posts/megatron-lm-moe/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://dawson-chen.github.io/posts/megatron-lm-moe/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Megatron Lm Moe" />
<meta property="og:description" content="MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。
最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue&hellip; i&rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。
但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。
MoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：
%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bumpX&#39; } } }%% graph LR x[&#34;X(n-1)&#34;] --&gt; p1[&#34;.&#34;] --&gt; input_ln[&#34;Layer Norm&#34;] input_ln --&gt; attn[&#34;Self Attention&#34;] attn --&gt; plus1((&#43;)) p1 --&gt; plus1 plus1 --&gt; p2[&#34;.&#34;] --&gt; attn_ln[&#34;Layer Norm&#34;] subgraph MoE Layer expert1[&#34;Expert 1&#34;] expert2[&#34;Expert 2&#34;] expert_dot[&#34;...&#34;] expertk[&#34;Expert K&#34;] end attn_ln -.-&gt; expert2[&#34;Expert 2&#34;] attn_ln --&gt; expert1[&#34;Expert 1&#34;] attn_ln -." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dawson-chen.github.io/posts/megatron-lm-moe/" />
<meta property="og:image" content="https://dawson-chen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-25T23:40:02+08:00" />
<meta property="article:modified_time" content="2024-03-25T23:40:02+08:00" /><meta property="og:site_name" content="ExampleSite" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dawson-chen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E" />
<meta name="twitter:title" content="Megatron Lm Moe"/>
<meta name="twitter:description" content="MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。
最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue&hellip; i&rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。
但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。
MoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：
%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bumpX&#39; } } }%% graph LR x[&#34;X(n-1)&#34;] --&gt; p1[&#34;.&#34;] --&gt; input_ln[&#34;Layer Norm&#34;] input_ln --&gt; attn[&#34;Self Attention&#34;] attn --&gt; plus1((&#43;)) p1 --&gt; plus1 plus1 --&gt; p2[&#34;.&#34;] --&gt; attn_ln[&#34;Layer Norm&#34;] subgraph MoE Layer expert1[&#34;Expert 1&#34;] expert2[&#34;Expert 2&#34;] expert_dot[&#34;...&#34;] expertk[&#34;Expert K&#34;] end attn_ln -.-&gt; expert2[&#34;Expert 2&#34;] attn_ln --&gt; expert1[&#34;Expert 1&#34;] attn_ln -."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dawson-chen.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Megatron Lm Moe",
      "item": "https://dawson-chen.github.io/posts/megatron-lm-moe/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Megatron Lm Moe",
  "name": "Megatron Lm Moe",
  "description": "MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。\n最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue\u0026hellip; i\u0026rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。\n但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。\nMoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：\n%%{ init: { \u0026#39;flowchart\u0026#39;: { \u0026#39;curve\u0026#39;: \u0026#39;bumpX\u0026#39; } } }%% graph LR x[\u0026#34;X(n-1)\u0026#34;] --\u0026gt; p1[\u0026#34;.\u0026#34;] --\u0026gt; input_ln[\u0026#34;Layer Norm\u0026#34;] input_ln --\u0026gt; attn[\u0026#34;Self Attention\u0026#34;] attn --\u0026gt; plus1((+)) p1 --\u0026gt; plus1 plus1 --\u0026gt; p2[\u0026#34;.\u0026#34;] --\u0026gt; attn_ln[\u0026#34;Layer Norm\u0026#34;] subgraph MoE Layer expert1[\u0026#34;Expert 1\u0026#34;] expert2[\u0026#34;Expert 2\u0026#34;] expert_dot[\u0026#34;...\u0026#34;] expertk[\u0026#34;Expert K\u0026#34;] end attn_ln -.-\u0026gt; expert2[\u0026#34;Expert 2\u0026#34;] attn_ln --\u0026gt; expert1[\u0026#34;Expert 1\u0026#34;] attn_ln -.",
  "keywords": [
    
  ],
  "articleBody": "MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。\n最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue… i’m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。\n但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。\nMoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：\n%%{ init: { 'flowchart': { 'curve': 'bumpX' } } }%% graph LR x[\"X(n-1)\"] --\u003e p1[\".\"] --\u003e input_ln[\"Layer Norm\"] input_ln --\u003e attn[\"Self Attention\"] attn --\u003e plus1((+)) p1 --\u003e plus1 plus1 --\u003e p2[\".\"] --\u003e attn_ln[\"Layer Norm\"] subgraph MoE Layer expert1[\"Expert 1\"] expert2[\"Expert 2\"] expert_dot[\"...\"] expertk[\"Expert K\"] end attn_ln -.-\u003e expert2[\"Expert 2\"] attn_ln --\u003e expert1[\"Expert 1\"] attn_ln -.-\u003e expert_dot[\"...\"] \u0026 expertk[\"Expert K\"] expert1 --\u003e plus((+)) plus --\u003e plus2((+)) expert2 \u0026 expert_dot \u0026 expertk -.-\u003e plus((+)) plus2 --\u003e x2[\"X(n)\"] p2 --\u003e plus2 classDef nodeNoBorder fill:#ffffff,stroke:#000000,stroke-width:0px; class expert_dot nodeNoBorder class x nodeNoBorder class x2 nodeNoBorder class p1 nodeNoBorder class p2 nodeNoBorder class plus nodeNoBorder class plus1 nodeNoBorder class plus2 nodeNoBorder 大多数情况下，MoE层是应用在MLP中的，也就是每个expert代表了一个MLP层。MoE并没有引入新的层，除了一个Router Network用来计算token和expert之间的匹配分数。\n看起来MoE模型实现上和稠密网络并没有太大的区别，从计算流程上来看确实是这样，下面介绍一个MoE layer最简单的实现（transformer中的MixtralSparseMoeBlock）的过程：\nclass MixtralSparseMoeBlock(nn.Module): def __init__(self, config): super().__init__() self.hidden_dim = config.hidden_size self.ffn_dim = config.intermediate_size self.num_experts = config.num_local_experts self.top_k = config.num_experts_per_tok # gating self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False) self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)]) def forward(self, hidden_states: torch.Tensor) -\u003e torch.Tensor: \"\"\" \"\"\" # 计算routing score router_logits = self.gate(hidden_states) routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float) # 根据topk选出每个token对应的激活专家 routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1) routing_weights /= routing_weights.sum(dim=-1, keepdim=True) final_hidden_states = [] # 在每个expert上计算选中的token for expert_idx in range(self.num_experts): expert_layer = self.experts[expert_idx] current_state = select_tokens(hidden_states, selected_experts) if current_state.shape[0] == 0: continue # 计算expert的输出 current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None] final_hidden_states.append(current_hidden_states) # 重组得到最后输出\tfinal_hidden_states = concat(final_hidden_states, selected_experts) return final_hidden_states, router_logits 注：这里省略了一些不必要的计算细节，只保留对过程理解有用的部分。比如：select_tokens，concat是为了简化而虚构的函数。\n上面的实现方法将所有的expert都加载到显存中，并且在计算expert的时候使用串行的方式。显然因为每个token计算的时候并不会激活所有的expert，虽然在实际训练的过程中每次的输入里包含很多个token，也就是可以保证不会有完全空闲的expert。但是expert上的计算压力明显要低于网络中其余部分，从而造成资源浪费。另外，在实际推理部署的时候，expert往往是分布在不同的设备上的，这就涉及多机通讯的问题。并且，router往往还要考虑负载均衡的问题。这些问题都给实现MoE的过程中增加了困难，下面看megatron是如何解决这些问题的。\n这里分为3大块去分析整个实现过程，首先是expert的并行方式，其次是Router的设计，最后是Dispatcher。\nExpert Parallel EP(Expert Parallel)是MoE特有的并行方式，其核心是将expert在多个DP模型副本之间共享，从而实现节约显存的目的。可以说没有EP，那么大规模的MoE模型训练是完全不可能实现的。在介绍EP的划分方式之前，我们先来看一下megatron中并行组的划分方式。\nMegatron中常规通讯组划分 首先我们来说一下没有EP的时候通讯组是怎么划分的。megatron中有TP，PP，DP的3种并行方式，这3种并行方式对应的通讯量大小排序是：TP\u003eDP\u003ePP。而在GPU集群里，设备内部的通讯带宽远大于设备间的带宽。因此通讯组划分原则就是，尽量让TP和DP通讯发生在设备内部，而PP组进行跨设备通讯。\n如果不太理解这个分配方式，可以直接看下面这个例子。我会在示意图里面标记出每个GPU对应的TP DP PP的组，以及上面存放了对应模型的那部分结构。PS：顺便提一下，megatron中的并行组划分代码在写的时候应该是没有考虑可读性，如果想更快的看懂这块代码，在看之前最好也画一个这样的图对照着。\n假设，现在有2台8卡的A100机器，需要训练一个8层的transformer模型，并行设置TP=2，PP=4，DP=2。那么示意图如下：\nnotions*说明：\n$M_{1:8}^1[1]$​，下标表示模型的1~8层参数，上标表示TP的第一个参数切片，方括号里表示第一个模型副本。 事实上，PP组的第一个节点和最后一个节点还需要保存embedding和unembedding的矩阵，这里为了方便省略掉了。\nMoE layer通讯组划分 通过上面这个例子我们知道了正常的Dense模型在并行情况下是如何切分的，那如果是MoE模型会是怎么样的呢？我们把上一个例子稍微改动一下，把Dense改为MoE模型，每一层transformer里面有8个expert。设置EP=2，意味着每2个模型副本间共享一个完整的MoE层。\n我们以1:2层为例，看一下expert会如何进行保存，为了方便我们用一个表格说明。因为PP=4，所以前2层的参数会存放在第1到4个节点上面，因此我们只列出这4个节点，以及节点上面存放的模型参数和expert参数。\n节点 模型参数 expert参数 1 $M^1_{1:2}[1]$ $Experts_{1:4}^1$ 2 $M^2_{1:2}[1]$ $Experts_{1:4}^2$ 3 $M^1_{1:2}[2]$ $Experts_{5:8}^1$ 4 $M^2_{1:2}[2]$ $Experts_{5:8}^2$ notions*说明：\n$Experts_{1:4}^1$中，下标表示第1~4个expert对应的参数，上标表示TP的第1个切片。 那么在EP并行的情况下，设备之间的通讯需求分为下面2个部分：\ntoken分发；\n通过上面表格可以看到，节点1-4共享一套完整的expert参数。(1, 2), (2, 3)是2个TP组，意味着这2个节点上的数据是相同的。但因为sequence parallel的存在，组内会在sequence方向上进行拆分输入。(1, 2)和 (2, 3)这2个组之间分别组成DP组，所以输入数据是不同的。这也就导致了节点1-4每个节点上面的token都是不一样的，所以在进行路由前，需要把这几个节点上的token都gather起来，再进行全局的分配。\n参数更新；\n因为EP的存在，expert参数和其他参数的DP组是不一样的，因此，要把存放有相同expert参数的节点放到一个expert独有的DP组里面。\n在megatron.core.parallel_state#initialize_model_parallel 中，上面2个组分别对应了变量_TENSOR_AND_EXPERT_PARALLEL_GROUP和_DATA_MODULO_EXPERT_PARALLEL_GROUP。\nRouter 路由是MoE中最重要的一环，决定了token与expert之间的对应关系。路由方式不光决定了模型的效果，同时也与负载均衡特性息息相关。按照路由的主体可以将路由方式分为3大类，分别是：token-based、expert-based、global assignment，大多数已有的路由方式都可以归纳到这个分类体系下。\n%%{ init: { 'flowchart': { 'curve': 'natural' } } }%% flowchart LR a[\"路由方式\"] a --\u003e b[\"token-based\"] \u0026 c[\"expert-based\"] \u0026 d[\"global assignment\"] b --\u003e e[\"hash\"] \u0026 f[\"RL\"] \u0026 g[\"topK\"] megatron中支持了2种路由方式，分别是TopK和global assignment，下面我们分别介绍2种方法的实现。\nglobal assignment global assignment将token和expert之间的匹配当做一个全局最优的线性匹配问题，这样做的好处有：1. 在训练过程中，可以做到给每个expert分配相同的token，不需要进行负载均衡；2. 对于routing collapse问题有一定的抑制作用，因为会有token分配到次优的expert上面。\nglobal assignment有很多种不同的解法，通常可能会想到的是Hungarian Algorithm，但是因为其并不能很好利用GPU的并行特点，下面介绍2种对于GPU计算友好的算法。\n拍卖行算法 在global assignment第一次被提出的论文《BASE Layers: Simplifying Training of Large, Sparse Models》中，就使用了拍卖行算法作为问题的实现方式。这个算法通过模拟拍卖的过程计算全局最优，在开源框架fairseq中实现了该算法的源码，这里在源码的基础上加了一些必要的注解帮助理解算法的过程。\ndef balanced_assignment(scores, max_iterations=100): # scores [8, 80] 8 experts, 80 jobs num_workers, num_jobs = scores.size() jobs_per_worker = num_jobs // num_workers value = scores.clone() # 每个job对每个worker的价值，刚开始出价是0，所以等于scores iterations = 0 cost = scores.new_zeros(1, num_jobs) # 每个job上面的标价，初始为0 jobs_with_bids = zeros(num_workers).bool() # 每个worker绑定的job数 while not jobs_with_bids.all(): # top_values, top_index [8, 11] # value表示job对worker的竞标价值：job对worker的价值 - 商品的报价 # 商品的价值初始为0 top_values, top_index = topk(value, k=jobs_per_worker + 1, dim=1) # worker进行加注 # 加注的量取决于当前job的竞标价值和次优价值之间的差异； # 显然这种规则可以避免过度的加注 bid_increments = top_values[:, :-1] - top_values[:, -1:] + eps # 每次下注只下最高的jobs_per_worker个任务，也就是在最理想的情况下，可以一次中标全部 bids = scatter( zeros(num_workers, num_jobs), dim=1, index=top_index[:, :-1], src=bid_increments ) if 0 \u003c iterations \u003c max_iterations: # If a worker won a job on the previous round, put in a minimal bid to retain # the job only if no other workers bid this round. bids[top_bidders, jobs_with_bids] = eps # Find the highest bidding worker per job # top_bids, top_bidders [1, 80] # 中标情况 top_bids, top_bidders = bids.max(dim=0) jobs_with_bids = top_bids \u003e 0 top_bidders = top_bidders[jobs_with_bids] # Make popular items more expensive cost += top_bids # 更新job的标价 value = scores - cost # 更具新的价值，重新计算每个worker和job之间的价值 if iterations \u003c max_iterations: # If a worker won a job, make sure it appears in its top-k on the next round # 如果竞标中了，把对应的value设置成无穷大，保证下一轮还会竞标 value[top_bidders, jobs_with_bids] = ∞ else: value[top_bidders, jobs_with_bids] = scores[top_bidders, jobs_with_bids] iterations += 1 return top_index[:,:-1].reshape(-1) 如果对该方法感兴趣，在论文《Auction Algorithms for Network Flow Problems: A Tutorial Introductionl》中可以找到收敛到最优点的证明。\nsinkhorn算法实现 相比于sinkhorn算法，它的一种特殊例子Wasserstein metric可能更出名一点，大名鼎鼎的WGAN中的W所代表的就是它。Wasserstein metric可以理解为两个不同分布之间的最短距离，同时也是Optimal Transport问题的最优解。\n什么是Optimal Transport？我们可以举一个例子：假设你有10个仓库在不同的位置，然后你有5个顾客需要从你这里进货。每个仓库中的货物数量用向量$c\\in \\mathbf R^{10}$c表示，每个顾客需要的货物用向量$r\\in \\mathbf R^5$表示，c和r可以被看成2个分布。进货的成本可以被表示为一个矩阵$M \\in \\mathbf R^{10\\times 5}$，同样任意一种进货的方式可以被表示为$P\\in \\mathbf R^{10\\times 5}$。r和c之间的Optimal Transport任务可以看成找到整体成本最小的进货方式$P^*$，并且此时的进货成本可以被看做是Wasserstein metric。\n在这个例子中，Optimal Transport的任务可以形式化写成下面这种方式： $$ d(r, c) = \\underset{valid\\ P}{min} \\sum_{i,j}{P_{ij}M_{ij}} $$ sinkhorn算法在此基础上加入P的信息熵作为一个限制项，确保配货方式不会落入极端情况。对应到这面这个例子中，你可能并不想出现所有人都去一个仓库进货的情况。 $$ d^{\\lambda}(r, c) = \\underset{valid\\ P}{min} \\sum_{i,j}{P_{ij}M_{ij}} + \\frac{1}{\\lambda}h(P) $$ 该算法的求解方法如下：\ngiven: $M$, $\\mathbf{r}$, $\\mathbf{c}$ and $\\lambda$ initialize: $P_\\lambda = e^{-\\lambda C}$ repeat\nscale the rows such that the row sums match $\\mathbf{r}$ scale the columns such that the column sums match $\\mathbf{c}$ until convergence 回到MoE的router任务中，token和expert之间的最优匹配可以被看成是在token上的分布与expert上均匀分布之间的最优传输距离。因此可以用sinkhorn求解，但是我们并不关心传输距离，而是可以把行动矩阵$P$看做是一个加了均衡负载约束的喜好分布。\nMegatron中使用的sinkhorn主要是为了得到全局最优分配矩阵，因此做了一些简化，与标准实现会有差异。\ndef sinkhorn(cost: torch.Tensor, tol: float = 0.0001): \"\"\"Sinkhorn based MoE routing function\"\"\" # 这里给的cost其实是logits，代表token和expert之间的匹配程度 cost = torch.exp(cost) # sinkhorn距离的最优解中的 $\\alpha$，$\\beta$ 分别是这里的d0和d1 d0 = torch.ones(cost.size(0), device=cost.device, dtype=cost.dtype) d1 = torch.ones(cost.size(1), device=cost.device, dtype=cost.dtype) eps = 0.00000001 error = 1e9 d1_old = d1 ## 原始分布和目标分布都是均匀分布，所以用1 / d0.size(0) 和 1 / d1.size(0) 表示 while error \u003e tol: d0 = (1 / d0.size(0)) * 1 / (torch.sum(d1 * cost, 1) + eps) d1 = (1 / d1.size(0)) * 1 / (torch.sum(d0.unsqueeze(1) * cost, 0) + eps) error = torch.mean(torch.abs(d1_old - d1)) d1_old = d1 return d1 * cost * d0.unsqueeze(1) topK topK的实现与transformers里MixtralSparseMoeBlock的实现类似，根据router输出的logits选出每个token对应的前k个expert，并用softmax计算出对应的prob，作为最终计算结果的调和参数。\n代码对应如下：\n辅助loss的最佳实现 Router里常见的辅助loss有2种，分别是load-banlance loss和z-loss。前者是为了应对route collapse问题，就是让router的结果更加的均匀，不会出现集中在个别expert上的情况。后者是为了防止gating网络计算的logits过大导致Router收敛变慢的情况。这2个loss都是在gating网络计算的logits上面进行计算得到的，loss的计算方法也没有什么特殊的，只是介绍一下loss生效的方式。\n常规的实现方法是将每一个gating网络上面计算的logits收集起来，在模型推理完后计算对应的loss，并加到言模型的交叉熵loss上面。transformers中的switch_transformers就是这样实现的，对应代码如下：\n这种方式的问题是如果开启了PP，每个stage都需要将对应的logits传递给下一个stage，当然这样做也没有太多问题。但是megatron里使用了一种更加简洁的方式，给人以耳目一新的感觉。\n首先，megatron的实现方式不需要传递中间变量，而是将loss当做网络的一部分。这里我们从megatron中摘抄一段z-loss的实现代码。\ndef z_loss_func(logits, z_loss_coeff): z_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff return z_loss class Router(MegatronModule): ... def apply_z_loss(self, logits): if self.config.moe_z_loss_coeff is not None: z_loss = z_loss_func(logits, self.config.moe_z_loss_coeff) logits = MoEAuxLossAutoScaler.apply(logits, z_loss) return logits def forward(self,): ... logits = self.apply_z_loss(logits) ... # l 是的，就是这么简单，z_loss_func函数接收logits并返回对应的loss，并且结果中包含了设定好的loss因子。loss并没有被返回并收集，而是直接作为网络的一个计算步骤，所以看起来MoEAuxLossAutoScaler是loss生效的关键。\n下面是MoEAuxLossAutoScaler的代码片段：\nclass MoEAuxLossAutoScaler(torch.autograd.Function): main_loss_backward_scale: torch.Tensor = torch.tensor(1.0) @staticmethod def forward(ctx, output: torch.Tensor, aux_loss: torch.Tensor): ctx.save_for_backward(aux_loss) return output @staticmethod def backward(ctx, grad_output: torch.Tensor): (aux_loss,) = ctx.saved_tensors aux_loss_backward_scale = MoEAuxLossAutoScaler.main_loss_backward_scale scaled_aux_loss_grad = torch.ones_like(aux_loss) * aux_loss_backward_scale return grad_output, scaled_aux_loss_grad @staticmethod def set_loss_scale(scale: torch.Tensor): MoEAuxLossAutoScaler.main_loss_backward_scale = scale 看起来也很简单，下面分析一下。MoEAuxLossAutoScaler是一个torch.autograd.Function的类，也就是意味着pytorch可以根据自动微分功能计算对应的梯度。forward函数中接收logits和aux_loss 2个参数，所以backward函数必须返回2个梯度向量，分别对应2个输入。backward函数接收1个向量，因为只有logits参与了后续的计算。backward里做了2件事情，分别是将logits的参数透传给上一个运算，并给aux_loss向量返回一个全是1的梯度，从而使得aux_loss对应的梯度能够传递给前面的运算。\n此时有一个问题，学习率是如何生效的呢？注意MoEAuxLossAutoScaler还有一个方法set_loss_scale，这个方法接收一个变量并赋值给静态变量main_loss_backward_scale，这个变量也会和backward中的梯度相乘，显然这个scale的作用就是将学习率传递给梯度。在megatron/core/pipeline_parallel/schedules.py 中调用了这个函数，并将当前的学习率赋值给该静态变量。\nDispatcher 实现过程 在开启EP的情况下，多个DP的模型副本共享一套完整的expert参数，也就是每个模型只有部分expert的参数。所以在计算前需要在多个DP之间重新分配输入数据，以保证每个token都分配到保存有对应expert参数的设备上面。我们还是用刚才的例子来分析一下这个流程，并介绍相应的代码实现和以及变量的含义。\n在刚才的例子中，节点 (1,2,3,4) 共享了模型第1~2层上面的experts，具体来说节点(1,2)作为一个完整的TP组保存了前4个expert参数，节点(3,4)保存了后4个expert的参数。\n节点 模型参数 expert参数 输入 1 $M^1_{1:2}[1]$ $Experts_{1:4}^1$ $X_1^{0:\\frac n 2}$ 2 $M^2_{1:2}[1]$ $Experts_{1:4}^2$ $X_1^{\\frac n 2: n}$ 3 $M^1_{1:2}[2]$ $Experts_{5:8}^1$ $X_2^{0:\\frac n 2}$ 4 $M^2_{1:2}[2]$ $Experts_{5:8}^2$ $X_2^{\\frac n 2: n}$ 为了方便，我们把前面的表格复制到这里，并添加每个节点上面的输入说明。输入标记$X_1^{0:\\frac n 2}$中，下标表示第一个DP组对应的输入，上标表示输入中$[0:\\frac n 2]$​的子序列，n表示序列的长度。\n在每个节点上面all-gather，得到全局的输入；\n全局输入为$X_{1:2}^{0:n}=[X_1^{0:\\frac n 2}, X_1^{\\frac n 2: n},X_2^{0:\\frac n 2},X_2^{\\frac n 2: n}]$，对应代码中的变量是global_hidden_states。同样还需要all-gather的有全局的token expert分配矩阵global_indices以及对应的probs矩阵global_probs。\n筛选出当前节点上面对应的输入，并按照expert index的序号排序；\n对应上面例子，节点(1,2)上面分别需要筛选出expert 1到4对应的输入，节点(3,4)上面筛选出expert 4到8对应的输入。本地的输入对应的变量是local_hidden_states，并且保留global_local_map矩阵用来记录本地输入在global输入中原来的位置。\n计算expert的结果；\nexpert推理，得到本地输入对应的输出。计算的过程分为2种，1种是遍历每个expert单独计算，另外一种方式是将所有的expert参数合并起来一次计算，下一节我们会详细讲一下。\n将计算完的结果分发到原来的设备上；\n通过ReduceScatter的方式完成。\n以上过程存在一个问题，既Megatron的实现没有进行token drop。极端情况下所有的token都分配到一个expert上面，会直接导致节点显存崩掉，因此存在一定的不稳定性，但至少效果上面是没有损失的。\n通讯量分析 只开启TP和SP的情况下，每一个transformer层需要做4次的all-gather和reduce-scatter，transformer层和MoE层各2次，对应的通讯量为$8D\\frac{N-1}{N}$，其中D为hidden层输入的数据量。当开启了EP之后，MoE层的通讯数据由原来的$D$变为$EP\\times D$，所以EP的通讯量变为$4(EP-1)D\\frac{N-1}{N}$。\n",
  "wordCount" : "894",
  "inLanguage": "en",
  "image": "https://dawson-chen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished": "2024-03-25T23:40:02+08:00",
  "dateModified": "2024-03-25T23:40:02+08:00",
  "author":{
    "@type": "Person",
    "name": "Me"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dawson-chen.github.io/posts/megatron-lm-moe/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "DawsonChen's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dawson-chen.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dawson-chen.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://dawson-chen.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://dawson-chen.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://dawson-chen.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Megatron Lm Moe
    </h1>
    <div class="post-meta"><span title='2024-03-25 23:40:02 +0800 CST'>March 25, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;894 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/megatron-lm-moe.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><p>MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。</p>
<p>最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue&hellip; i&rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。</p>
<p>但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。</p>
<h2 id="moe结构回顾">MoE结构回顾<a hidden class="anchor" aria-hidden="true" href="#moe结构回顾">#</a></h2>
<p>首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：</p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bumpX&#39; } } }%%
graph LR
	x[&#34;X(n-1)&#34;] --&gt; p1[&#34;.&#34;] --&gt; input_ln[&#34;Layer Norm&#34;]
	input_ln --&gt; attn[&#34;Self Attention&#34;]
	attn --&gt; plus1((+)) 
	p1 --&gt; plus1
	plus1 --&gt; p2[&#34;.&#34;] --&gt; attn_ln[&#34;Layer Norm&#34;]
	
	subgraph MoE Layer
		expert1[&#34;Expert 1&#34;]
		expert2[&#34;Expert 2&#34;]
		expert_dot[&#34;...&#34;]
		expertk[&#34;Expert K&#34;]
	end
	
	attn_ln -.-&gt; expert2[&#34;Expert 2&#34;]
	attn_ln --&gt; expert1[&#34;Expert 1&#34;]
	attn_ln -.-&gt; expert_dot[&#34;...&#34;] &amp; expertk[&#34;Expert K&#34;]
	
	expert1 --&gt; plus((+))
	plus --&gt; plus2((+))
	expert2 &amp; expert_dot &amp; expertk -.-&gt; plus((+))
	plus2 --&gt; x2[&#34;X(n)&#34;]
	p2 --&gt; plus2

	
	classDef nodeNoBorder fill:#ffffff,stroke:#000000,stroke-width:0px;
	class expert_dot nodeNoBorder
	class x nodeNoBorder
	class x2 nodeNoBorder
	class p1 nodeNoBorder
	class p2 nodeNoBorder
	class plus nodeNoBorder
	class plus1 nodeNoBorder
	class plus2 nodeNoBorder
</code></pre><p>大多数情况下，MoE层是应用在MLP中的，也就是每个expert代表了一个MLP层。MoE并没有引入新的层，除了一个Router Network用来计算token和expert之间的匹配分数。</p>
<p>看起来MoE模型实现上和稠密网络并没有太大的区别，从计算流程上来看确实是这样，下面介绍一个MoE layer最简单的实现（transformer中的MixtralSparseMoeBlock）的过程：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MixtralSparseMoeBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_local_experts</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_experts_per_tok</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># gating</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">MixtralBlockSparseTop2MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34; &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算routing score</span>
</span></span><span class="line"><span class="cl">        <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">router_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 根据topk选出每个token对应的激活专家</span>
</span></span><span class="line"><span class="cl">        <span class="n">routing_weights</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">routing_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">routing_weights</span> <span class="o">/=</span> <span class="n">routing_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">				
</span></span><span class="line"><span class="cl">       	<span class="n">final_hidden_states</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在每个expert上计算选中的token</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">expert_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">expert_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts</span><span class="p">[</span><span class="n">expert_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">current_state</span> <span class="o">=</span> <span class="n">select_tokens</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">selected_experts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">current_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">						<span class="c1"># 计算expert的输出</span>
</span></span><span class="line"><span class="cl">            <span class="n">current_hidden_states</span> <span class="o">=</span> <span class="n">expert_layer</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span> <span class="o">*</span> <span class="n">routing_weights</span><span class="p">[</span><span class="n">top_x</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">final_hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">				<span class="c1"># 重组得到最后输出		</span>
</span></span><span class="line"><span class="cl">        <span class="n">final_hidden_states</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span><span class="n">final_hidden_states</span><span class="p">,</span> <span class="n">selected_experts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">final_hidden_states</span><span class="p">,</span> <span class="n">router_logits</span>
</span></span></code></pre></div><blockquote>
<p><em>注：这里省略了一些不必要的计算细节，只保留对过程理解有用的部分。比如：select_tokens，concat是为了简化而虚构的函数。</em></p>
</blockquote>
<p>上面的实现方法将所有的expert都加载到显存中，并且在计算expert的时候使用串行的方式。显然因为每个token计算的时候并不会激活所有的expert，虽然在实际训练的过程中每次的输入里包含很多个token，也就是可以保证不会有完全空闲的expert。但是expert上的计算压力明显要低于网络中其余部分，从而造成资源浪费。另外，在实际推理部署的时候，expert往往是分布在不同的设备上的，这就涉及多机通讯的问题。并且，router往往还要考虑负载均衡的问题。这些问题都给实现MoE的过程中增加了困难，下面看megatron是如何解决这些问题的。</p>
<p>这里分为3大块去分析整个实现过程，首先是expert的并行方式，其次是Router的设计，最后是Dispatcher。</p>
<h2 id="expert-parallel">Expert Parallel<a hidden class="anchor" aria-hidden="true" href="#expert-parallel">#</a></h2>
<p>EP(Expert Parallel)是MoE特有的并行方式，其核心是将expert在多个DP模型副本之间共享，从而实现节约显存的目的。可以说没有EP，那么大规模的MoE模型训练是完全不可能实现的。在介绍EP的划分方式之前，我们先来看一下megatron中并行组的划分方式。</p>
<h3 id="megatron中常规通讯组划分">Megatron中常规通讯组划分<a hidden class="anchor" aria-hidden="true" href="#megatron中常规通讯组划分">#</a></h3>
<p>首先我们来说一下没有EP的时候通讯组是怎么划分的。megatron中有TP，PP，DP的3种并行方式，这3种并行方式对应的通讯量大小排序是：TP&gt;DP&gt;PP。而在GPU集群里，设备内部的通讯带宽远大于设备间的带宽。因此通讯组划分原则就是，尽量让TP和DP通讯发生在设备内部，而PP组进行跨设备通讯。</p>
<p>如果不太理解这个分配方式，可以直接看下面这个例子。我会在示意图里面标记出每个GPU对应的TP DP PP的组，以及上面存放了对应模型的那部分结构。PS：顺便提一下，megatron中的并行组划分代码在写的时候应该是没有考虑可读性，如果想更快的看懂这块代码，在看之前最好也画一个这样的图对照着。</p>
<p>假设，现在有2台8卡的A100机器，需要训练一个8层的transformer模型，并行设置TP=2，PP=4，DP=2。那么示意图如下：</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/AllenChennn/picgo-repo/master/55a024601cf9e85c9b6252a39794ed1.jpg" alt="55a024601cf9e85c9b6252a39794ed1"  />
</p>
<p><strong>notions*说明</strong>：</p>
<ul>
<li>$M_{1:8}^1[1]$​，下标表示模型的1~8层参数，上标表示TP的第一个参数切片，方括号里表示第一个模型副本。</li>
</ul>
<p>事实上，PP组的第一个节点和最后一个节点还需要保存embedding和unembedding的矩阵，这里为了方便省略掉了。</p>
<h3 id="moe-layer通讯组划分">MoE layer通讯组划分<a hidden class="anchor" aria-hidden="true" href="#moe-layer通讯组划分">#</a></h3>
<p>通过上面这个例子我们知道了正常的Dense模型在并行情况下是如何切分的，那如果是MoE模型会是怎么样的呢？我们把上一个例子稍微改动一下，把Dense改为MoE模型，每一层transformer里面有8个expert。设置EP=2，意味着每2个模型副本间共享一个完整的MoE层。</p>
<p>我们以1:2层为例，看一下expert会如何进行保存，为了方便我们用一个表格说明。因为PP=4，所以前2层的参数会存放在第1到4个节点上面，因此我们只列出这4个节点，以及节点上面存放的模型参数和expert参数。</p>
<table>
<thead>
<tr>
<th>节点</th>
<th>模型参数</th>
<th>expert参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$M^1_{1:2}[1]$</td>
<td>$Experts_{1:4}^1$</td>
</tr>
<tr>
<td>2</td>
<td>$M^2_{1:2}[1]$</td>
<td>$Experts_{1:4}^2$</td>
</tr>
<tr>
<td>3</td>
<td>$M^1_{1:2}[2]$</td>
<td>$Experts_{5:8}^1$</td>
</tr>
<tr>
<td>4</td>
<td>$M^2_{1:2}[2]$</td>
<td>$Experts_{5:8}^2$</td>
</tr>
</tbody>
</table>
<p><strong>notions*说明</strong>：</p>
<ul>
<li>$Experts_{1:4}^1$中，下标表示第1~4个expert对应的参数，上标表示TP的第1个切片。</li>
</ul>
<p>那么在EP并行的情况下，设备之间的通讯需求分为下面2个部分：</p>
<ul>
<li>
<p>token分发；</p>
<p>通过上面表格可以看到，节点1-4共享一套完整的expert参数。(1, 2), (2, 3)是2个TP组，意味着这2个节点上的数据是相同的。但因为sequence parallel的存在，组内会在sequence方向上进行拆分输入。(1, 2)和 (2, 3)这2个组之间分别组成DP组，所以输入数据是不同的。这也就导致了节点1-4每个节点上面的token都是不一样的，所以在进行路由前，需要把这几个节点上的token都gather起来，再进行全局的分配。</p>
</li>
<li>
<p>参数更新；</p>
<p>因为EP的存在，expert参数和其他参数的DP组是不一样的，因此，要把存放有相同expert参数的节点放到一个expert独有的DP组里面。</p>
</li>
</ul>
<p>在megatron.core.parallel_state#initialize_model_parallel 中，上面2个组分别对应了变量_TENSOR_AND_EXPERT_PARALLEL_GROUP和_DATA_MODULO_EXPERT_PARALLEL_GROUP。</p>
<h2 id="router">Router<a hidden class="anchor" aria-hidden="true" href="#router">#</a></h2>
<p>路由是MoE中最重要的一环，决定了token与expert之间的对应关系。路由方式不光决定了模型的效果，同时也与负载均衡特性息息相关。按照路由的主体可以将路由方式分为3大类，分别是：token-based、expert-based、global assignment，大多数已有的路由方式都可以归纳到这个分类体系下。</p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;natural&#39; } } }%%
flowchart LR
	a[&#34;路由方式&#34;]
	a --&gt; b[&#34;token-based&#34;] &amp; c[&#34;expert-based&#34;] &amp; d[&#34;global assignment&#34;]
	b --&gt; e[&#34;hash&#34;] &amp; f[&#34;RL&#34;] &amp; g[&#34;topK&#34;]

	
</code></pre><p>megatron中支持了2种路由方式，分别是TopK和global assignment，下面我们分别介绍2种方法的实现。</p>
<h3 id="global-assignment">global assignment<a hidden class="anchor" aria-hidden="true" href="#global-assignment">#</a></h3>
<p>global assignment将token和expert之间的匹配当做一个全局最优的线性匹配问题，这样做的好处有：1. 在训练过程中，可以做到给每个expert分配相同的token，不需要进行负载均衡；2. 对于routing collapse问题有一定的抑制作用，因为会有token分配到次优的expert上面。</p>
<p>global assignment有很多种不同的解法，通常可能会想到的是Hungarian Algorithm，但是因为其并不能很好利用GPU的并行特点，下面介绍2种对于GPU计算友好的算法。</p>
<h4 id="拍卖行算法">拍卖行算法<a hidden class="anchor" aria-hidden="true" href="#拍卖行算法">#</a></h4>
<p>在global assignment第一次被提出的论文《BASE Layers: Simplifying Training of Large, Sparse Models》中，就使用了拍卖行算法作为问题的实现方式。这个算法通过模拟拍卖的过程计算全局最优，在开源框架fairseq中实现了该算法的源码，这里在源码的基础上加了一些必要的注解帮助理解算法的过程。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">balanced_assignment</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># scores [8, 80]   8 experts, 80 jobs</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_workers</span><span class="p">,</span> <span class="n">num_jobs</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> 
</span></span><span class="line"><span class="cl">    <span class="n">jobs_per_worker</span> <span class="o">=</span> <span class="n">num_jobs</span> <span class="o">//</span> <span class="n">num_workers</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>   <span class="c1"># 每个job对每个worker的价值，刚开始出价是0，所以等于scores</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">iterations</span> <span class="o">=</span> <span class="mi">0</span> 
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_jobs</span><span class="p">)</span>  <span class="c1"># 每个job上面的标价，初始为0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">jobs_with_bids</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>  <span class="c1"># 每个worker绑定的job数</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">jobs_with_bids</span><span class="o">.</span><span class="n">all</span><span class="p">():</span> 
</span></span><span class="line"><span class="cl">        <span class="c1"># top_values, top_index [8, 11]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># value表示job对worker的竞标价值：job对worker的价值 - 商品的报价</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 商品的价值初始为0</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_values</span><span class="p">,</span> <span class="n">top_index</span> <span class="o">=</span> <span class="n">topk</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">jobs_per_worker</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="c1"># worker进行加注</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 加注的量取决于当前job的竞标价值和次优价值之间的差异；</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 显然这种规则可以避免过度的加注</span>
</span></span><span class="line"><span class="cl">        <span class="n">bid_increments</span> <span class="o">=</span> <span class="n">top_values</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">top_values</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">eps</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 每次下注只下最高的jobs_per_worker个任务，也就是在最理想的情况下，可以一次中标全部</span>
</span></span><span class="line"><span class="cl">        <span class="n">bids</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">zeros</span><span class="p">(</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">num_jobs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">index</span><span class="o">=</span><span class="n">top_index</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="n">bid_increments</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">iterations</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># If a worker won a job on the previous round, put in a minimal bid to retain </span>
</span></span><span class="line"><span class="cl">            <span class="c1"># the job only if no other workers bid this round. </span>
</span></span><span class="line"><span class="cl">            <span class="n">bids</span><span class="p">[</span><span class="n">top_bidders</span><span class="p">,</span> <span class="n">jobs_with_bids</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Find the highest bidding worker per job </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># top_bids, top_bidders [1, 80]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 中标情况</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_bids</span><span class="p">,</span> <span class="n">top_bidders</span> <span class="o">=</span> <span class="n">bids</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">jobs_with_bids</span> <span class="o">=</span> <span class="n">top_bids</span> <span class="o">&gt;</span> <span class="mi">0</span> 
</span></span><span class="line"><span class="cl">        <span class="n">top_bidders</span> <span class="o">=</span> <span class="n">top_bidders</span><span class="p">[</span><span class="n">jobs_with_bids</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Make popular items more expensive </span>
</span></span><span class="line"><span class="cl">        <span class="n">cost</span> <span class="o">+=</span> <span class="n">top_bids</span>  <span class="c1"># 更新job的标价</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">cost</span>  <span class="c1"># 更具新的价值，重新计算每个worker和job之间的价值</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">iterations</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># If a worker won a job, make sure it appears in its top-k on the next round</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果竞标中了，把对应的value设置成无穷大，保证下一轮还会竞标 </span>
</span></span><span class="line"><span class="cl">            <span class="n">value</span><span class="p">[</span><span class="n">top_bidders</span><span class="p">,</span> <span class="n">jobs_with_bids</span><span class="p">]</span> <span class="o">=</span> <span class="err">∞</span> 
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">         <span class="n">value</span><span class="p">[</span><span class="n">top_bidders</span><span class="p">,</span> <span class="n">jobs_with_bids</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">top_bidders</span><span class="p">,</span> <span class="n">jobs_with_bids</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">        <span class="n">iterations</span> <span class="o">+=</span> <span class="mi">1</span> 
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">top_index</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>如果对该方法感兴趣，在论文《Auction Algorithms for Network Flow Problems: A Tutorial Introductionl》中可以找到收敛到最优点的证明。</p>
<h4 id="sinkhorn算法实现">sinkhorn算法实现<a hidden class="anchor" aria-hidden="true" href="#sinkhorn算法实现">#</a></h4>
<p>相比于sinkhorn算法，它的一种特殊例子Wasserstein metric可能更出名一点，大名鼎鼎的WGAN中的W所代表的就是它。Wasserstein metric可以理解为两个不同分布之间的最短距离，同时也是Optimal Transport问题的最优解。</p>
<p>什么是Optimal Transport？我们可以举一个例子：假设你有10个仓库在不同的位置，然后你有5个顾客需要从你这里进货。每个仓库中的货物数量用向量$c\in \mathbf R^{10}$c表示，每个顾客需要的货物用向量$r\in \mathbf R^5$表示，c和r可以被看成2个分布。进货的成本可以被表示为一个矩阵$M \in \mathbf R^{10\times 5}$，同样任意一种进货的方式可以被表示为$P\in \mathbf R^{10\times 5}$。r和c之间的Optimal Transport任务可以看成找到整体成本最小的进货方式$P^*$，并且此时的进货成本可以被看做是Wasserstein metric。</p>
<p>在这个例子中，Optimal Transport的任务可以形式化写成下面这种方式：
$$
d(r, c) = \underset{valid\ P}{min}  \sum_{i,j}{P_{ij}M_{ij}}
$$
sinkhorn算法在此基础上加入P的信息熵作为一个限制项，确保配货方式不会落入极端情况。对应到这面这个例子中，你可能并不想出现所有人都去一个仓库进货的情况。
$$
d^{\lambda}(r, c) = \underset{valid\ P}{min}  \sum_{i,j}{P_{ij}M_{ij}} + \frac{1}{\lambda}h(P)
$$
该算法的求解方法如下：</p>
<blockquote>
<p><strong>given</strong>: $M$, $\mathbf{r}$, $\mathbf{c}$ and $\lambda$
<strong>initialize</strong>: $P_\lambda = e^{-\lambda C}$
<strong>repeat</strong></p>
<blockquote>
<ol>
<li><strong>scale the rows</strong> such that the row sums match $\mathbf{r}$</li>
<li><strong>scale the columns</strong> such that the column sums match $\mathbf{c}$
<strong>until</strong> convergence</li>
</ol>
</blockquote>
</blockquote>
<p>回到MoE的router任务中，token和expert之间的最优匹配可以被看成是在token上的分布与expert上均匀分布之间的最优传输距离。因此可以用sinkhorn求解，但是我们并不关心传输距离，而是可以把行动矩阵$P$看做是一个加了均衡负载约束的喜好分布。</p>
<p>Megatron中使用的sinkhorn主要是为了得到全局最优分配矩阵，因此做了一些简化，与标准实现会有差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sinkhorn</span><span class="p">(</span><span class="n">cost</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Sinkhorn based MoE routing function&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这里给的cost其实是logits，代表token和expert之间的匹配程度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># sinkhorn距离的最优解中的 $\alpha$，$\beta$ 分别是这里的d0和d1</span>
</span></span><span class="line"><span class="cl">    <span class="n">d0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">cost</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cost</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cost</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">cost</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cost</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cost</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.00000001</span>
</span></span><span class="line"><span class="cl">    <span class="n">error</span> <span class="o">=</span> <span class="mf">1e9</span>
</span></span><span class="line"><span class="cl">    <span class="n">d1_old</span> <span class="o">=</span> <span class="n">d1</span>
</span></span><span class="line"><span class="cl">    <span class="c1">## 原始分布和目标分布都是均匀分布，所以用1 / d0.size(0) 和 1 / d1.size(0) 表示</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">error</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">d0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">d0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d1</span> <span class="o">*</span> <span class="n">cost</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">d1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">d1</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">cost</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">d1_old</span> <span class="o">-</span> <span class="n">d1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">d1_old</span> <span class="o">=</span> <span class="n">d1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">d1</span> <span class="o">*</span> <span class="n">cost</span> <span class="o">*</span> <span class="n">d0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="topk">topK<a hidden class="anchor" aria-hidden="true" href="#topk">#</a></h3>
<p>topK的实现与transformers里MixtralSparseMoeBlock的实现类似，根据router输出的logits选出每个token对应的前k个expert，并用softmax计算出对应的prob，作为最终计算结果的调和参数。</p>
<p>代码对应如下：</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/AllenChennn/picgo-repo/master/image-20240319175203075.png" alt="image-20240319175203075"  />
</p>
<h3 id="辅助loss的最佳实现">辅助loss的最佳实现<a hidden class="anchor" aria-hidden="true" href="#辅助loss的最佳实现">#</a></h3>
<p>Router里常见的辅助loss有2种，分别是load-banlance loss和z-loss。前者是为了应对route collapse问题，就是让router的结果更加的均匀，不会出现集中在个别expert上的情况。后者是为了防止gating网络计算的logits过大导致Router收敛变慢的情况。这2个loss都是在gating网络计算的logits上面进行计算得到的，loss的计算方法也没有什么特殊的，只是介绍一下loss生效的方式。</p>
<p>常规的实现方法是将每一个gating网络上面计算的logits收集起来，在模型推理完后计算对应的loss，并加到言模型的交叉熵loss上面。transformers中的switch_transformers就是这样实现的，对应代码如下：</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/AllenChennn/picgo-repo/master/image-20240318103517524.png" alt="image-20240318103517524"  />
</p>
<p>这种方式的问题是如果开启了PP，每个stage都需要将对应的logits传递给下一个stage，当然这样做也没有太多问题。但是megatron里使用了一种更加简洁的方式，给人以耳目一新的感觉。</p>
<p>首先，megatron的实现方式不需要传递中间变量，而是将loss当做网络的一部分。这里我们从megatron中摘抄一段z-loss的实现代码。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">z_loss_func</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">z_loss_coeff</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">z_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">z_loss_coeff</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">z_loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Router</span><span class="p">(</span><span class="n">MegatronModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="o">...</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">apply_z_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">moe_z_loss_coeff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">z_loss</span> <span class="o">=</span> <span class="n">z_loss_func</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">moe_z_loss_coeff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">MoEAuxLossAutoScaler</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">z_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_z_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># l</span>
</span></span></code></pre></div><p>是的，就是这么简单，z_loss_func函数接收logits并返回对应的loss，并且结果中包含了设定好的loss因子。loss并没有被返回并收集，而是直接作为网络的一个计算步骤，所以看起来MoEAuxLossAutoScaler是loss生效的关键。</p>
<p>下面是MoEAuxLossAutoScaler的代码片段：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MoEAuxLossAutoScaler</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">main_loss_backward_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">aux_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">aux_loss</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span></span><span class="line"><span class="cl">        <span class="n">aux_loss_backward_scale</span> <span class="o">=</span> <span class="n">MoEAuxLossAutoScaler</span><span class="o">.</span><span class="n">main_loss_backward_scale</span>
</span></span><span class="line"><span class="cl">        <span class="n">scaled_aux_loss_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">aux_loss</span><span class="p">)</span> <span class="o">*</span> <span class="n">aux_loss_backward_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">scaled_aux_loss_grad</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_loss_scale</span><span class="p">(</span><span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">MoEAuxLossAutoScaler</span><span class="o">.</span><span class="n">main_loss_backward_scale</span> <span class="o">=</span> <span class="n">scale</span>
</span></span></code></pre></div><p>看起来也很简单，下面分析一下。<code>MoEAuxLossAutoScaler</code>是一个<code>torch.autograd.Function</code>的类，也就是意味着pytorch可以根据自动微分功能计算对应的梯度。<code>forward</code>函数中接收logits和aux_loss 2个参数，所以<code>backward</code>函数必须返回2个梯度向量，分别对应2个输入。<code>backward</code>函数接收1个向量，因为只有logits参与了后续的计算。<code>backward</code>里做了2件事情，分别是将logits的参数透传给上一个运算，并给aux_loss向量返回一个全是1的梯度，从而使得aux_loss对应的梯度能够传递给前面的运算。</p>
<p>此时有一个问题，学习率是如何生效的呢？注意MoEAuxLossAutoScaler还有一个方法<code>set_loss_scale</code>，这个方法接收一个变量并赋值给静态变量main_loss_backward_scale，这个变量也会和backward中的梯度相乘，显然这个scale的作用就是将学习率传递给梯度。在<code>megatron/core/pipeline_parallel/schedules.py</code> 中调用了这个函数，并将当前的学习率赋值给该静态变量。</p>
<h2 id="dispatcher">Dispatcher<a hidden class="anchor" aria-hidden="true" href="#dispatcher">#</a></h2>
<h3 id="实现过程">实现过程<a hidden class="anchor" aria-hidden="true" href="#实现过程">#</a></h3>
<p>在开启EP的情况下，多个DP的模型副本共享一套完整的expert参数，也就是每个模型只有部分expert的参数。所以在计算前需要在多个DP之间重新分配输入数据，以保证每个token都分配到保存有对应expert参数的设备上面。我们还是用刚才的例子来分析一下这个流程，并介绍相应的代码实现和以及变量的含义。</p>
<p>在刚才的例子中，节点 (1,2,3,4) 共享了模型第1~2层上面的experts，具体来说节点(1,2)作为一个完整的TP组保存了前4个expert参数，节点(3,4)保存了后4个expert的参数。</p>
<table>
<thead>
<tr>
<th>节点</th>
<th>模型参数</th>
<th>expert参数</th>
<th>输入</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$M^1_{1:2}[1]$</td>
<td>$Experts_{1:4}^1$</td>
<td>$X_1^{0:\frac n 2}$</td>
</tr>
<tr>
<td>2</td>
<td>$M^2_{1:2}[1]$</td>
<td>$Experts_{1:4}^2$</td>
<td>$X_1^{\frac n 2: n}$</td>
</tr>
<tr>
<td>3</td>
<td>$M^1_{1:2}[2]$</td>
<td>$Experts_{5:8}^1$</td>
<td>$X_2^{0:\frac n 2}$</td>
</tr>
<tr>
<td>4</td>
<td>$M^2_{1:2}[2]$</td>
<td>$Experts_{5:8}^2$</td>
<td>$X_2^{\frac n 2: n}$</td>
</tr>
</tbody>
</table>
<p>为了方便，我们把前面的表格复制到这里，并添加每个节点上面的输入说明。输入标记$X_1^{0:\frac n 2}$中，下标表示第一个DP组对应的输入，上标表示输入中$[0:\frac n 2]$​的子序列，n表示序列的长度。</p>
<ol>
<li>
<p>在每个节点上面all-gather，得到全局的输入；</p>
<p>全局输入为$X_{1:2}^{0:n}=[X_1^{0:\frac n 2}, X_1^{\frac n 2: n},X_2^{0:\frac n 2},X_2^{\frac n 2: n}]$，对应代码中的变量是<code>global_hidden_states</code>。同样还需要all-gather的有全局的token expert分配矩阵<code>global_indices</code>以及对应的probs矩阵<code>global_probs</code>。</p>
</li>
<li>
<p>筛选出当前节点上面对应的输入，并按照expert index的序号排序；</p>
<p>对应上面例子，节点(1,2)上面分别需要筛选出expert 1到4对应的输入，节点(3,4)上面筛选出expert 4到8对应的输入。本地的输入对应的变量是<code>local_hidden_states</code>，并且保留<code>global_local_map</code>矩阵用来记录本地输入在global输入中原来的位置。</p>
</li>
<li>
<p>计算expert的结果；</p>
<p>expert推理，得到本地输入对应的输出。计算的过程分为2种，1种是遍历每个expert单独计算，另外一种方式是将所有的expert参数合并起来一次计算，下一节我们会详细讲一下。</p>
</li>
<li>
<p>将计算完的结果分发到原来的设备上；</p>
<p>通过ReduceScatter的方式完成。</p>
</li>
</ol>
<p>以上过程存在一个问题，既Megatron的实现没有进行token drop。极端情况下所有的token都分配到一个expert上面，会直接导致节点显存崩掉，因此存在一定的不稳定性，但至少效果上面是没有损失的。</p>
<h3 id="通讯量分析">通讯量分析<a hidden class="anchor" aria-hidden="true" href="#通讯量分析">#</a></h3>
<p>只开启TP和SP的情况下，每一个transformer层需要做4次的all-gather和reduce-scatter，transformer层和MoE层各2次，对应的通讯量为$8D\frac{N-1}{N}$，其中D为hidden层输入的数据量。当开启了EP之后，MoE层的通讯数据由原来的$D$变为$EP\times D$，所以EP的通讯量变为$4(EP-1)D\frac{N-1}{N}$。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://dawson-chen.github.io/posts/megatron-lm-pipline/">
    <span class="title">Next »</span>
    <br>
    <span>Megatron Lm Pipline</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on x"
            href="https://x.com/intent/tweet/?text=Megatron%20Lm%20Moe&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f&amp;title=Megatron%20Lm%20Moe&amp;summary=Megatron%20Lm%20Moe&amp;source=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f&title=Megatron%20Lm%20Moe">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on whatsapp"
            href="https://api.whatsapp.com/send?text=Megatron%20Lm%20Moe%20-%20https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on telegram"
            href="https://telegram.me/share/url?text=Megatron%20Lm%20Moe&amp;url=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Megatron Lm Moe on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Megatron%20Lm%20Moe&u=https%3a%2f%2fdawson-chen.github.io%2fposts%2fmegatron-lm-moe%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://dawson-chen.github.io/">DawsonChen&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
