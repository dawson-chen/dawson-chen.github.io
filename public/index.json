[{"content":"背景 MoE模型可以在推理算力不变的情况下，继续扩大模型的规模，从而获得到scaling up带来提升。但是在实际应用场景下，这种提升并非没有代价。\n模型的推理性能；\n因为MoE训练带来显著的通讯量提升，并且在越大规模上面这种提升越巨大，所以MoE的训练性能相比于同样激活参数量的Dense网络只有50%~80%。但当模型处于真实应用场景下，相比与训练性能，我们更关心的是MoE模型的推理性能，MoE模型的推理性能严重依赖于设备之间的通讯带宽，因此会给部署带来额外的成本。\n端侧应用的限制；\nMoE模型虽然激活参数较少，但是模型的总参数量会增大数倍，这在端侧这种内存受限的场景下应用起来并不容易。虽然，在服务端应用的时候可以通过EP这种方式极大的降低总参数量带来的影响。\n因此MoE to Dense的技术可以使MoE模型能够克服上面2个缺点（当然了，因为已经变成一个彻底的Dense模型）。并且，考虑到MoE模型expert之间存在极大的冗余性，缩小MoE总参数量就看起来是非常合理的一种需求了。\n2篇相关论文 One Student Knows All Experts Know: From Sparse to Dense National University of Singapore, Huawei, Oct 2022\n总结：\n应该是第一篇提出将MoE能力压缩到Dense中，看得出来Huawei在发展MoE技术上还是走到前面的。同时结合手机业务的应用场景（背景中说的第2点），提出了MoE to Dense的技术。\n文章提出了一项任务knowledge gather，就是将多个expert中的知识合并到单个expert中，以训练出与 MoE 模型效果类似的稠密学生模型。该任务分为知识收集和知识蒸馏两个阶段，知识收集中探索了四种不同的知识收集方法，知识蒸馏则利用整合好的知识进一步优化学生模型。在实验中，该方法在计算机视觉和自然语言处理两个领域取得了优异的表现。\n知识收集方法分为4种：summation、averaging、Top-K Knowledge Gathering (Top-KG)、Singular Value Decomposition Knowledge Gathering (SVD-KG)。前2个方法类似于模型的参数合并，而后面2种方法是论文中提出的，可以尽可能把重要的参数提取出来。不管用哪种方法，合并都给参数中引入了噪声，因此下一步就是用蒸馏的方式恢复模型的能力。\n论文中的主要创新应该是知识收集的方式，那么最终要的应该是验证知识收集的能力，但可惜的是给出的结果并没有充分的验证。MoE to Dense应用很重要的一点是花尽量少的代价将MoE的能力迁移到Dense模型上面，论文中并没有说明第二阶段蒸馏用的计算量，而是从蒸馏后最终效果和传统的蒸馏方法进行对比。\nExperts Weights Averaging: A New General Training Scheme for Vision Transformers Aug 2023, Fudan University\nre-parameterization，即二次参数化方法，是在CV中提出的一种方法，旨在解决多分支类型的网络结构在推理时的低效，比如 ResNet。具有代表性的是RepVGG，在训练的时候使用多分支结构，但是在推理阶段使用卷积核合并得到一个单分支的网络。该方法最重要的是合并后的结构等价性，而MoE的expert并不存在等价的合并方式。\n因此，论文为了解决这个问题，在每次训练后人为的将expert之间的参数距离拉近。方法如下：\n这里的做法可能有一点隐患，因为MoE的训练过程是会导致expert之间的差异越来越大，如果训练中人为对expert之间参数进行了平滑，那么是否同时也降低了MoE能取得的效果呢？\n在训练结束后，通过平均每个 MoE 的专家，将每个 MoE 转换为 FFN，将模型转换回原始 ViT 以进行推理。论文还提供了理论分析，证明了该方法的有效性和通用性，并在各种 2D 和 3D 视觉任务、ViT 架构和数据集上进行了广泛实验。\n这篇文章的出发点是利用MoE结合重参数化提升ViT的效果，同时也降低了MoE模型的部署难度，是一个不错的思路。\n后记 MoE to Dense并不是一个很常见的需求，2篇论文解决的场景都或多或少都有点推理资源敏感。但我觉得随着MoE的模型越来越大，那么对应的推理压力也会越来越大，虽然有专家并行，但实际要实现和同激活参数的Dense模型相同的推理效率并不容易，因此MoE to Dense也会变得越来越有价值。另外MoE中一定存在大量的冗余信息，可以简单说2个现象：1. 增加激活专家并不会带来明显的效果增益；2. 不管用什么方法训练，在推理的时候有些专家被激活的比例任然比较少，因此对MoE做裁剪是必须得一个步骤，而裁剪和转换Dense都需要搞清楚MoE学习到的参数特性。\n这个方向也有很多的挑战，举2个方面：\n目前MoE结构趋向于Deepseek提出的Fine-Grained + shared expert 方式，这又给MoE to Dense的转换增加了难度。因为不光要考虑转换方式有效性，同时还要兼顾模型结构的变换。 这个事情有一个不在明面上的好处是，通过验证不同的转换方案同时也得到一些MoE技术内在的insight。但是这个事情再深一点就要考虑模型参数的可解释性，这是一个更加困难的领域。 ","permalink":"https://dawson-chen.github.io/posts/moe-to-dense-introduce/","summary":"背景 MoE模型可以在推理算力不变的情况下，继续扩大模型的规模，从而获得到scaling up带来提升。但是在实际应用场景下，这种提升并非没有代价。\n模型的推理性能；\n因为MoE训练带来显著的通讯量提升，并且在越大规模上面这种提升越巨大，所以MoE的训练性能相比于同样激活参数量的Dense网络只有50%~80%。但当模型处于真实应用场景下，相比与训练性能，我们更关心的是MoE模型的推理性能，MoE模型的推理性能严重依赖于设备之间的通讯带宽，因此会给部署带来额外的成本。\n端侧应用的限制；\nMoE模型虽然激活参数较少，但是模型的总参数量会增大数倍，这在端侧这种内存受限的场景下应用起来并不容易。虽然，在服务端应用的时候可以通过EP这种方式极大的降低总参数量带来的影响。\n因此MoE to Dense的技术可以使MoE模型能够克服上面2个缺点（当然了，因为已经变成一个彻底的Dense模型）。并且，考虑到MoE模型expert之间存在极大的冗余性，缩小MoE总参数量就看起来是非常合理的一种需求了。\n2篇相关论文 One Student Knows All Experts Know: From Sparse to Dense National University of Singapore, Huawei, Oct 2022\n总结：\n应该是第一篇提出将MoE能力压缩到Dense中，看得出来Huawei在发展MoE技术上还是走到前面的。同时结合手机业务的应用场景（背景中说的第2点），提出了MoE to Dense的技术。\n文章提出了一项任务knowledge gather，就是将多个expert中的知识合并到单个expert中，以训练出与 MoE 模型效果类似的稠密学生模型。该任务分为知识收集和知识蒸馏两个阶段，知识收集中探索了四种不同的知识收集方法，知识蒸馏则利用整合好的知识进一步优化学生模型。在实验中，该方法在计算机视觉和自然语言处理两个领域取得了优异的表现。\n知识收集方法分为4种：summation、averaging、Top-K Knowledge Gathering (Top-KG)、Singular Value Decomposition Knowledge Gathering (SVD-KG)。前2个方法类似于模型的参数合并，而后面2种方法是论文中提出的，可以尽可能把重要的参数提取出来。不管用哪种方法，合并都给参数中引入了噪声，因此下一步就是用蒸馏的方式恢复模型的能力。\n论文中的主要创新应该是知识收集的方式，那么最终要的应该是验证知识收集的能力，但可惜的是给出的结果并没有充分的验证。MoE to Dense应用很重要的一点是花尽量少的代价将MoE的能力迁移到Dense模型上面，论文中并没有说明第二阶段蒸馏用的计算量，而是从蒸馏后最终效果和传统的蒸馏方法进行对比。\nExperts Weights Averaging: A New General Training Scheme for Vision Transformers Aug 2023, Fudan University\nre-parameterization，即二次参数化方法，是在CV中提出的一种方法，旨在解决多分支类型的网络结构在推理时的低效，比如 ResNet。具有代表性的是RepVGG，在训练的时候使用多分支结构，但是在推理阶段使用卷积核合并得到一个单分支的网络。该方法最重要的是合并后的结构等价性，而MoE的expert并不存在等价的合并方式。\n因此，论文为了解决这个问题，在每次训练后人为的将expert之间的参数距离拉近。方法如下：\n这里的做法可能有一点隐患，因为MoE的训练过程是会导致expert之间的差异越来越大，如果训练中人为对expert之间参数进行了平滑，那么是否同时也降低了MoE能取得的效果呢？\n在训练结束后，通过平均每个 MoE 的专家，将每个 MoE 转换为 FFN，将模型转换回原始 ViT 以进行推理。论文还提供了理论分析，证明了该方法的有效性和通用性，并在各种 2D 和 3D 视觉任务、ViT 架构和数据集上进行了广泛实验。","title":"MoE to Dense介绍以及相关论文速览"},{"content":"MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。\n最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue\u0026hellip; i\u0026rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。\n但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。\nMoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：\n%%{ init: { \u0026#39;flowchart\u0026#39;: { \u0026#39;curve\u0026#39;: \u0026#39;bumpX\u0026#39; } } }%% graph LR x[\u0026#34;X(n-1)\u0026#34;] --\u0026gt; p1[\u0026#34;.\u0026#34;] --\u0026gt; input_ln[\u0026#34;Layer Norm\u0026#34;] input_ln --\u0026gt; attn[\u0026#34;Self Attention\u0026#34;] attn --\u0026gt; plus1((+)) p1 --\u0026gt; plus1 plus1 --\u0026gt; p2[\u0026#34;.\u0026#34;] --\u0026gt; attn_ln[\u0026#34;Layer Norm\u0026#34;] subgraph MoE Layer expert1[\u0026#34;Expert 1\u0026#34;] expert2[\u0026#34;Expert 2\u0026#34;] expert_dot[\u0026#34;...\u0026#34;] expertk[\u0026#34;Expert K\u0026#34;] end attn_ln -.-\u0026gt; expert2[\u0026#34;Expert 2\u0026#34;] attn_ln --\u0026gt; expert1[\u0026#34;Expert 1\u0026#34;] attn_ln -.-\u0026gt; expert_dot[\u0026#34;...\u0026#34;] \u0026amp; expertk[\u0026#34;Expert K\u0026#34;] expert1 --\u0026gt; plus((+)) plus --\u0026gt; plus2((+)) expert2 \u0026amp; expert_dot \u0026amp; expertk -.-\u0026gt; plus((+)) plus2 --\u0026gt; x2[\u0026#34;X(n)\u0026#34;] p2 --\u0026gt; plus2 classDef nodeNoBorder fill:#ffffff,stroke:#000000,stroke-width:0px; class expert_dot nodeNoBorder class x nodeNoBorder class x2 nodeNoBorder class p1 nodeNoBorder class p2 nodeNoBorder class plus nodeNoBorder class plus1 nodeNoBorder class plus2 nodeNoBorder 大多数情况下，MoE层是应用在MLP中的，也就是每个expert代表了一个MLP层。MoE并没有引入新的层，除了一个Router Network用来计算token和expert之间的匹配分数。\n看起来MoE模型实现上和稠密网络并没有太大的区别，从计算流程上来看确实是这样，下面介绍一个MoE layer最简单的实现（transformer中的MixtralSparseMoeBlock）的过程：\nclass MixtralSparseMoeBlock(nn.Module): def __init__(self, config): super().__init__() self.hidden_dim = config.hidden_size self.ffn_dim = config.intermediate_size self.num_experts = config.num_local_experts self.top_k = config.num_experts_per_tok # gating self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False) self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)]) def forward(self, hidden_states: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # 计算routing score router_logits = self.gate(hidden_states) routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float) # 根据topk选出每个token对应的激活专家 routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1) routing_weights /= routing_weights.sum(dim=-1, keepdim=True) final_hidden_states = [] # 在每个expert上计算选中的token for expert_idx in range(self.num_experts): expert_layer = self.experts[expert_idx] current_state = select_tokens(hidden_states, selected_experts) if current_state.shape[0] == 0: continue # 计算expert的输出 current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None] final_hidden_states.append(current_hidden_states) # 重组得到最后输出\tfinal_hidden_states = concat(final_hidden_states, selected_experts) return final_hidden_states, router_logits 注：这里省略了一些不必要的计算细节，只保留对过程理解有用的部分。比如：select_tokens，concat是为了简化而虚构的函数。\n上面的实现方法将所有的expert都加载到显存中，并且在计算expert的时候使用串行的方式。显然因为每个token计算的时候并不会激活所有的expert，虽然在实际训练的过程中每次的输入里包含很多个token，也就是可以保证不会有完全空闲的expert。但是expert上的计算压力明显要低于网络中其余部分，从而造成资源浪费。另外，在实际推理部署的时候，expert往往是分布在不同的设备上的，这就涉及多机通讯的问题。并且，router往往还要考虑负载均衡的问题。这些问题都给实现MoE的过程中增加了困难，下面看megatron是如何解决这些问题的。\n这里分为3大块去分析整个实现过程，首先是expert的并行方式，其次是Router的设计，最后是Dispatcher。\nExpert Parallel EP(Expert Parallel)是MoE特有的并行方式，其核心是将expert在多个DP模型副本之间共享，从而实现节约显存的目的。可以说没有EP，那么大规模的MoE模型训练是完全不可能实现的。在介绍EP的划分方式之前，我们先来看一下megatron中并行组的划分方式。\nMegatron中常规通讯组划分 首先我们来说一下没有EP的时候通讯组是怎么划分的。megatron中有TP，PP，DP的3种并行方式，这3种并行方式对应的通讯量大小排序是：TP\u0026gt;DP\u0026gt;PP。而在GPU集群里，设备内部的通讯带宽远大于设备间的带宽。因此通讯组划分原则就是，尽量让TP和DP通讯发生在设备内部，而PP组进行跨设备通讯。\n如果不太理解这个分配方式，可以直接看下面这个例子。我会在示意图里面标记出每个GPU对应的TP DP PP的组，以及上面存放了对应模型的那部分结构。PS：顺便提一下，megatron中的并行组划分代码在写的时候应该是没有考虑可读性，如果想更快的看懂这块代码，在看之前最好也画一个这样的图对照着。\n假设，现在有2台8卡的A100机器，需要训练一个8层的transformer模型，并行设置TP=2，PP=4，DP=2。那么示意图如下：\nnotions*说明：\n$M_{1:8}^1[1]$​，下标表示模型的1~8层参数，上标表示TP的第一个参数切片，方括号里表示第一个模型副本。 事实上，PP组的第一个节点和最后一个节点还需要保存embedding和unembedding的矩阵，这里为了方便省略掉了。\nMoE layer通讯组划分 通过上面这个例子我们知道了正常的Dense模型在并行情况下是如何切分的，那如果是MoE模型会是怎么样的呢？我们把上一个例子稍微改动一下，把Dense改为MoE模型，每一层transformer里面有8个expert。设置EP=2，意味着每2个模型副本间共享一个完整的MoE层。\n我们以1:2层为例，看一下expert会如何进行保存，为了方便我们用一个表格说明。因为PP=4，所以前2层的参数会存放在第1到4个节点上面，因此我们只列出这4个节点，以及节点上面存放的模型参数和expert参数。\n节点 模型参数 expert参数 1 $M^1_{1:2}[1]$ $Experts_{1:4}^1$ 2 $M^2_{1:2}[1]$ $Experts_{1:4}^2$ 3 $M^1_{1:2}[2]$ $Experts_{5:8}^1$ 4 $M^2_{1:2}[2]$ $Experts_{5:8}^2$ notions*说明：\n$Experts_{1:4}^1$中，下标表示第1~4个expert对应的参数，上标表示TP的第1个切片。 那么在EP并行的情况下，设备之间的通讯需求分为下面2个部分：\ntoken分发；\n通过上面表格可以看到，节点1-4共享一套完整的expert参数。(1, 2), (2, 3)是2个TP组，意味着这2个节点上的数据是相同的。但因为sequence parallel的存在，组内会在sequence方向上进行拆分输入。(1, 2)和 (2, 3)这2个组之间分别组成DP组，所以输入数据是不同的。这也就导致了节点1-4每个节点上面的token都是不一样的，所以在进行路由前，需要把这几个节点上的token都gather起来，再进行全局的分配。\n参数更新；\n因为EP的存在，expert参数和其他参数的DP组是不一样的，因此，要把存放有相同expert参数的节点放到一个expert独有的DP组里面。\n在megatron.core.parallel_state#initialize_model_parallel 中，上面2个组分别对应了变量_TENSOR_AND_EXPERT_PARALLEL_GROUP和_DATA_MODULO_EXPERT_PARALLEL_GROUP。\nRouter 路由是MoE中最重要的一环，决定了token与expert之间的对应关系。路由方式不光决定了模型的效果，同时也与负载均衡特性息息相关。按照路由的主体可以将路由方式分为3大类，分别是：token-based、expert-based、global assignment，大多数已有的路由方式都可以归纳到这个分类体系下。\n%%{ init: { \u0026#39;flowchart\u0026#39;: { \u0026#39;curve\u0026#39;: \u0026#39;natural\u0026#39; } } }%% flowchart LR a[\u0026#34;路由方式\u0026#34;] a --\u0026gt; b[\u0026#34;token-based\u0026#34;] \u0026amp; c[\u0026#34;expert-based\u0026#34;] \u0026amp; d[\u0026#34;global assignment\u0026#34;] b --\u0026gt; e[\u0026#34;hash\u0026#34;] \u0026amp; f[\u0026#34;RL\u0026#34;] \u0026amp; g[\u0026#34;topK\u0026#34;] megatron中支持了2种路由方式，分别是TopK和global assignment，下面我们分别介绍2种方法的实现。\nglobal assignment global assignment将token和expert之间的匹配当做一个全局最优的线性匹配问题，这样做的好处有：1. 在训练过程中，可以做到给每个expert分配相同的token，不需要进行负载均衡；2. 对于routing collapse问题有一定的抑制作用，因为会有token分配到次优的expert上面。\nglobal assignment有很多种不同的解法，通常可能会想到的是Hungarian Algorithm，但是因为其并不能很好利用GPU的并行特点，下面介绍2种对于GPU计算友好的算法。\n拍卖行算法 在global assignment第一次被提出的论文《BASE Layers: Simplifying Training of Large, Sparse Models》中，就使用了拍卖行算法作为问题的实现方式。这个算法通过模拟拍卖的过程计算全局最优，在开源框架fairseq中实现了该算法的源码，这里在源码的基础上加了一些必要的注解帮助理解算法的过程。\ndef balanced_assignment(scores, max_iterations=100): # scores [8, 80] 8 experts, 80 jobs num_workers, num_jobs = scores.size() jobs_per_worker = num_jobs // num_workers value = scores.clone() # 每个job对每个worker的价值，刚开始出价是0，所以等于scores iterations = 0 cost = scores.new_zeros(1, num_jobs) # 每个job上面的标价，初始为0 jobs_with_bids = zeros(num_workers).bool() # 每个worker绑定的job数 while not jobs_with_bids.all(): # top_values, top_index [8, 11] # value表示job对worker的竞标价值：job对worker的价值 - 商品的报价 # 商品的价值初始为0 top_values, top_index = topk(value, k=jobs_per_worker + 1, dim=1) # worker进行加注 # 加注的量取决于当前job的竞标价值和次优价值之间的差异； # 显然这种规则可以避免过度的加注 bid_increments = top_values[:, :-1] - top_values[:, -1:] + eps # 每次下注只下最高的jobs_per_worker个任务，也就是在最理想的情况下，可以一次中标全部 bids = scatter( zeros(num_workers, num_jobs), dim=1, index=top_index[:, :-1], src=bid_increments ) if 0 \u0026lt; iterations \u0026lt; max_iterations: # If a worker won a job on the previous round, put in a minimal bid to retain # the job only if no other workers bid this round. bids[top_bidders, jobs_with_bids] = eps # Find the highest bidding worker per job # top_bids, top_bidders [1, 80] # 中标情况 top_bids, top_bidders = bids.max(dim=0) jobs_with_bids = top_bids \u0026gt; 0 top_bidders = top_bidders[jobs_with_bids] # Make popular items more expensive cost += top_bids # 更新job的标价 value = scores - cost # 更具新的价值，重新计算每个worker和job之间的价值 if iterations \u0026lt; max_iterations: # If a worker won a job, make sure it appears in its top-k on the next round # 如果竞标中了，把对应的value设置成无穷大，保证下一轮还会竞标 value[top_bidders, jobs_with_bids] = ∞ else: value[top_bidders, jobs_with_bids] = scores[top_bidders, jobs_with_bids] iterations += 1 return top_index[:,:-1].reshape(-1) 如果对该方法感兴趣，在论文《Auction Algorithms for Network Flow Problems: A Tutorial Introductionl》中可以找到收敛到最优点的证明。\nsinkhorn算法实现 相比于sinkhorn算法，它的一种特殊例子Wasserstein metric可能更出名一点，大名鼎鼎的WGAN中的W所代表的就是它。Wasserstein metric可以理解为两个不同分布之间的最短距离，同时也是Optimal Transport问题的最优解。\n什么是Optimal Transport？我们可以举一个例子：假设你有10个仓库在不同的位置，然后你有5个顾客需要从你这里进货。每个仓库中的货物数量用向量$c\\in \\mathbf R^{10}$c表示，每个顾客需要的货物用向量$r\\in \\mathbf R^5$表示，c和r可以被看成2个分布。进货的成本可以被表示为一个矩阵$M \\in \\mathbf R^{10\\times 5}$，同样任意一种进货的方式可以被表示为$P\\in \\mathbf R^{10\\times 5}$。r和c之间的Optimal Transport任务可以看成找到整体成本最小的进货方式$P^*$，并且此时的进货成本可以被看做是Wasserstein metric。\n在这个例子中，Optimal Transport的任务可以形式化写成下面这种方式： $$ d(r, c) = \\underset{valid\\ P}{min} \\sum_{i,j}{P_{ij}M_{ij}} $$ sinkhorn算法在此基础上加入P的信息熵作为一个限制项，确保配货方式不会落入极端情况。对应到这面这个例子中，你可能并不想出现所有人都去一个仓库进货的情况。 $$ d^{\\lambda}(r, c) = \\underset{valid\\ P}{min} \\sum_{i,j}{P_{ij}M_{ij}} + \\frac{1}{\\lambda}h(P) $$ 该算法的求解方法如下：\ngiven: $M$, $\\mathbf{r}$, $\\mathbf{c}$ and $\\lambda$ initialize: $P_\\lambda = e^{-\\lambda C}$ repeat\nscale the rows such that the row sums match $\\mathbf{r}$ scale the columns such that the column sums match $\\mathbf{c}$ until convergence 回到MoE的router任务中，token和expert之间的最优匹配可以被看成是在token上的分布与expert上均匀分布之间的最优传输距离。因此可以用sinkhorn求解，但是我们并不关心传输距离，而是可以把行动矩阵$P$看做是一个加了均衡负载约束的喜好分布。\nMegatron中使用的sinkhorn主要是为了得到全局最优分配矩阵，因此做了一些简化，与标准实现会有差异。\ndef sinkhorn(cost: torch.Tensor, tol: float = 0.0001): \u0026#34;\u0026#34;\u0026#34;Sinkhorn based MoE routing function\u0026#34;\u0026#34;\u0026#34; # 这里给的cost其实是logits，代表token和expert之间的匹配程度 cost = torch.exp(cost) # sinkhorn距离的最优解中的 $\\alpha$，$\\beta$ 分别是这里的d0和d1 d0 = torch.ones(cost.size(0), device=cost.device, dtype=cost.dtype) d1 = torch.ones(cost.size(1), device=cost.device, dtype=cost.dtype) eps = 0.00000001 error = 1e9 d1_old = d1 ## 原始分布和目标分布都是均匀分布，所以用1 / d0.size(0) 和 1 / d1.size(0) 表示 while error \u0026gt; tol: d0 = (1 / d0.size(0)) * 1 / (torch.sum(d1 * cost, 1) + eps) d1 = (1 / d1.size(0)) * 1 / (torch.sum(d0.unsqueeze(1) * cost, 0) + eps) error = torch.mean(torch.abs(d1_old - d1)) d1_old = d1 return d1 * cost * d0.unsqueeze(1) topK topK的实现与transformers里MixtralSparseMoeBlock的实现类似，根据router输出的logits选出每个token对应的前k个expert，并用softmax计算出对应的prob，作为最终计算结果的调和参数。\n代码对应如下：\n辅助loss的最佳实现 Router里常见的辅助loss有2种，分别是load-banlance loss和z-loss。前者是为了应对route collapse问题，就是让router的结果更加的均匀，不会出现集中在个别expert上的情况。后者是为了防止gating网络计算的logits过大导致Router收敛变慢的情况。这2个loss都是在gating网络计算的logits上面进行计算得到的，loss的计算方法也没有什么特殊的，只是介绍一下loss生效的方式。\n常规的实现方法是将每一个gating网络上面计算的logits收集起来，在模型推理完后计算对应的loss，并加到言模型的交叉熵loss上面。transformers中的switch_transformers就是这样实现的，对应代码如下：\n这种方式的问题是如果开启了PP，每个stage都需要将对应的logits传递给下一个stage，当然这样做也没有太多问题。但是megatron里使用了一种更加简洁的方式，给人以耳目一新的感觉。\n首先，megatron的实现方式不需要传递中间变量，而是将loss当做网络的一部分。这里我们从megatron中摘抄一段z-loss的实现代码。\ndef z_loss_func(logits, z_loss_coeff): z_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff return z_loss class Router(MegatronModule): ... def apply_z_loss(self, logits): if self.config.moe_z_loss_coeff is not None: z_loss = z_loss_func(logits, self.config.moe_z_loss_coeff) logits = MoEAuxLossAutoScaler.apply(logits, z_loss) return logits def forward(self,): ... logits = self.apply_z_loss(logits) ... # l 是的，就是这么简单，z_loss_func函数接收logits并返回对应的loss，并且结果中包含了设定好的loss因子。loss并没有被返回并收集，而是直接作为网络的一个计算步骤，所以看起来MoEAuxLossAutoScaler是loss生效的关键。\n下面是MoEAuxLossAutoScaler的代码片段：\nclass MoEAuxLossAutoScaler(torch.autograd.Function): main_loss_backward_scale: torch.Tensor = torch.tensor(1.0) @staticmethod def forward(ctx, output: torch.Tensor, aux_loss: torch.Tensor): ctx.save_for_backward(aux_loss) return output @staticmethod def backward(ctx, grad_output: torch.Tensor): (aux_loss,) = ctx.saved_tensors aux_loss_backward_scale = MoEAuxLossAutoScaler.main_loss_backward_scale scaled_aux_loss_grad = torch.ones_like(aux_loss) * aux_loss_backward_scale return grad_output, scaled_aux_loss_grad @staticmethod def set_loss_scale(scale: torch.Tensor): MoEAuxLossAutoScaler.main_loss_backward_scale = scale 看起来也很简单，下面分析一下。MoEAuxLossAutoScaler是一个torch.autograd.Function的类，也就是意味着pytorch可以根据自动微分功能计算对应的梯度。forward函数中接收logits和aux_loss 2个参数，所以backward函数必须返回2个梯度向量，分别对应2个输入。backward函数接收1个向量，因为只有logits参与了后续的计算。backward里做了2件事情，分别是将logits的参数透传给上一个运算，并给aux_loss向量返回一个全是1的梯度，从而使得aux_loss对应的梯度能够传递给前面的运算。\n此时有一个问题，学习率是如何生效的呢？注意MoEAuxLossAutoScaler还有一个方法set_loss_scale，这个方法接收一个变量并赋值给静态变量main_loss_backward_scale，这个变量也会和backward中的梯度相乘，显然这个scale的作用就是将学习率传递给梯度。在megatron/core/pipeline_parallel/schedules.py 中调用了这个函数，并将当前的学习率赋值给该静态变量。\nDispatcher 实现过程 在开启EP的情况下，多个DP的模型副本共享一套完整的expert参数，也就是每个模型只有部分expert的参数。所以在计算前需要在多个DP之间重新分配输入数据，以保证每个token都分配到保存有对应expert参数的设备上面。我们还是用刚才的例子来分析一下这个流程，并介绍相应的代码实现和以及变量的含义。\n在刚才的例子中，节点 (1,2,3,4) 共享了模型第1~2层上面的experts，具体来说节点(1,2)作为一个完整的TP组保存了前4个expert参数，节点(3,4)保存了后4个expert的参数。\n节点 模型参数 expert参数 输入 1 $M^1_{1:2}[1]$ $Experts_{1:4}^1$ $X_1^{0:\\frac n 2}$ 2 $M^2_{1:2}[1]$ $Experts_{1:4}^2$ $X_1^{\\frac n 2: n}$ 3 $M^1_{1:2}[2]$ $Experts_{5:8}^1$ $X_2^{0:\\frac n 2}$ 4 $M^2_{1:2}[2]$ $Experts_{5:8}^2$ $X_2^{\\frac n 2: n}$ 为了方便，我们把前面的表格复制到这里，并添加每个节点上面的输入说明。输入标记$X_1^{0:\\frac n 2}$中，下标表示第一个DP组对应的输入，上标表示输入中$[0:\\frac n 2]$​的子序列，n表示序列的长度。\n在每个节点上面all-gather，得到全局的输入；\n全局输入为$X_{1:2}^{0:n}=[X_1^{0:\\frac n 2}, X_1^{\\frac n 2: n},X_2^{0:\\frac n 2},X_2^{\\frac n 2: n}]$，对应代码中的变量是global_hidden_states。同样还需要all-gather的有全局的token expert分配矩阵global_indices以及对应的probs矩阵global_probs。\n筛选出当前节点上面对应的输入，并按照expert index的序号排序；\n对应上面例子，节点(1,2)上面分别需要筛选出expert 1到4对应的输入，节点(3,4)上面筛选出expert 4到8对应的输入。本地的输入对应的变量是local_hidden_states，并且保留global_local_map矩阵用来记录本地输入在global输入中原来的位置。\n计算expert的结果；\nexpert推理，得到本地输入对应的输出。计算的过程分为2种，1种是遍历每个expert单独计算，另外一种方式是将所有的expert参数合并起来一次计算，下一节我们会详细讲一下。\n将计算完的结果分发到原来的设备上；\n通过ReduceScatter的方式完成。\n以上过程存在一个问题，既Megatron的实现没有进行token drop。极端情况下所有的token都分配到一个expert上面，会直接导致节点显存崩掉，因此存在一定的不稳定性，但至少效果上面是没有损失的。\n通讯量分析 只开启TP和SP的情况下，每一个transformer层需要做4次的all-gather和reduce-scatter，transformer层和MoE层各2次，对应的通讯量为$8D\\frac{N-1}{N}$，其中D为hidden层输入的数据量。当开启了EP之后，MoE层的通讯数据由原来的$D$变为$EP\\times D$，所以EP的通讯量变为$4(EP-1)D\\frac{N-1}{N}$。\n","permalink":"https://dawson-chen.github.io/posts/megatron-lm-moe/","summary":"MoE指的是sparse mixture of experts，sparse表示推理的时候不是所有的参数都会被激活。通常情况下MoE被认为是一种scaling up模型的技术，使用同样的资源训练更大的模型，某些设定下其效果甚至可能达到与同样参数量稠密网络相当的水平（Deepseek MoE 2B，见论文）。\n最近社区里有很多MoE的开源工作，xAI发布了300B的MoE模型，苹果发布了MoE的多模态模型。不禁让人想到一个问题，MoE会是AI的未来吗？这是一个很难回答的问题，从我个人的观点出发，在硬件水平不出现巨大飞跃的前提下，答案是肯定的（Quantum come to recue\u0026hellip; i\u0026rsquo;m waiting）。一方面是因为我相信处在最前沿的模型规模还会呈现大幅的提升，需要有技术来弥补硬件水平和扩大后模型规模之间的差距，而MoE是这方面一项成熟同时具有进一步提升潜力的方法。另外一方面，从神经元活动的分布的角度来看，人脑某些区域也是稀疏的，在进化论的角度也可以被看成一种减少能量消耗的方法。再说一下为什么MoE不会是未来，首先在MoE架构理论中有很多的漏洞，比如训练中需要用辅助loss保持exert激活的均匀性，路由训练过程中会震荡。虽然这些问题都有对应的方法去解决，但这种缝缝补补的技术带来收益的同时也限制了收益的上限（MoE的scaling law中可以体现）。\n但这篇博客并不是为了讲MoE技术本身，而是解析一下megatron是如何实现MoE的训练的，以及大规模的MoE模型如何进行并行，同时增加对megatron的了解。\nMoE结构回顾 首先，看一下在最主流的transformer框架里，MoE的结构如下图所示：\n%%{ init: { \u0026#39;flowchart\u0026#39;: { \u0026#39;curve\u0026#39;: \u0026#39;bumpX\u0026#39; } } }%% graph LR x[\u0026#34;X(n-1)\u0026#34;] --\u0026gt; p1[\u0026#34;.\u0026#34;] --\u0026gt; input_ln[\u0026#34;Layer Norm\u0026#34;] input_ln --\u0026gt; attn[\u0026#34;Self Attention\u0026#34;] attn --\u0026gt; plus1((+)) p1 --\u0026gt; plus1 plus1 --\u0026gt; p2[\u0026#34;.\u0026#34;] --\u0026gt; attn_ln[\u0026#34;Layer Norm\u0026#34;] subgraph MoE Layer expert1[\u0026#34;Expert 1\u0026#34;] expert2[\u0026#34;Expert 2\u0026#34;] expert_dot[\u0026#34;...\u0026#34;] expertk[\u0026#34;Expert K\u0026#34;] end attn_ln -.-\u0026gt; expert2[\u0026#34;Expert 2\u0026#34;] attn_ln --\u0026gt; expert1[\u0026#34;Expert 1\u0026#34;] attn_ln -.","title":"Megatron-LM解读：MoE的实现方式"},{"content":"Megatron中包含了大多数目前大模型预训练中所需要的并行技术，并且相较于Deepspeed在硬件层面可以得到更多的优化支持。Megatron的优势体现在其先进的并行化设计上面，而其中流水线并行是非常重要的创新点。相比于tensor并行，流水线并行对通讯的压力更小，使得多机训练超大模型成为可能。而在现代工业生产体系里，流水线早已经是一种耳熟能详的科学管理方式。本文中我们结合工业流水线的视角，分析megatron中流水线的设计与具体实现方式。\n在大模型蓬勃发展的时代，超大模型训练对框架能力的要求越来越高，工作分工也逐渐演变成算法+框架工程师2部分合作的模式。虽然说专业的人干专业的事情，但是算法工程师对框架原理有一定了解依然是必要的。这里有3个我认为重要的理由：\n模型训练中遇到的大多数的问题是算法和工程的混合问题； 算法工程师需要能够独立开发一些小需求； 有助于和框架工程师沟通效率的提升； 认识流水线 除了曾经在流水线上短暂的工作经验之外，我对流水线的认识主要来自于一个汽车工业发展早期的故事。\n1903年的美国底特律，一位出生自普通家庭的汽车工程师亨利福特开始了他的第二次创业。福特拥有多年的汽车设计经验，在上次创业过程中他制造出了性能出众的汽车，虽然通过早期的赛事证明了自己，但最终因为汽车造价太高而导致无法被大众接受，从而导致公司破产。这次福特吸取经验打算将目光瞄向民用市场，制造可以被广大的农场主接受的汽车。\n通过多年快速的技术迭代后，在1908年福特推出了后来鼎鼎大名的T型车，出色的设计以及过硬的质量使得该车很快成为了市场上的抢手货。到了1913年，福特已经是美国最出名的企业家之一，个人财富更是达到了数十亿。此时，可以说是名利双收的福特却开心不起来，一方面T型车的订单已经排到了几个月以后，另一方面工厂的生产效率却一直提不上去。为了实现当初定下的目标，福特目前最关心的只有2件事情，一是继续扩大产能，二是降低汽车的成本。放到今天的工业生产模式下，扩大生产几乎就等于降低产品成本，但在当时的手工作坊装配模式下，扩大生产意味着人才需求的直线上升以及巨大的管理成本，福特知道生产模式的变革已经迫在眉睫。\n1913年福特引入了流水线作为装配生产的形式，从最初的车架被推上流水线，经过各个部件的装配，最终被推下流水线，全流程耗时不过几个小时。效率的提升不光将产能提升数倍，同时汽车的成本下降到原来的一半，也为后来福特推出双休制拉高工人工资奠定了基础。毫无疑问，这是福特汽车发展史上最辉煌的时代。\n福特公司在使用流水线装配T型汽车的场景 from DALL·E 为什么一个简单的流水线能够带来这么大的改变，我们分析一下流水线的特点，首先是将原来复杂的工作流程切分成多个小块，每个工人只需要负责其中某个单子的工序，降低了对装配工人技术的依赖；另外，不同于传统的扩大生产需要同时扩大生产设备的模式，流水线并不需要设备的增加，保证了成本降低；最后也是最重要的，流水线子程序划分的越细小，生产效率的提升越高。\n为什么要讲这个故事，因为在Megatron中使用的流水线并行的思想与工业生产中的流水线不能说一模一样，只能说是完全一致。对传统流水线有基本的认知可以帮助理解流水线并行，另外哪有那么多创新啊，说白了就是互相借鉴罢了。\n流水线并行设计 流水线并行的英文是Pipline Parallelism，后面简称PP。\n生产生产中的流水线是把一个复杂的过程拆分成多个简单的子过程，每个工人只负责其中的一部分。在模型推理中的PP与传统流水线是完全一致的，大模型被按照层数拆成多个子模型，由单独的GPU设备负责子模型的推理计算，每个节点做的事情就是不断的接收前面节点的输入，然后计算完并将结果传给后面的节点。整个流水线的吞吐量与流水线的细分程度相关，并成线性关系（忽略切分的不均匀度，以及进入时间和流出时间）。\n当PP用在训练过程中的时候，事情发生了一点变化。因为训练是一个双向的过程，所以整个流水线会变成如上图所示。此时流水线的进入节点和流出节点是同一个，或者可以理解为每个节点同时处在2条方向相反的流水线中。此时，每个节点执行前向动作与后向动作的序列方式就叫做流水线策略。下面从简单到复杂介绍3种不同的流水线策略，并通过简单的语言说明设计的原理。\nFill-Drain Fill-Drain就是先运行前向流水线，然后再执行反向流水线。整体的流程如下图所示：\n通过类比生产流水线，我们很好理解当流水线开始运行的时候，处于流水线后面的节点会有一段等待时间。所以出于利用率考虑，我们在使用流水线的时候，希望执行的长度越多越好。在PP里面，就是我们希望在前向流水线执行足够多的批次之后再开始执行反向流水线，从而降低设备空闲的时间，在这个图里面也可以叫做bubble time。\n然而，事情并没有那么顺利，我们知道模型在前向过程中需要记录计算的激活值，用来在反向传播的时候计算对当前节点输入的梯度。所以节点的显存上限决定了序列长度的最大值，因此引出了1F1B的流水线策略。\nOne Forward and One Backward(1F1B) 显然，如果降低了激活值带来的显存消耗，就可以尽可能的增加执行的序列长度。因为前向流水线执行过程中，输入是分批次的，每个批次对应的梯度是可以单独计算的，所以第1份输入对应的梯度并不需要等所有批次都执行完后才开始计算。由此可见，在流水线中需要尽量提前反向传播的时间是一种有效的方式。每个批次梯度计算最早的时间节点就是在最后一个前向流水线节点执行完之后，因此形成的策略就是流水线中的每个节点在执行完前向之后，只要有反向流水线的任务，就需要执行一次反向流水线的任务。\n最终形成的序列执行顺序如上图所示，这种方式相对于Fill-Drain并没有减少气泡的时间，但是因为降低了激活值占用的显存，因此可以使用更长的序列长度，从而增加了设备的利用率。\nInterleaved 1F1B 我们在与传统流水线的类比中解释过，流水线切分的越细，整体的吞吐量越高。但是当设备数量一定的情况下，有没有办法能够增加流水线切分的粒度呢，那就是Interleaved 1F1B。\n如上图所示，这种方式是将流水线划分的更细，但是因为设备数量是固定的，所以每个节点上需要执行多个子模型，最终流水线执行如下图所示：\n事实上，这种情况下每个设备节点同时处于4条流水线中，2条前向流水线，2条后向流水线。这种方式可以减少bubble time，从而提升了设备利用率。\n代码实现 其实只要理解PP设计的原理，那么代码写起来也是非常清晰的。我们先用伪代码描述整个流程，然后分析一下megatron实现过程中的要点。megatron里主要使用的是1F1B的调度方法，因此我们以1F1B为例进行分析，其他实现类似。\n虽然流水线调度看起来很复杂，但其特点是每个节点的操作逻辑基本是一致的，边界情况也很清晰，所以实现的时候只需要考虑每个节点的执行逻辑即可。\n节点间通讯 以传统的流水线为例，假设我们需要用代码实现一个多进程的流水线。那么要素一共有3个：\n等待上一个节点的执行结果； 执行当前节点的操作； 将结果传给下一个节点； PP实现的时候，因为每个节点对应了前向后向2条流水线，所以需要4种通讯能力，分别对应了前后2条流水上接受和发送的能力。megatron在实现的时候有一个专门的函数用来实现这种功能，下面是源码：\ndef communicate(tensor_send_next, tensor_send_prev, recv_forward, recv_backward): \u0026#34;\u0026#34;\u0026#34;Communicate tensors between stages.\u0026#34;\u0026#34;\u0026#34; args = get_args() # Create placeholder tensors for receive in forward and backward directions # if needed. tensor_recv_prev = None tensor_recv_next = None tensor_shape = (args.seq_length, args.micro_batch_size, args.hidden_size) dtype = args.params_dtype if args.fp32_residual_connection: dtype = torch.float if recv_forward: tensor_recv_prev = torch.empty(tensor_shape, requires_grad=True, device=torch.cuda.current_device(), dtype=dtype) if recv_backward: tensor_recv_next = torch.empty(tensor_shape, requires_grad=True, device=torch.cuda.current_device(), dtype=dtype) # Send tensors in both the forward and backward directions as appropriate. ops = [] if tensor_send_prev is not None: send_prev_op = torch.distributed.P2POp(torch.distributed.isend, tensor_send_prev, mpu.get_pipeline_model_parallel_prev_rank()) ops.append(send_prev_op) if tensor_recv_prev is not None: recv_prev_op = torch.distributed.P2POp(torch.distributed.irecv, tensor_recv_prev, mpu.get_pipeline_model_parallel_prev_rank()) ops.append(recv_prev_op) if tensor_send_next is not None: send_next_op = torch.distributed.P2POp(torch.distributed.isend, tensor_send_next, mpu.get_pipeline_model_parallel_next_rank()) ops.append(send_next_op) if tensor_recv_next is not None: recv_next_op = torch.distributed.P2POp(torch.distributed.irecv, tensor_recv_next, mpu.get_pipeline_model_parallel_next_rank()) ops.append(recv_next_op) reqs = torch.distributed.batch_isend_irecv(ops) for req in reqs: req.wait() return tensor_recv_prev, tensor_recv_next torch.distributed.P2POp是点对点操作的封装类，torch.distributed.isend是要执行的异步操作，操作执行发生在torch.distributed.batch_isend_irecv里。\n调度流程伪代码 下面为1F1B的整体流程，megatron在实现的时候将warmup和cooldown单独拆分出来，其他逻辑大体是一致的，感兴趣的可以参照着去看源代码。\ndef forward_backward_step(losses): ## 1F1B # forward stage if not cooldown_stage(): if not is_first_node(): fwd_inputs = wait recieve_forward_pipline() else: fwd_inputs = next(data_loader) fwd_outputs = model.inference(fwd_inputs) wait send_forward_pipline(fwd_outputs) # backward stage if not warmup_stage(): if is_last_node(): loss = fwd_outputs losses.append(loss) bwd_outputs = optimizer.backward(model, loss) wait send_backward_outputs(bwd_outputs) else: bwd_inputs = wait recieve_backward_outputs() bwd_outputs = optimizer.backward(model, bwd_inputs) if not is_first_node(): wait send_backward_outputs(bwd_outputs) losses = [] while len(losses) != num_micro_batches: forward_backward_step(losses) optimizer.step() 推荐大家刚开始学习megatron-lm的时候去看一下v2.0的版本，包含了pipline parallelism，tensor parallelism，data parallelism，fp16混合精度训练 的特性，一共1w行代码比较简单易懂。\n","permalink":"https://dawson-chen.github.io/posts/megatron-lm-pipline/","summary":"Megatron中包含了大多数目前大模型预训练中所需要的并行技术，并且相较于Deepspeed在硬件层面可以得到更多的优化支持。Megatron的优势体现在其先进的并行化设计上面，而其中流水线并行是非常重要的创新点。相比于tensor并行，流水线并行对通讯的压力更小，使得多机训练超大模型成为可能。而在现代工业生产体系里，流水线早已经是一种耳熟能详的科学管理方式。本文中我们结合工业流水线的视角，分析megatron中流水线的设计与具体实现方式。\n在大模型蓬勃发展的时代，超大模型训练对框架能力的要求越来越高，工作分工也逐渐演变成算法+框架工程师2部分合作的模式。虽然说专业的人干专业的事情，但是算法工程师对框架原理有一定了解依然是必要的。这里有3个我认为重要的理由：\n模型训练中遇到的大多数的问题是算法和工程的混合问题； 算法工程师需要能够独立开发一些小需求； 有助于和框架工程师沟通效率的提升； 认识流水线 除了曾经在流水线上短暂的工作经验之外，我对流水线的认识主要来自于一个汽车工业发展早期的故事。\n1903年的美国底特律，一位出生自普通家庭的汽车工程师亨利福特开始了他的第二次创业。福特拥有多年的汽车设计经验，在上次创业过程中他制造出了性能出众的汽车，虽然通过早期的赛事证明了自己，但最终因为汽车造价太高而导致无法被大众接受，从而导致公司破产。这次福特吸取经验打算将目光瞄向民用市场，制造可以被广大的农场主接受的汽车。\n通过多年快速的技术迭代后，在1908年福特推出了后来鼎鼎大名的T型车，出色的设计以及过硬的质量使得该车很快成为了市场上的抢手货。到了1913年，福特已经是美国最出名的企业家之一，个人财富更是达到了数十亿。此时，可以说是名利双收的福特却开心不起来，一方面T型车的订单已经排到了几个月以后，另一方面工厂的生产效率却一直提不上去。为了实现当初定下的目标，福特目前最关心的只有2件事情，一是继续扩大产能，二是降低汽车的成本。放到今天的工业生产模式下，扩大生产几乎就等于降低产品成本，但在当时的手工作坊装配模式下，扩大生产意味着人才需求的直线上升以及巨大的管理成本，福特知道生产模式的变革已经迫在眉睫。\n1913年福特引入了流水线作为装配生产的形式，从最初的车架被推上流水线，经过各个部件的装配，最终被推下流水线，全流程耗时不过几个小时。效率的提升不光将产能提升数倍，同时汽车的成本下降到原来的一半，也为后来福特推出双休制拉高工人工资奠定了基础。毫无疑问，这是福特汽车发展史上最辉煌的时代。\n福特公司在使用流水线装配T型汽车的场景 from DALL·E 为什么一个简单的流水线能够带来这么大的改变，我们分析一下流水线的特点，首先是将原来复杂的工作流程切分成多个小块，每个工人只需要负责其中某个单子的工序，降低了对装配工人技术的依赖；另外，不同于传统的扩大生产需要同时扩大生产设备的模式，流水线并不需要设备的增加，保证了成本降低；最后也是最重要的，流水线子程序划分的越细小，生产效率的提升越高。\n为什么要讲这个故事，因为在Megatron中使用的流水线并行的思想与工业生产中的流水线不能说一模一样，只能说是完全一致。对传统流水线有基本的认知可以帮助理解流水线并行，另外哪有那么多创新啊，说白了就是互相借鉴罢了。\n流水线并行设计 流水线并行的英文是Pipline Parallelism，后面简称PP。\n生产生产中的流水线是把一个复杂的过程拆分成多个简单的子过程，每个工人只负责其中的一部分。在模型推理中的PP与传统流水线是完全一致的，大模型被按照层数拆成多个子模型，由单独的GPU设备负责子模型的推理计算，每个节点做的事情就是不断的接收前面节点的输入，然后计算完并将结果传给后面的节点。整个流水线的吞吐量与流水线的细分程度相关，并成线性关系（忽略切分的不均匀度，以及进入时间和流出时间）。\n当PP用在训练过程中的时候，事情发生了一点变化。因为训练是一个双向的过程，所以整个流水线会变成如上图所示。此时流水线的进入节点和流出节点是同一个，或者可以理解为每个节点同时处在2条方向相反的流水线中。此时，每个节点执行前向动作与后向动作的序列方式就叫做流水线策略。下面从简单到复杂介绍3种不同的流水线策略，并通过简单的语言说明设计的原理。\nFill-Drain Fill-Drain就是先运行前向流水线，然后再执行反向流水线。整体的流程如下图所示：\n通过类比生产流水线，我们很好理解当流水线开始运行的时候，处于流水线后面的节点会有一段等待时间。所以出于利用率考虑，我们在使用流水线的时候，希望执行的长度越多越好。在PP里面，就是我们希望在前向流水线执行足够多的批次之后再开始执行反向流水线，从而降低设备空闲的时间，在这个图里面也可以叫做bubble time。\n然而，事情并没有那么顺利，我们知道模型在前向过程中需要记录计算的激活值，用来在反向传播的时候计算对当前节点输入的梯度。所以节点的显存上限决定了序列长度的最大值，因此引出了1F1B的流水线策略。\nOne Forward and One Backward(1F1B) 显然，如果降低了激活值带来的显存消耗，就可以尽可能的增加执行的序列长度。因为前向流水线执行过程中，输入是分批次的，每个批次对应的梯度是可以单独计算的，所以第1份输入对应的梯度并不需要等所有批次都执行完后才开始计算。由此可见，在流水线中需要尽量提前反向传播的时间是一种有效的方式。每个批次梯度计算最早的时间节点就是在最后一个前向流水线节点执行完之后，因此形成的策略就是流水线中的每个节点在执行完前向之后，只要有反向流水线的任务，就需要执行一次反向流水线的任务。\n最终形成的序列执行顺序如上图所示，这种方式相对于Fill-Drain并没有减少气泡的时间，但是因为降低了激活值占用的显存，因此可以使用更长的序列长度，从而增加了设备的利用率。\nInterleaved 1F1B 我们在与传统流水线的类比中解释过，流水线切分的越细，整体的吞吐量越高。但是当设备数量一定的情况下，有没有办法能够增加流水线切分的粒度呢，那就是Interleaved 1F1B。\n如上图所示，这种方式是将流水线划分的更细，但是因为设备数量是固定的，所以每个节点上需要执行多个子模型，最终流水线执行如下图所示：\n事实上，这种情况下每个设备节点同时处于4条流水线中，2条前向流水线，2条后向流水线。这种方式可以减少bubble time，从而提升了设备利用率。\n代码实现 其实只要理解PP设计的原理，那么代码写起来也是非常清晰的。我们先用伪代码描述整个流程，然后分析一下megatron实现过程中的要点。megatron里主要使用的是1F1B的调度方法，因此我们以1F1B为例进行分析，其他实现类似。\n虽然流水线调度看起来很复杂，但其特点是每个节点的操作逻辑基本是一致的，边界情况也很清晰，所以实现的时候只需要考虑每个节点的执行逻辑即可。\n节点间通讯 以传统的流水线为例，假设我们需要用代码实现一个多进程的流水线。那么要素一共有3个：\n等待上一个节点的执行结果； 执行当前节点的操作； 将结果传给下一个节点； PP实现的时候，因为每个节点对应了前向后向2条流水线，所以需要4种通讯能力，分别对应了前后2条流水上接受和发送的能力。megatron在实现的时候有一个专门的函数用来实现这种功能，下面是源码：\ndef communicate(tensor_send_next, tensor_send_prev, recv_forward, recv_backward): \u0026#34;\u0026#34;\u0026#34;Communicate tensors between stages.\u0026#34;\u0026#34;\u0026#34; args = get_args() # Create placeholder tensors for receive in forward and backward directions # if needed.","title":"Megatron-LM解读：流水线并行原理和代码解读"},{"content":"在OpenAI 2018年的一篇论文《An Empirical Model of Large-Batch Training》中就介绍了batch size的选择问题，论文中gradient noise scale作为选择batch size的关键指标。\n简单来说，我们实际中用的SGD产生的loss是真实loss的一种近似，这种近似对应的偏差和我们选择的batch size相关，batch size越大偏差越小。究其本质原因，近似的偏差与批数据的信息有关，当训练loss比较大的时候 可以认为数据中的所有信息都是相关信息，而当loss越来越小，批数据中包含的信息偏差占比会越来越高。\n论文中最大的亮点在于通过严密的推论得出上面的结论，并且推导出固定的模型要达到相同的loss，在不同的batch size下所需训练时长和需要的训练数据量之间的关系，如下所示： $$ (\\frac{S}{S_{min}}-1)(\\frac{E}{E_{min}}-1)=1 \\tag{A1} $$ 其中$E=BS$表示训练使用的数据，通过这个公式，可以得到：\nbatch size越大，训练step减小，所需的数据会增加； 训练所需的步数有最小值； 分别解释一下，第一点是因为：当数据中有用信息占比高得时候，小batch size和大batch size得到得梯度方向是差不多的，因此数据的使用率就比较低。第二点，同样是数据的利用效率，如果把batch size很大的情况下，gradient noise已经很小，继续将batch size翻倍得到的收益就很小，所以即使batch size增加到很大，依然需要一定数量的更新步数，也就是$S_{min}$。\n论文中基于gradient noise scale给出一个batch size的经验选择是$B=E_{min}/S_{min}$，在OpenAI Scaling Laws论文中进一步根据经验总结为： $$ B_{crit}\\approx \\frac{B_}{L^{1/\\alpha_B }} \\tag{3} $$ 其中，$B_$和$\\alpha_B$为经验拟合值，分别约等于$2\\times10^8$和$0.21$。\n问题：为什么梯度累积太大会导致loss升高？\n如果batch size远小于$B_{crit}$那么训练的时间会大大增加，但是并不会显著的减小需要的训练量。需要注意的是这里假设了无限的并行条件，当我们在实际中使用梯度累积增大batch size，使得更接近$B_{crit}$​，那么训练总步数会减少，但是总的时间反而会增加。\n下面进行说明，根据公式A1可以得到： $$ E=\\frac{E_{min}}{1-\\frac{S_{min}}{S}} $$ 意味着，当我通过梯度累积增加batch size，S会减小 但是为了达到同样loss所需的训练数据会增加。而梯度累积并不影响训练速度，过相同的case需要的时间是一样的，也就是需要更多的时间才可以达到同样的loss。\n这个结论也告诉我们，如果只是为了提升训练速度或者提升训练效果，梯度累积并不会有帮助。当然还由其他的影响因素，比如PP并行方式需要大的batch size提升计算效率，或者提升算法训练的稳定性。\n不同batch size下数据的修正 由公式A1得到： $$ D_{min} = \\frac{D}{1+\\frac{B}{B_{crit}}} $$ 如果2个不同的batch size，达到相同的loss所需数据量的比值为： $$ \\frac{D_1}{D_2} = \\frac{B_{crit}+B_1}{B_{crit}+B_2} $$ 通过这个修正公式可以看到，如果选择了不正确的batch size，那么会导致实际训练的token的作用并没有最大化。所以，可能会出现实际训练1.2T的token，但实际上仅相当于500B的效果。\n欢迎关注我的个人公众号：AI学舍\n","permalink":"https://dawson-chen.github.io/posts/batch-size/","summary":"在OpenAI 2018年的一篇论文《An Empirical Model of Large-Batch Training》中就介绍了batch size的选择问题，论文中gradient noise scale作为选择batch size的关键指标。\n简单来说，我们实际中用的SGD产生的loss是真实loss的一种近似，这种近似对应的偏差和我们选择的batch size相关，batch size越大偏差越小。究其本质原因，近似的偏差与批数据的信息有关，当训练loss比较大的时候 可以认为数据中的所有信息都是相关信息，而当loss越来越小，批数据中包含的信息偏差占比会越来越高。\n论文中最大的亮点在于通过严密的推论得出上面的结论，并且推导出固定的模型要达到相同的loss，在不同的batch size下所需训练时长和需要的训练数据量之间的关系，如下所示： $$ (\\frac{S}{S_{min}}-1)(\\frac{E}{E_{min}}-1)=1 \\tag{A1} $$ 其中$E=BS$表示训练使用的数据，通过这个公式，可以得到：\nbatch size越大，训练step减小，所需的数据会增加； 训练所需的步数有最小值； 分别解释一下，第一点是因为：当数据中有用信息占比高得时候，小batch size和大batch size得到得梯度方向是差不多的，因此数据的使用率就比较低。第二点，同样是数据的利用效率，如果把batch size很大的情况下，gradient noise已经很小，继续将batch size翻倍得到的收益就很小，所以即使batch size增加到很大，依然需要一定数量的更新步数，也就是$S_{min}$。\n论文中基于gradient noise scale给出一个batch size的经验选择是$B=E_{min}/S_{min}$，在OpenAI Scaling Laws论文中进一步根据经验总结为： $$ B_{crit}\\approx \\frac{B_}{L^{1/\\alpha_B }} \\tag{3} $$ 其中，$B_$和$\\alpha_B$为经验拟合值，分别约等于$2\\times10^8$和$0.21$。\n问题：为什么梯度累积太大会导致loss升高？\n如果batch size远小于$B_{crit}$那么训练的时间会大大增加，但是并不会显著的减小需要的训练量。需要注意的是这里假设了无限的并行条件，当我们在实际中使用梯度累积增大batch size，使得更接近$B_{crit}$​，那么训练总步数会减少，但是总的时间反而会增加。\n下面进行说明，根据公式A1可以得到： $$ E=\\frac{E_{min}}{1-\\frac{S_{min}}{S}} $$ 意味着，当我通过梯度累积增加batch size，S会减小 但是为了达到同样loss所需的训练数据会增加。而梯度累积并不影响训练速度，过相同的case需要的时间是一样的，也就是需要更多的时间才可以达到同样的loss。\n这个结论也告诉我们，如果只是为了提升训练速度或者提升训练效果，梯度累积并不会有帮助。当然还由其他的影响因素，比如PP并行方式需要大的batch size提升计算效率，或者提升算法训练的稳定性。\n不同batch size下数据的修正 由公式A1得到： $$ D_{min} = \\frac{D}{1+\\frac{B}{B_{crit}}} $$ 如果2个不同的batch size，达到相同的loss所需数据量的比值为： $$ \\frac{D_1}{D_2} = \\frac{B_{crit}+B_1}{B_{crit}+B_2} $$ 通过这个修正公式可以看到，如果选择了不正确的batch size，那么会导致实际训练的token的作用并没有最大化。所以，可能会出现实际训练1.","title":"Batch Size杂谈"},{"content":" 2023-11-29写； 2023-12-06修改：增加适配模型开发流程说明；增加bug解决记录； Deepspeed-Chat是一个优秀且易用的PPO开源实现，实际在使用时HybridEngine开发是PPO工程相关的重要一环，本次分享的目的：\n了解整体Deepspeed的架构，和代码逻辑； 清楚如何在Deepspeed上进行HE适配相关开发； 主要是对新的模型结构的适配； 先回答为什么需要做适配，因为HE(hybrid engine)本身解决的问题是将训练中的zero3模型，转换成更高效的对设备通讯压力不大的推理模式，可以是不带tp的全参数推理，也可以是带tp的推理。所以不管是哪种形式的推理，都需要重构推理图，并且处理好这种模式转换间设计的大量引用，也是适配需要做的全部事情。虽然我相信这个过程一定可以改成完全自动的形式，但是目前还没有找到这种实现方式。\n然后看一下HE的优势是什么，推理速度不用说，就相当于带或不带zero3的差距，通常10x左右。还有一个优势是带TP的HE方式在内存上的优势，这一点在模型比较大且显存压力较大的场景下尤为重要。下面举ppo的例子，如果训练的sft和actor模型大小是70b，reward和critic是7b，一共4个模型，考虑32张A100-80G的场景：\nZeRO3训练模型占用显存（每张卡）：\nActor: (优化器12 + 参数2 + 梯度2) * 70 / 32 = 35G SFT：通常offload，显存可以记作0G reward+critic：3.9G 每张卡总共需要：38.9G\n占用的显存是够的，但是zero3推理速度在ppo生成阶段几乎不可用；\n（HE模式）ZeRO3训练模型+TP推理占用显存（每张卡）：\nTP的size设为8\n训练阶段：TP的参数释放掉，和ZeRO3模式一样，38.9G； 生成阶段：70 * 2 = 140G（全部推理参数），TP参数切片140 / 8 = 17.5G，所以一共需要56.4G； 训练用ZeRO3节约显存，生成用HE提升速度。如果不用TP的HE那么140G放到一张卡上，目前还没有设备能支持。\n1. 整体架构 1.1 启动流程 涉及代码都在deepspeed/launcher目录下面，一共2个文件 runner和launcher。\n1.2 Zero3 zero3架构是什么样的，如何实现？\nhttps://github.com/microsoft/DeepSpeed/blob/2c2a7f31bcc20ae12ce8d2b8af14448939ebdf12/deepspeed/runtime/zero/stage3.py#L120C9-L120C9\n自动对参数进行all_gather，使用后自动释放；实现核心 ZeROOrderedDict\n1.3 Hybrid Engine 为什么需要HE？\nZero3是用来训练的并行方式，推理的时候有很大劣势，并且不能扩展到多机的情况。\nHE如何起到作用？实现方式？\nHE内容：1. 自定义算子；2. tensor parallelism；\n不带TP的方式； 适用于 7b、13b 单机多卡 带TP的方式； 适用于66b、70b 更大模型的训练 比如说模型ChatGLM2，它的代码实现里transformer块对应的实现类是 GLMBlock，HE就是实现一个新的推理过程，带或者不带TP区别就是这个推理过程是不是分布式，然后替换掉 GLMBlock的forward方法。\n这种替换方式就像下面这个例子：\nclass Bird: def move(self,): print(f\u0026#39;i\\\u0026#39;m flying\u0026#39;) class Pig: def move(self,): print(f\u0026#39;i\\\u0026#39;m walking\u0026#39;) bird = Bird() pig = Pig() pig.move = bird.move pig.move() \u0026#39;\u0026#39;\u0026#39; output \u0026gt;\u0026gt;\u0026gt; i\u0026#39;m flying \u0026#39;\u0026#39;\u0026#39; 在python里，Pig想要飞的方式是借用Bird的翅膀，而不需要通过继承实现。\n运作流程 Container、Inference、Module、ops之间的调用关系；\nHE运作流程，有2个部分：\n初始化，module到Continer的创建流程；\npolicy定义的位置； 入口：hybrid_engine.py populate_all_inference_policies policy定义文件：deepspeed/module_inject/replace_policy.py policy和container对应关系：deepspeed/module_inject/utils.py Container创建过程； Create_module 创建推理图；新的forward函数 set_params_wo_copy container将模型变量赋值给计算图；只给引用 不复制 forward的替换发生在eval方法里； 计算图的构建； 保证和原生pytorch的计算过程保持一致； 尽量使用ds提前定义的cuda算子； generate的过程；\n2. 如何适配新的模型 2.1 关键点 以chatglm2和BlueLM为例，几个关键点：\n如何定义一个新的模型； 参考HE的架构，主要新增policy、container，以及使用对应的ops重构计算图；\nPolicy： 定义原模型中关键的参数变量； 定义原模型中的参数引用； 定义要替换的模块，一般为transformer层级模块对应的类； Container； 保存计算图中需要的参数tensor引用； 生成推理用的module，用module.forward替换掉原模块中的forward； 重构推理计算图； 检查算子是否可以复用； 检查结果是否正确； TP相关的代码；\n改动代码之前，请阅读arxiv.org/pdf/1909.08053.pdf论文，了解Tensor Parallelism的大模型分布式训练方式。在HE中只关注前向推理的流程，适配时最主要的是确定好参数切分的方式。\n参数复制，拆分； 推理时的reduce操作； 计算流程实现；\n*Inference相关的代码；\nchatglm2 66b 8卡推理；\n2.2 适配开发流程 HE本质上就是用zero3参数收集起来，重新生成一个计算图（container.module.forward函数）复现并替换原pytorch模型的推理结果；\n找到要替换的类，policy对应的类； 一般在要适配的模型定义文件里面；\n找到container.module.forward调用入口； 在替换模型定义文件里，policy对应的类的forward函数的调用入口，注意入口的参数传递和container对应的forward声明对齐；\n检查点：\n参数引用是否正确； 计算流程是否一致； 单步调试； 每一步的计算和pytorch代码是否一致；\nTP代码开发的区别： container需要单独保存参数； 原始参数的切片； 在生成阶段，需要先准备tp参数；更重要的是，在生成结束之后要及时释放tp的参数； 在container的有相应的方法需要实现，主要有参数准备、参数释放； 计算阶段需要适配tp流程； 显然的变化是，计算用的weight的大小会改变； 注意：generate阶段需要对输入进行拼接； 2.3 单元测试方法 如何进行多机程序的debug；\n主要难点是多机程序的调试，这里提供2种方式；\n可以使用pdb在单节点上进行调试代码；代码如下：\n## 在需要阻塞的位置插入下面的代码 if dist.get_rank() == 0: # 判断是否为0的进程 breakpoint() # 断点 dist.barrier() # 当0进程阻塞时，其他节点进行等待 （理论可行，未做尝试）用单进程模拟多进程；\ndeepspeed启动命令中提供了 --force_multi参数，使单进程可以用来模拟多进程，并在vscode中配置启动命令用deepspeed，这样可以使用vscode进行单步调试。\npdb的了解和相关命令查看pdb \u0026mdash; Python 的调试器 — Python 3.12.0 文档。\n调试程序如何写；\n参考deepspeed/tests/下面对应的文件，比如：test_ds_bluelm.py。\n3. 开发中BUGs解决过程 INFLIGHT状态异常 耗时：2天\nINFLIGHT状态参数出现异常，解决思路：\n验证inflight状态出现的时机；\n分别在生成训练之后打印参数的状态；\n第1次生成... Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 925}) 第1次生成... 第1次训练... Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 762, \u0026lt;ZeroParamStatus.AVAILABLE: 1\u0026gt;: 163}) 第1次训练... 第2次生成... Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 758, \u0026lt;ZeroParamStatus.AVAILABLE: 1\u0026gt;: 163, \u0026lt;ZeroParamStatus.INFLIGHT: 3\u0026gt;: 4}) 发生报错 流程如上面的记录，在第二次训练的时候有参数没有释放；\n查看训练过程中，AVAILABLE状态的变量有哪些？\nAvailable 变量：\n\u0026#39;model.layers.x.self_attn.k_proj.lora_left_weight\u0026#39;, \u0026#39;model.layers.x.self_attn.v_proj.lora_left_weight\u0026#39;, \u0026#39;model.layers.x.input_layernorm.weight\u0026#39;, \u0026#39;model.layers.x.post_attention_layernorm.weight\u0026#39; 40层 * 4 = 160个 \u0026#39;model.norm.weight\u0026#39;, \u0026#39;model.embed_layer_norm.weight\u0026#39;, \u0026#39;model.embed_layer_norm.bias\u0026#39; 3个 Inflight 变量\n\u0026#39;model.layers.39.self_attn.q_proj.weight\u0026#39;, \u0026#39;model.layers.39.self_attn.q_proj.lora_right_weight\u0026#39;, \u0026#39;model.layers.39.self_attn.q_proj.lora_left_weight\u0026#39;, \u0026#39;model.layers.39.self_attn.k_proj.weight\u0026#39; 大概率和lora是有关系的，从这里下手找原因；\n修复container中lora参数的bug；\n修复mlp参数中lora没有起作用；\n修复mlp和attention中参数拷贝的bug；\nBlueLMAttention细节： self.attn_qkvw 需要等于None； self.attn_qw, self.attn_kw, self.attn_vw 在推理过程中合并成qkvw； 使用提前开辟好内存的方式 ==修复lora相关代码后，问题仍然存在。==\n测试glm2-6b用zero3训练的时候，参数状态变化情况；\n生成阶段 Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 423}) 训练阶段 Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 338, \u0026lt;ZeroParamStatus.AVAILABLE: 1\u0026gt;: 85}) 训练后第一次生成阶段 Counter({\u0026lt;ZeroParamStatus.NOT_AVAILABLE: 2\u0026gt;: 337, \u0026lt;ZeroParamStatus.AVAILABLE: 1\u0026gt;: 85, \u0026lt;ZeroParamStatus.INFLIGHT: 3\u0026gt;: 1}) [\u0026#39;transformer.encoder.layers.0.input_layernorm.weight\u0026#39;, \u0026#39;transformer.encoder.layers.0.self_attention.query_key_value.bias\u0026#39;, \u0026#39;transformer.encoder.layers.0.post_attention_layernorm.weight\u0026#39;, ... # 每一层 \u0026#39;transformer.encoder.final_layernorm.weight\u0026#39;] [\u0026#39;transformer.encoder.layers.27.mlp.dense_4h_to_h.weight\u0026#39;] 和bluelm测试的效果差不多，但是glm6b可以正常训练；\n检查glm6b的代码有什么区别；\n训练脚本； gradient checkpoint、offload不一致 container 参数复制； ==去掉gradient checkpoint之后可以正常训练；==\n梳理zero3过程，找到为什么会出现inflight状态，以及对应的意义；\nAVAILABLE 和 INFLIGHT 状态转换图 INFLIGHT # parameters are being gathered. 显存超出bug 耗时：一天\n显存超出BUG解决；\n原因分析：显存workspace申请不够，导致在使用算子计算softmax的时候超出限制； 解决方法如下： 修改自定义算子申请显存的逻辑； 代码在csrc/transformer/inference/includes/inference_context.h#L98 GenWorkSpace函数里 调大ds_bluelm.py里面申请显存的参数； 代码在deepspeed/model_implementations/transformers/ds_bluelm.py BlueLMAttention里forward计算attention的部分改成pytorch实现； pytorch动态申请显存，只要显存够就不存在溢出的风险； 第2种方法，通过调大长度参数，仍然没有解决；\n第3种方法，使用pytorch重新定义attention计算过程；\nkv-cache适配时，遇到present_key_value向量无理由自发改变的问题；\n通过改变present_key_value内存地址解决；==原因未知，如果有，就是对cuda敬畏之心不够！==\n","permalink":"https://dawson-chen.github.io/posts/deepspeed-hybrid-engine-dev/","summary":"2023-11-29写； 2023-12-06修改：增加适配模型开发流程说明；增加bug解决记录； Deepspeed-Chat是一个优秀且易用的PPO开源实现，实际在使用时HybridEngine开发是PPO工程相关的重要一环，本次分享的目的：\n了解整体Deepspeed的架构，和代码逻辑； 清楚如何在Deepspeed上进行HE适配相关开发； 主要是对新的模型结构的适配； 先回答为什么需要做适配，因为HE(hybrid engine)本身解决的问题是将训练中的zero3模型，转换成更高效的对设备通讯压力不大的推理模式，可以是不带tp的全参数推理，也可以是带tp的推理。所以不管是哪种形式的推理，都需要重构推理图，并且处理好这种模式转换间设计的大量引用，也是适配需要做的全部事情。虽然我相信这个过程一定可以改成完全自动的形式，但是目前还没有找到这种实现方式。\n然后看一下HE的优势是什么，推理速度不用说，就相当于带或不带zero3的差距，通常10x左右。还有一个优势是带TP的HE方式在内存上的优势，这一点在模型比较大且显存压力较大的场景下尤为重要。下面举ppo的例子，如果训练的sft和actor模型大小是70b，reward和critic是7b，一共4个模型，考虑32张A100-80G的场景：\nZeRO3训练模型占用显存（每张卡）：\nActor: (优化器12 + 参数2 + 梯度2) * 70 / 32 = 35G SFT：通常offload，显存可以记作0G reward+critic：3.9G 每张卡总共需要：38.9G\n占用的显存是够的，但是zero3推理速度在ppo生成阶段几乎不可用；\n（HE模式）ZeRO3训练模型+TP推理占用显存（每张卡）：\nTP的size设为8\n训练阶段：TP的参数释放掉，和ZeRO3模式一样，38.9G； 生成阶段：70 * 2 = 140G（全部推理参数），TP参数切片140 / 8 = 17.5G，所以一共需要56.4G； 训练用ZeRO3节约显存，生成用HE提升速度。如果不用TP的HE那么140G放到一张卡上，目前还没有设备能支持。\n1. 整体架构 1.1 启动流程 涉及代码都在deepspeed/launcher目录下面，一共2个文件 runner和launcher。\n1.2 Zero3 zero3架构是什么样的，如何实现？\nhttps://github.com/microsoft/DeepSpeed/blob/2c2a7f31bcc20ae12ce8d2b8af14448939ebdf12/deepspeed/runtime/zero/stage3.py#L120C9-L120C9\n自动对参数进行all_gather，使用后自动释放；实现核心 ZeROOrderedDict\n1.3 Hybrid Engine 为什么需要HE？\nZero3是用来训练的并行方式，推理的时候有很大劣势，并且不能扩展到多机的情况。\nHE如何起到作用？实现方式？\nHE内容：1. 自定义算子；2. tensor parallelism；\n不带TP的方式； 适用于 7b、13b 单机多卡 带TP的方式； 适用于66b、70b 更大模型的训练 比如说模型ChatGLM2，它的代码实现里transformer块对应的实现类是 GLMBlock，HE就是实现一个新的推理过程，带或者不带TP区别就是这个推理过程是不是分布式，然后替换掉 GLMBlock的forward方法。","title":"Deepspeed-HybridEngine开发指南"},{"content":"背景 Dense网络的scaling law如下：\n$$ Log\\ \\mathit{L}(N) \\triangleq a\\ log \\mathit N + d \\tag{1} $$\n来自Scaling laws for neural language models\n不同的分词器、模型结构、数据都会影响这2个值，所以需要重新评估。\nMoE的scaling law建模出自论文 Unified Scaling Laws for Routed Language Models, DeepMind, Feb 2022，关键的工作是基于Dense网络的scaling law，并结合MoE的实验特性，设计出新的建模。\n关键假设：MoE模型收敛后（如果没有特殊说明，后续所有的loss都是指收敛后的）的log-loss，是基底参数两log和expert数量log的双线性组合。\n表示公式如下：\n$$ log L(N, E)\\triangleq a\\ log\\ N + b\\ log\\ \\hat{E} + c\\ log\\ N log \\hat{E} + d \\tag{2} $$\n$$ where\\ \\ \\ \\ \\frac{1}{\\hat{E}} \\triangleq \\frac{1}{E-1+(\\frac{1}{E_{start}}-\\frac{1}{E_{max}})} + \\frac{1}{E_{max}} $$\n注意：其中 $log$ 函数使用的基底为10。\n解释一下其中使用到的变量：\n$E$ 表示expert的数量，$\\hat{E}$ 表示饱和化的 $E$，用来衡量expert数量变大后效果变差的衰减； $N$ 表示对应基底模型的参数量； $a,b,c,d,E_{start},E_{max}$ 为待拟合的参数； 建模方式的演进 下面介绍如何从公式(1)一步步到公式(2)的，以及对应的逻辑。\n理论推导部分 如果给定$N$，那么$E$一定程度上与整体参数量成正比；\n很容易想到\n$$ log\\ L_N(E)\\triangleq b\\ log\\ E + d\u0026rsquo; \\tag{3} $$\n$E=1$的时候代表了Dense网络的情况；\n带入公式3得到了$log\\ L_N(E)= d\u0026rsquo;$，所以有$d\u0026rsquo;=a\\ log \\mathit N + d$；\n由此可以得到公式1和公式3的结合：\n$$ log\\ L(N,E)\\triangleq a\\ log\\ N + b\\ log\\ E + d \\tag{4} $$\n到这一步，基于推论的建模就到头了，后续改动都是通过实验观察得到的。\n实验修正部分 观察1：公式4在拟合过程中，$b$会随着模型参数增大而增大。\n反映了基底模型越大的时候，expert增加带来收益的下降趋势。而在公式4中，$log N$对应的斜率是固定的$a$，因此存在误差。\n实验中发现斜率变化与$log\\ N$大概成正比，如下图：\n所以增加一项$log\\ N$与$log\\ E$的交叉特征，得到公式5。\n$$ log\\ L(N,E)\\triangleq a\\ log\\ N + b\\ log\\ E + c\\ log\\ N\\ log\\ E + d \\tag{5} $$\n此时$log\\ E$对应的斜率为$b+c\\ log\\ N$，如果c为正数那么$N$增大会让斜率增大，既log-loss下降的速度降低。所以一个好的MoE的方法应该让$c$尽量接近于0。\n观察2：因为MoE方法中的特性，$E$过大和过小都会影响模型的效果；\n比如：\n如果E多大的时候，会遇到gradient方差变大的情况（expert之间差异比较大），从而降低模型效果； 如果E特别小的时候，固定的负担（指负载平衡loss）的影响会更明显，可能影响模型效果； 因此对$E$进行饱和化处理，公式为\n$$ \\frac{1}{\\hat{E}} \\triangleq \\frac{1}{E-1+(\\frac{1}{E_{start}}-\\frac{1}{E_{max}})} + \\frac{1}{E_{max}} $$\n主要特性是$E\\to1, \\hat{E}\\to E_{start}$，$E\\to \\infty, \\hat{E}\\to E_{max}$。\n取，画出$E$从1到512过程中$\\hat{E}$的变化，可以看到当$E$增大的时候，$\\hat{E}$增加变缓 代表了增大expert数量带来的收益逐渐降低。因此，在实际使用MoE时，尽量设置不超过128的expert数量。\n至此，得到最终的scaling law建模，即公式(1)。另外，因为我们的实验以及场景都是在小于128的场景下进行的，所以饱和化带来的收益比较小，因此，可以沿用论文中的$E_{max}$和$E_{start}$设置，所需需要拟合的参数只有$a,b,c,d$ 这4个。\n论文中最终拟合的参数如下：\n等价有效参数 通过最终拟合的scaling law，可以计算出MoE设定下对应相同效果的Dense模型参数。\n计算过程很简单，解方程：$L(\\bar{N}, 1)=L(N, E)$。\n得到解：\n$$ \\bar N \\triangleq (N)^{\\alpha(\\hat{E})/\\alpha(E_{start})} (\\hat{E}/E_{start})^{b/\\alpha{E_{start}}} $$\n显而易见，EPC可以带入Dense网络的scaling law计算。\nEPC计算代码如下：\nimport numpy as np # compute EPC有效参数 E = 16 # Number of Experts N = 7_241_728_000 # Parameter Count in Base Model def compute_EPC_by_law(N, E): a, b, c, d = -0.082, -0.108, 0.009, 1.104 e_start, e_max = 1.847, 314.478 log = np.log10 def alpha(e): return a + c * log(e) E_saturating = 1 / (1 / (E-1+1/(1/e_start-1/e_max)) + 1 / e_max) factor1 = np.power(N, alpha(E_saturating) / alpha(e_start)) factor2 = np.power(E_saturating / e_start, b / alpha(e_start) ) return factor1 * factor2 通过这个公式可以计算得到一系列MoE模型设定下对应的Dense网络表，如下：\nbase参数 expert数量(等价dense参数量) 10M 8(23.88M), 16(33.89M), 32(48.12M), 64(67.24M), 128(90.77M) 50M 8(105.73M), 16(142.87M), 32(193.16M), 64(257.59M), 128(333.41M) 100M 8(200.66M), 16(265.50M), 32(351.46M), 64(459.33M), 128(583.90M) 300M 8(554.00M), 16(708.92M), 32(907.58M), 64(1.15B), 128(1.42B) 500M 8(888.35M), 16(1.12B), 32(1.41B), 64(1.76B), 128(2.14B) 800M 8(1.37B), 16(1.70B), 32(2.12B), 64(2.60B), 128(3.14B) 1B 8(1.69B), 16(2.08B), 32(2.57B), 64(3.14B), 128(3.76B) 3B 8(4.65B), 16(5.55B), 32(6.63B), 64(7.85B), 128(9.13B) 5B 8(7.46B), 16(8.77B), 32(10.30B), 64(12.02B), 128(13.80B) 7B 8(10.19B), 16(11.85B), 32(13.78B), 64(15.91B), 128(18.11B) 13B 8(18.05B), 16(20.60B), 32(23.51B), 64(26.68B), 128(29.87B) 70B 8(85.59B), 16(92.80B), 32(100.62B), 64(108.71B), 128(116.51B) 130B 8(151.69B), 16(161.39B), 32(171.74B), 64(182.23B), 128(192.18B) 200B 8(225.88B), 16(237.21B), 32(249.12B), 64(261.05B), 128(272.23B) [!important]\n应当注意，计算过程中使用的是论文中的数据，可作为参考不代表最终效果！\n最终我们期望得到这样的一组scaling law图表，用来指导后续的结构选型。 ","permalink":"https://dawson-chen.github.io/posts/moe-scaling-law/","summary":"背景 Dense网络的scaling law如下：\n$$ Log\\ \\mathit{L}(N) \\triangleq a\\ log \\mathit N + d \\tag{1} $$\n来自Scaling laws for neural language models\n不同的分词器、模型结构、数据都会影响这2个值，所以需要重新评估。\nMoE的scaling law建模出自论文 Unified Scaling Laws for Routed Language Models, DeepMind, Feb 2022，关键的工作是基于Dense网络的scaling law，并结合MoE的实验特性，设计出新的建模。\n关键假设：MoE模型收敛后（如果没有特殊说明，后续所有的loss都是指收敛后的）的log-loss，是基底参数两log和expert数量log的双线性组合。\n表示公式如下：\n$$ log L(N, E)\\triangleq a\\ log\\ N + b\\ log\\ \\hat{E} + c\\ log\\ N log \\hat{E} + d \\tag{2} $$\n$$ where\\ \\ \\ \\ \\frac{1}{\\hat{E}} \\triangleq \\frac{1}{E-1+(\\frac{1}{E_{start}}-\\frac{1}{E_{max}})} + \\frac{1}{E_{max}} $$\n注意：其中 $log$ 函数使用的基底为10。","title":"Moe的Scaling Law"},{"content":" 陈道一，12/14/2023\nMoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\n第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。\n为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。\nMoE为什么会起作用？ scaling law层面的解释\nScaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。\nMoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。\n典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：\n模型结构层面的猜想\n观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。\n观察2：模型越靠后的层学习越不够充分。\n基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。\n所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。\n[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3\nExpert真的是专家吗？ 考虑下面3个MoE路由配置的实验：\n将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1\u0026gt;2\u0026gt;3，但真实的实验结论是2\u0026gt;3\u0026raquo;1（23.22\u0026gt;23.27\u0026raquo;23.99）。\n对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。\n如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；\n以Mixtral-7B*8 MoE为例：\ndim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7.2b 计算过程如下：\ntransformers: FFNs: 176,160,768 * 8 = 45,097,156,608 gate: 4096 * 8 = 1,048,576 MA: 4096 * (128 * 48) + 4096 * 4096= 41,943,040 LN: 4096 * 2 = 8,192 total: (176,160,768 * 8 + 41,943,040 + 8,192 + 4096 * 8) * 32 = 46,440,644,608 others: embed \u0026amp; output_w: 262,144,000 total: 46,440,644,608 + 262,144,000 = 46,702,788,608 = 46B active params: (176,160,768 * 2 + 41,943,040 + 8,192 + 4096 * 8) * 32 + 262,144,000 = 12,879,921,152 = 12.8B 模型训练的并行方式分为3种，DP(data parallel) / TP(tensor parallel) / PP(pipline parallel)，MoE模型在训练时可以同时使用这3种模型外，还可以加入EP(expert parallel)方式。EP的精髓就是多个模型中共享expert，2点理解：\n因为MoE每次前向中只用到一小部分expert，如果每个模型保留完整的expert，一定会导致大多数expert空闲的情况； 如果DP是8，EP是2，那么2个模型共用一套完整的experts； 训练并行设定：TP2 DP8 EP8 （megatron方案），需要显存如下：\nmodule MoE参数/单卡 Dense参数/单卡 Emb and Output $h * vocab *2 /TP$=262 144 $h * vocab *2 /TP$=262 144 experts $hffn3 * num_experts * n_layers / TP / EP$=88 080 384 $hffn3 * n_layers / TP$=88 080 384 gate $h*num_experts * n_layers$=1 048 576 / GQA $hhdim(nhead+n_kv_heads) * 2 * n_layers / TP$=671 088 640 $hhdim(nhead+n_kv_heads) * 2 * n_layers / TP$=671 088 640 LN $h*2 * n_layers$=262 144 $h*2 * n_layers$=262 144 total 3,622,043,648 3,620,995,072 推理所需显存/单卡 7,244,087,296 7,241,990,144 训练所需显存/单卡 57,952,698,368 57,935,921,152 通过加入EP的方式，在7B的模型大小下，MoE训练所需的显存于正常7B相差不大，结论成立。\n注：实际计算中，MoE 的激活值会相比原有增大 EP 倍，和训练长度有关，以实际为准。\n结论2：expert数量越多，训练时所需的最小设备数越多；\n不超过7B的情况下，最小设备数的计算的逻辑如下：\n确定了训练使用设备的GPU显存大小，以及基底模型规模； 比如：80G，7B 确定可以使用的最小TP(tensor parallel)数； 因为每张卡上可以训练的参数量差不多是3-4B，因此TP=2的时候满足要求，每张卡3.5B； EP(expert parallel)使用最大，以节约内存； EP = num_experts 因为expert在DP中共享，所以DP必须是EP的整数倍； DP=EP 最终，最小设备数为：DP * TP。 在7B情况下，num_experts=8 最小设备数为16、num_experts=16 最小设备数为32。\n在Mixtral-7Bx8 MoE的例子中，训练时可以跑起来的最小硬件要求：A100 80G x 8张 x 2台。\n注1：超过7B时，必须要使用PP才能训练。\n注2：模型较大的情况下，还需要考虑稳定性其他因素，最优的并行组合需要实测后才能得出；\n训练成本 刨除显存以及设备数量这2点因素，训练成本以及速度计算如下：\n差异主要来自推理过程中活跃参数的数量差异； MoE 7b*8 的活跃参数是12b，所需算力相当于训练12b模型所需的算力； 推理成本 以Mixtral-7Bx8为例，整体参数量为46b，活跃参数是12b，但是在并行条件下推理速度接近于7.2b基底模型。因为虽然模型计算量翻倍了，但是模型容量主要是由宽度带来的，wide model更利于并行计算。\n但是由于计算量增加，所以在固定设备数量的前提下，对外服务的最大吞吐量会减半。也就是说：\nMoE模型会降低推理系统的最大吞吐量，但是时延变化不大； 吞吐量降低多少主要与活跃参数相关。 Mixtral-7Bx8MoE的收益和成本？ 成本\n成本主要取决于推理过程种的活跃参数，Mistral 7Bx8的MoE模型每次推理使用2个expert（top2），所以训练成本大概相当于7B模型的2倍，既训练一个12B模型所需的算力资源。\n收益\nModel active params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K LLaMA 2 7B 7b 44.4 77.1 69.5 77.9 68.7 43.2 17.5 56.6 11.6 26.1 3.9 16.0 LLaMA 2 13B 13b 55.6 80.7 72.9 80.8 75.2 48.8 16.7 64.0 18.9 58.4 6.0 34.3 LLaMA 2 33B 33b 56.8 83.7 76.2 82.2 79.6 54.4 24.1 68.5 25.0 40.9 8.4 44.1 LLaMA 2 70B 70B 69.9 85.4 80.4 82.6 79.9 56.5 25.4 73.0 29.3 49.8 13.8 69.6 Mixtral 7B 7B 62.5 81.0 81.0 82.2 80.5 54.9 23.2 62.5 26.2 50.2 12.7 50.0 Mistral 8x7B 12B 70.6 84.4 84.4 83.6 83.1 59.7 30.6 71.5 40.2 60.7 28.4 74.4 Mixtral 7B -\u0026gt; MoE 7Bx8，上涨15.6%； LLaMA 7B -\u0026gt; 13B，上涨19.4%； 结论：考虑到分数越高越难提升，mixtral的MoE 7Bx8的效果收益可以认为与直接训练13B的模型相仿。\n数据来自：Mixtral of experts | Mistral AI | Open source models\nMoE应用场景与对应的收益成本 *基础要求：建议的模型方案，所消耗的算力与基底模型保持不变。\n注：Switch-transformers, HashMoE, DeepspeedMoE等论文中的方案都可以满足该要求。\n场景一：在模型规模扩大遇到瓶颈时，通过MoE继续提升模型效果，并且使用保持相同的算力资源。\n收益：\n10%以上的效果提升； 类比mixtral 7bx8 MoE提升15%，GPT4比GPT3提升 20+% 成本：\n没有明显的成本增加； 场景二：对于线上模型，可以替换为更小基底的MoE模型。\n收益：\n在相同的推理资源下，维持相同的最大吞吐量，并大大降低模型的时延； 小基底的MoE模型使用的训练资源更少。 成本：\n没有明显的成本增加，除了开发的复杂度变高； MoE有哪些重要的工作 时间 单位 模型名称 gating sparsity Arch Jun 2020 Google GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding 引用：543 learnable token-based top2 MA\u0026gt;MoE\u0026gt;MA\u0026gt;FFN Jun 2021 Google Switch-transformers引用：1024 learnable token-based top1 MA\u0026gt;MoE Jul 2021 FAIR Hash Layers For Large Sparse Models引用：95 determinative hash-based *top1（hash决定） MA\u0026gt;MoE Apr 2022 Google Brain ST-MoE: DesigningStable and Transferable Sparse Expert Models引用：28 learnable token-based top2 [MA\u0026gt;FFN]*3 \u0026gt;MA\u0026gt;MoE Aug 2022 Google GLaM: Efficient Scaling of Language Models with Mixture-of-Experts引用：240 learnable token-based top2 MA\u0026gt;MoE\u0026gt;MA\u0026gt;FFN Oct 2022 Google Mixture-of-Experts with Expert Choice Routing引用：56 learnable expert-based top2 MA\u0026gt;MoE Oct 2022 University of Texas Residual Mixture of Experts引用：19 core + residual Top2 MA\u0026gt;RMoE MoE的scaling law： 来自 Deepmind, 2022, Unified Scaling Laws for Routed Language Models\n计算公式如下：\n需要注意的有以下几点：\nlog的基底为10；\n默认MoE的位置是 MA-\u0026gt;FFN-\u0026gt;MA-\u0026gt;MoE；\n也就是隔一层放置一个，如果是每层放置，对应的expert数量要*2；\n对应的参数有3组，分别对应了3种路由算法：S-BASE、RL-R、HASH；\n其中Hash效果最差，与self-learning参数的路由最接近；\nmega-blocks对应的方案是S-BASE的方案，效果最好；所以如果只是使用的话注意选择合适的参数；\n一个scaling law的使用场景是计算有效参数，能够把MoE的模型参数对应到对应的Dense网络参数，计算方式如下：\n实测，Mixtral 7bx8对应的有效参数大小是12.2B，与前面从效果和算力层面的推测一致。\n难收敛如何解决 原因1：MoE刚开始训练时expert是随机初始化的，并且训练过程中expert的能力在不断变化，所以初期gating网络一直在拟合变化中的mapping关系，导致收敛时间边长；\n解决方法1：分阶段训练；Evo-MoE\na. 首先训练一个正常的网络；b. 通过随机mask将expert分化成不同的expert；c. 加入gating网络，从topk逐渐降低到top1；\n解决方法2：使用固定的映射作为route，hash-based routing。\n这种方式可以提前生成token对应的expert映射，所以不存在波动的问题。\n原因2：gating网络数值不稳定性；\ngating网络在更新时，输入的logits会增大。因为softmax的导数与输入成正比，所以输入越大会导致梯度越大，容易产生不稳定。\n解决方法：加入z-loss，限制输入gating网络的logits的大小。\n其他原因：\n乘性算子：RMS LN、GEGLU使用会增加不稳定性； 越大的模型越不稳定； 相关解决方法：\nfloat32去计算gating 网络中的softmax； 使用更小的初始化参数； expert负载均衡如何解决？ 问题提出：Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\nWe have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts；\n如果不做任何控制，gating网络会倾向于给某几个expert更高的权重。\n少量的负载不均衡不会影响模型效果，但是会影响推理和训练的速度。\n解决方法：\n附加损失auxiliary loss hash-based route 微调效果不稳定如何解决？ 问题原因：\nMoE模型比Dense模型更容易在少量数据上过拟合； MoE参数微调容易带来基础性能的丢失； 解决方法：\nexpert dropout调整到比正常参数高一些； 正常0.1，expert 0.4 只更新非expert参数往往能够得到和全量更新同样的效果。 附录 相关论文速览 Adaptive mixtures of local experts\nHinton, 1991 https://direct.mit.edu/neco/article/3/1/79-87/5560\n这篇论文第一次介绍moe的方法，其主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nGoogle, Jun 2022 http://arxiv.org/abs/2101.03961\n目的还是把模型做大，容量提升 但是不明显增加推理的计算量。论文主要围绕3个问题提出解决方案：复杂度、通讯消耗、训练稳定性。实验基于T5家族模型上进行，第一次证明了这类稀疏模型可以用低精度(BF16)训练。最终效果，同样的资源情况下训练速度提升4-7倍，在多语种场景下在全部101种语言上观察到提升。\nBalancing has been previously shown to be important for training MoE models!\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nGoogle, Aug 2022 http://arxiv.org/abs/2112.06905\n第一次将MoE应用到GPT3这种规格的模型上面，最终的效果也是在29个NLP任务中领先GPT3。最大的模型参数量是GPT3的7倍，但是训练消耗仅仅是1/3。有趣的是，他们用训练使用能量来衡量训练消耗。\n**PS：**虽然模型结构和GShard完全类似，但是应用规模比其他的工作要大，所以有较高的参考价值。\nGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nGoogle, 2021 https://arxiv.org/pdf/2006.16668.pdf\n第一次将MoE应用到transformer结构中，并且介绍了很多工程的细节，比如并行 以及 负载均衡。在工程方面，提出了一套API来处理MoE高效并行的实现，将模型结构和具体实现分离，并且在使用起来更符合语义层面的理解。值得深入研究，对工程实现方面的借鉴意义更大。\nBrainformers: Trading Simplicity for Efficiency\nDeepmind, 2023 http://arxiv.org/abs/2306.00008\n提出一种新的FFN、Attn、Gate模块的排列方式，比正常的attention排列得到的模型更高效（原文quality and efficiency）。与同是MoE模型的GLaM相比，提升2倍的收敛速度，降低5倍的单步训练时长。\n新的模型结构通过自动化程序挖掘出来；top-2 has demonstrated stronger empirical performance than top-1 gating；\nTo avoid causal leakage in decoding mode, we suggest normalizing along the expert dimension for both token-based routing and expert-based routing. 因为在训练的过程中，其实是用全文做的normalize，但是decode的时候看不到全文。\n问题：在Token-based Routing Versus Expert-based Routing这章节里，为什么要做normalize？\nST-MoE: Designing Stable and Transferable Sparse Expert Models\nGoogle Brain, Apr 2022 http://arxiv.org/abs/2202.08906\n主要针对MoE模型的训练稳定性，以及微调质量无法保证 这2个问题，提出了对应的解决方案，最终得到的模型叫做ST-MoE-32B。效果层面上，通过微调的MoE模型第一次在下游任务中得到SoTA。\n主要通过模型结构以及loss的改动，增加训练的稳定性。并且对训练稳定性进行了深入的研究，定量分析了一些模型改动对稳定性的影响。可能是对MoE训练稳定性问题研究最多的一篇论文。\nMixture-of-Experts with Expert Choice Routing\nGoogle, Oct 2022 https://arxiv.org/pdf/2202.09368.pdf\n文章的动机是解决不好的routing策略，导致expert训练不充分的问题。不同于正常的routing策略是token选择experts，本文中提出的策略是expert选择tokens。对比了Switch Transformer和GShard 2种方法，发现训练速度可以提高1倍，并且效果GLUE和SuperGLUE上表现更好。\n第一次提出expert-based routing的方式，后来在BrainFormer中有被用到。为什么这种方式会更好呢，背后的直觉是什么？\nFrom Sparse to Soft Mixtures of Experts\nGoogle Deepmind, Aug 2023 http://arxiv.org/abs/2308.00951\n视觉方向的一篇MoE论文，主要解决的问题是：训练不稳定、token dropping、experts扩展、微调效果。提出一种完全可导的稀疏MoE结构（==正常的MoE不是也可导吗？==）本质上，这篇文章也提出了一种新的Routing策略，对比了Token-based和Expert-based这2种routing策略，在推理速度、expert数量扩展上都有不同程度的优势。\nHash layers for large sparse models\nFacebook AI, Jul 2021 http://arxiv.org/abs/2106.04426\n通过对token Hash选择expert，省去了gating网络，优于SwitchTransformer和传统的Transformer模型。同时探索了不同的hash算法对应的效果，并且分析hashing-routing方法的有效性。\n感觉上Hash方法更接近于随机去选择expert，为什么还能起到效果，很奇怪？Hash如何与语义关联的？\n正常的gating网络和expert需要一起进行训练，而在刚开始的时候gating网络是随机的，expert之间也没有什么区别，那么随着expert不断训练它所包含的知识也在改变，而gating网络去学习这种变化的mapping关系，可能会导致模型最后效果很差。PS：本质上还是如何将expert训练的更有差异化的问题。基于这个问题，这篇文章提出的hashing至少在训练过程中会更稳定，确定了hash算法，那么token和expert之间的mapping关系就定下来了。PS:直觉上可以加快模型训练的速度。\n这篇论文实验非常详细，感觉对于routing策略来说是一个很强的baseline。结论说明一点，固定的routing方法能得到更好的训练结果\nGo Wider Instead of Deeper\nNational University of Singapore, Sep 2021 http://arxiv.org/abs/2107.11817\nMOE模型虽然保证计算量不增加太多，但是模型占用显存是显著增加了的。这篇论文中通过层间共享参数的机制，降低了显存的占用，并且得到的效果还不错（类似ALBERT结合MOE）。\n把ALBERT中层共享参数的方法与MoE模型，然后声称是Wider Net。然而，计算过程还是会有多层的推理，每一层的layer norm使用的是不同的参数。有点标题党的嫌疑，并且测试模型的规模都比较小，没有太多可以借鉴的地方。\nAdaptive mixtures of local experts论文阅读 BP提出自1986年，这篇文章写自1991年。\n主要想解决的问题是：当BP用来训练一个解决多个不同问题的网络时，因为任务之间的干涉导致模型难以收敛和泛化。虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n原来的公式：\n$$ \\mathbb{E}^c = {\\parallel \\vec{d^c}-\\sum_i{p_i^c \\vec{o_i^c}} \\parallel}^2 $$\n其中，$c$ 是case，$d$是目标输出，$p$ 是expert对应的gate weight，$o$ 是export对应的输出。\n分析：如果其中1个expert的输出（$o_i$）改变，导致expert所有输出的调和超过了最终的输出（$\\vec{d^c}$），那么所有expert梯度都会变为反方向。造成的结果就是，expert之间会互相干扰，导致收敛速度变慢，并且学习到的expert会倾向于合作的关系，共同作用于1个case并得到最终结果。\n那么有没有方法expert之间更倾向于竞争关系呢。\n这个问题有2篇相关的论文：\nLearning piecewise control strategies inamo dular connectionist architecture Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks ==TODO==：一种方法是给目标函数添加惩罚项，来奖励expert之间更多的竞争。\n如果把loss改成如下：\n$$ \\mathbb{E}^c = \\sum_i p_i^c{\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel}^2 $$\n最直观的变化是，每个expert的输出单独拿出来和目标进行比较，所以不管gate网络和其他网络的输出是什么，每个expert得到的梯度方向只取决于自身的输出和目标的差值。但如果其中某个expert输出对应的error变小，那么gate网络对应该expert的概率就会增加。这点可以通过梯度 $\\frac{\\partial \\mathbb{E}^c}{p_i}={\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel}^2$很明显的看到，$p_i$梯度大小取决于error平方的大小，梯度越大下降的越快。\n新的目标函数对应每个expert的梯度如下：\n$$ \\frac{\\partial \\mathbb{E}^c}{\\partial \\vec{o_i^c}}=-2p_i^c(\\vec{d^c}-\\vec{o_i^c}) $$\n此时会出现一个问题，当初期网络随机初始化后，可以认为每个expert对应的权重$p_i^c$是相同的，那么每个expert的梯度取决于error的大小，所以拟合最好的expert得到的梯度最小。为了解决这个问题，提出了新的目标函数如下：\n$$ \\mathbb{E}^c=-log\\sum_i{p_i^c e^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel^2}} $$\n此时每个expert输出对应的梯度为：\n$$ \\frac{\\partial \\mathbb{E}^c}{\\partial \\vec{o_i^c}}=-\\Big \\frac{p_i^c e ^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_i^c} \\parallel^2}}{\\sum_j{p_j^c e ^{-\\frac12\\parallel \\vec{d^c}-\\vec{o_j^c} \\parallel^2}}} \\Big $$\n梯度的前部分确保了表现好的expert能够得到更快的训练。\n","permalink":"https://dawson-chen.github.io/posts/moe-introduce/","summary":"陈道一，12/14/2023\nMoE的发明的动机是什么？ MoE提出自hinton 1991年的论文Adaptive mixtures of local experts，主要目的是加快模型训练的速度。因为正常训练过程中，如果使用同一个网络去拟合全部的数据，因为数据是多样性的，所以拟合的过程会比较慢。这时候如果有多个专家网络，每个专家网络只拟合一部分的数据，那么模型整体的学习速度以及泛化的能力都会增强。\n第一次将MoE应用到transformer中的工作是Google 2021年的GShard，并且确定了最近几年MoE工作的主要动机：保持相同训练和推理资源的同时，通过增加模型的体积代价来提升模型学习效果。\n为什么在1991年提出直到最近才重新进入视野？ 1991年还处在BP算法刚刚提出来的阶段，最优的模型也就是多层感知机。当模型本身的容量较低的时候，在复杂的场景下，用一个网络去拟合所有的数据，会因为数据的多样性，所以拟合的过程会比较慢。所以MoE被提出用来增加模型在复杂场景下学习的效果，虽然在LLM时代，只要有足够的算力和数据，模型规模扩大一定能带来更好的效果。但在当时的算力稀缺并且缺少模型scaleup需要的技术时，这种方法还是可以提高参数利用率的。\n而当LLM发展到GPT3的规模，推理和训练对应的优化方法也趋近于完善，继续scale up更多依赖于硬件的提升。那么当算力发展变缓或者获取成本变高的时候，就需要另外一种可以继续scale up但不那么依赖于硬件的方式，MoE开始进入人们的视野。\nMoE为什么会起作用？ scaling law层面的解释\nScaling laws for neural language models揭示了模型规模、数据大小、以及算力大小之间的规律，并且建议对算力最佳利用方式是：在固定数据集大小下 尽量训练更大规模的模型。这里的规模一般指的是模型参数数量 以及 需要的计算量，参数量增加同时计算量也会增加。\nMoE相当于一次解耦，只增加模型参数数量、同时保持需求计算量相对恒定，所以效果提升符合scaling law的规律。\n典型的MoE在transformer结构中应用如下（switch-transformer, Google, Jul 2022）：\n模型结构层面的猜想\n观察1：FFN可以理解为KV对，K代表了文本模式、V代表了文本分布^[1]^，越靠后的层学习到越复杂的模式。\n观察2：模型越靠后的层学习越不够充分。\n基于这2个观察，可以做出一个假设：学习不充分的原因是容量不够，越靠后的层对容量需求越大。支撑这个假设的一个实验观察是：如果只有一个MoE层，放在越靠后的位置，最终效果越好^[3],[4]^。另外，从直觉上复杂的模式对应的数量是简单模式的指数倍，需要更多的参数去拟合。\n所以MoE通过增加FFN的数量，增加了模型的容量，可以学习到更多的文本模式，所以得到更好的效果。\n[1] Transformer Feed-Forward Layers Are Key-Value Memories [2] Deepspeed-MoE#PR-MoE [3] Hash Layers For Large Sparse Models#Figure 3\nExpert真的是专家吗？ 考虑下面3个MoE路由配置的实验：\n将语义相似的token路由到相同的expert上； 将语义相似的token路由到不同的expert上； 随机将token指定到一个固定的expert上； 如果把expert理解成不同领域的专家，那么应该是1\u0026gt;2\u0026gt;3，但真实的实验结论是2\u0026gt;3\u0026raquo;1（23.22\u0026gt;23.27\u0026raquo;23.99）。\n对此一个合理的解释是：experts并不是理解中的”领域专家“ 分别学习不同的领域知识，而是增加对相似文本模式之间的区分度；相似的文本模式更可能发生在相似的token上面，所以相似的token应该路由到不同的expert上。\n如何计算训练和推理成本？ 结论1：MoE训练所需的显存与基底模型并没有明显差别；\n以Mixtral-7B*8 MoE为例：\ndim n_layers hidden_dim n_heads n_kv_heads vocab_size num_experts_per_tok num_experts 4096 32 14336 32 8 32000 2 8 整体参数量：46b 活跃参数量：12b 对应基底模型参数量：7.","title":"Moe(Mixtrue of Experts)技术调研"},{"content":"PPO的过程就像所有的强化学习一样，从原始的采样分布出发，不断循环3个步骤：采样、奖励、优化（policy gradient）。结合提前训练好的人类偏好模型得到奖励信号，从而实现与人类偏好对齐的目的。\nppo算法提出是在2017年，应用在语言模型上的相关工作最早发表于2019、2020年（Fine-Tuning Language Models from Human Preferences，Learning to summarize with human feedback），并且后续相关的多个开源代码并未有大改动。然而后来者仍然在使用ppo实现对齐偏好的效果上挣扎，由此可以猜测该技术的应用难度要高于技术难度，而公开的论文中只给出一些实验性质的浅层说明，真正的核心隐藏在只言片语中，需要后来者结合实践慢慢发掘。因此本文尝试将实践中获取到的一些认知记录下来，给后续的技术应用作为参考。\nPPO训练的条件 从ppo的流程来看，一共分为3个阶段：采样、奖励、优化。优化阶段主要是算法的事情，比如说一些trick和参数调整，在个别任务中可能参数调节非常的敏感，但总的来看一个稳定的版本在大多数情况下是够用的。这些trick和参数在开源的工作中都可以见多，基本上都是大同小异。反而是在ppo看来2个固定的环节：采样和奖励，对最终的效果影响最大。采样的模型来自于sft的训练结果，奖励来自于训练好的偏好模型，前者决定了ppo的理论上限，后者决定了ppo的实际训练上限。\n总的来说，想要通过ppo提升模型的能力，需要保证2个条件：\n足够的采样空间； 考虑一个极端的情况，如果每次采样的结果相差都不大，那么很快critic-actor会收敛到稳态（critic预测的value接近真实的q-value，advantage接近于0，actor loss接近于0）； 如果把ppo理解成在sft的采样空间里做熵减，那么采样空间的范围就决定了ppo可以达到的上限； 另外，如果sft的熵足够低，那么意味着采样会非常集中，从效率的角度其实是不利于强化学习训练的，因为agent探索的效率很低，导致模型难以收敛。 在采样空间上保证一定的准确率； ppo根据奖励和KL散度惩罚来调整对应生成字符的概率分布，所以准确率很重要； reward model的泛化性很重要，因为随着ppo的训练，采样分布一直在变； KL惩罚因子一定程度上保证了rm在采样空间上的准确率。 SFT采样空间衡量 因为ppo是从sft开始训练的，并且其过程依赖于采样的多样性，所以衡量采样空间的大小能从感官上预测ppo模型的训练效果。\n实践中可用的指标包括以下5个：\nentropy of next token probability; 下一个字符概率分布的熵 distribution of reward score; 采样分布上奖励值的分布 maximum next token probability; 下一个字符的最大概率值 sampled token probability; 采样token的概率值 number of possible token; 可采样token的数量 正则化reward score\n上图中记录了一次ppo训练过程中的多个reward score分布，几乎看不到有任何的变化。\n因为reward score分布其实是2个变量的叠加分布，即：不同case上得到的reward score的分布 + 同一case上生成不同case的reward分布。我们希望用同一case上生成不同case的reward分布来反应采样空间的大小，所以采用以下正则方法。\n正则方法：reward norm = reward - mean reward from same prompt + global mean reward\n正则后的reward score分布一定程度上可以反应出模型采样空间大小，且同一case上的reward范围直接影响ppo的学习效果，因此是一个比较重要的指标。\n影响因素 生成参数\n在ppo采样过程中主要用到的生成参数是temperature、top_k、top_p，整体来说影响不大的是temperature，但是增大到1以上会引起模型性能急剧下降，其余参数top_p, top_k影响比较小。\n不同epoch的sft模型\n在SFT过程中，随着训练epoch的增加，sft的训练数据上的生成概率分布熵会不断的减小，并且不断的接近熵的下限，即 model collapse模式塌陷。\n一般我会用模型生成的答案对应的reward的分布密度图去衡量不同epoch之间的差异，选好sft的epoch是非常重要的一步。因为我们要保证ppo过程中的生成多样性，所以要保证初始化模型在ppo训练数据上的熵足够大。\n下图有一个密度图。\n当然，sft的熵降低的程度也取决于训练数据的分布。\nRM准确率 reward model的准确率，指的是rm打分的偏序关系和人工标注相同的概率。因为ppo依赖于正确的reward奖励进行训练，所以rm在ppo训练过程中的准确率决定了其在人工标注上提升的上限。\n从实践出发，一方面要在不同版本的sft采样空间上保持高准确率；另一方面在ppo训练过程中保持准确率。前者在sft上不断采样，并且通过rm的自测，应该是可以保证的；而后者是比较难实现的，因为rm对ppo采样的数据占比比较少；另外，在ppo训练过程中有没有有效的手段能够检测reward模型的准确率呢？\n一个简单的方法就是尽量多的从ppo模型上采样，并且持续迭代RM的训练数据。ppo的采样至少要提到和sft一样多的比例，这样才能保证训练过程中reward有一个比较稳定的准确率。\n那么在ppo训练过程中，有没有办法能够检测当前PPO采样空间下rm的准确率呢？\nreward score的分布的方差；\n是ppo-max的论文里首先提出来的。\n在实际中我并没有发现这种大范围的长尾分布，但是确实在一些常见的失效模式上发现了分布方差的变化。下面几个图记录了几组不同的ppo训练过程中，reward 分布的变化。\n这种检验方式不太敏感，只能发现一些比较大的失效模式，比如说rm的漏洞、kl散度过大，对于rm准确率下降无明显反应，如上图左1所示。\nGPT4打分；\n因为目前reward model训练数据的打分也来自于gpt4，所以用gpt4去监测reward model在当前ppo采样空间上的偏序关系准确率，也是比较可行的一种方案。\n标注偏好数据测试集上的logp对比；\n标注过的偏好数据可以和人类偏好对齐，通过logp的对比可以得出choice和reject答案对应的概率，如果choice的概率更高则认为是正确，通过logp可以得到一个准确率。\nppo收益比较小的原因 原因一：采样空间太小\nppo如果要提升模型在reward model上的分数，那么依赖于在采样空间内做熵减。但是如果采样空间本来已经很小，那么ppo的上限就被确定了。\n对于一个固定大小的模型，我们观察到的是它是有熵的下限的，也就是不管用sft或者ppo，只要达到了这个下限，就很难去继续做熵减了。这可以解释一种普遍的认知，大量的标注数据+sft可以一定程度代替RLHF。\n单从熵减的角度，目前看来sft和ppo的效果是一样的，也就是说，在固定的数据范围内，如果sft已经把熵推到了极限，那么ppo很难在上面做到效果，不管算法细节怎么变。\n原因二：数据集选择\n我认为相对于sft，ppo很重要的一点优势就是可以在更大的范围上优化。因为ppo从采样中学习，即使拿到的sft在某类数据上没有学习的很好，只要它可以在采样中有个别比较好的答案，都可以学习优化。这点虽然sft也可以通过采样数据质检得到，但是效率的上限一定弱于ppo。\n而目前ppo在做数据上关注的是维度对齐，但是我们使用的数据范围仍然在sft的数据范围里面。sft在自己训练数据上采样的平均熵是0.12，在ppo的训练数据上采样的平均熵是0.19，差距非常小。通过这个数据，可以说明sft在ppo的训练数据上也拟合的很好了。\n也就是说，本来ppo有更快实现泛化的能力。\n如何解决 我们可以想一个问题：为什么OpenAI和Anthropic可以用ppo去不断迭代模型呢？\n最重要的一点，这2个工作中ppo用的指令数据都是来自线上用户，意味着每次新的训练指令中都用之前没有见过的指令（至少在相当长的一段时间，新指令的占比是比较高的）。预训练模型在经过sft后可以解决一个小范围的问题，随着迭代这个范围的圈子不断增加。与sft差别较大的指令数据集上，模型的采样空间会增大，从而保证了ppo继续训练的上限。\n另外，选用ppo迭代侧面说明，ppo有比sft更高的迭代效率。支持这点的要素有：\nRM模型规模更小，意味着RM过拟合的风险低，泛化性更好； 因为泛化性好，所以有更长的迭代周期； 众所周知，PPO不需要答案数据进行训练，数据利用率更高； 总而言之，解决思路就是 先找到ppo可以学习的范围，然后发挥ppo的迭代效率。这里面核心观点是要改变ppo初始化sft的采样空间，以下是几个具体的点：\n数据集\n在sft的基础上要扩大ppo数据范围，对于sft已经拟合比较好的场景，数据需求少一点，其他未拟合场景上需求多一点。这里说的数据范围，包括大的维度，也包括指令格式和内容等。\n初始化sft\n就算是在sft的训练数据上，训练过度也会导致过拟合，降低在训练数据范围上的泛化性，所以ppo选择的sft版本应该应该在一个平衡点上。\n如何选择这个点，可以类比强化学习里的探索和利用，如果ppo学习的范围是不同于sft的，我们希望agent可以更多去探索，选靠前的epoch；反之亦然。\n感想 PPO算法的作用就是，降低采样分布中低分区域的概率密度，增大高分区域的概率密度，以最大化可以获得的奖励。从这个角度理解，PPO的本质就是在agent的采样空间上面做熵减的过程。\n当一项新的技术提出来以后，往往人们对它的效果会过度的预估，而真正在应用过程中一些工程细节往往才是制约它发挥的决定性因素，比如：工程实现、针对业务的调整、甚至高效的分工。但是，我依然认为技术没有好坏，只有合不合适。\n","permalink":"https://dawson-chen.github.io/posts/ppo-practice/","summary":"PPO的过程就像所有的强化学习一样，从原始的采样分布出发，不断循环3个步骤：采样、奖励、优化（policy gradient）。结合提前训练好的人类偏好模型得到奖励信号，从而实现与人类偏好对齐的目的。\nppo算法提出是在2017年，应用在语言模型上的相关工作最早发表于2019、2020年（Fine-Tuning Language Models from Human Preferences，Learning to summarize with human feedback），并且后续相关的多个开源代码并未有大改动。然而后来者仍然在使用ppo实现对齐偏好的效果上挣扎，由此可以猜测该技术的应用难度要高于技术难度，而公开的论文中只给出一些实验性质的浅层说明，真正的核心隐藏在只言片语中，需要后来者结合实践慢慢发掘。因此本文尝试将实践中获取到的一些认知记录下来，给后续的技术应用作为参考。\nPPO训练的条件 从ppo的流程来看，一共分为3个阶段：采样、奖励、优化。优化阶段主要是算法的事情，比如说一些trick和参数调整，在个别任务中可能参数调节非常的敏感，但总的来看一个稳定的版本在大多数情况下是够用的。这些trick和参数在开源的工作中都可以见多，基本上都是大同小异。反而是在ppo看来2个固定的环节：采样和奖励，对最终的效果影响最大。采样的模型来自于sft的训练结果，奖励来自于训练好的偏好模型，前者决定了ppo的理论上限，后者决定了ppo的实际训练上限。\n总的来说，想要通过ppo提升模型的能力，需要保证2个条件：\n足够的采样空间； 考虑一个极端的情况，如果每次采样的结果相差都不大，那么很快critic-actor会收敛到稳态（critic预测的value接近真实的q-value，advantage接近于0，actor loss接近于0）； 如果把ppo理解成在sft的采样空间里做熵减，那么采样空间的范围就决定了ppo可以达到的上限； 另外，如果sft的熵足够低，那么意味着采样会非常集中，从效率的角度其实是不利于强化学习训练的，因为agent探索的效率很低，导致模型难以收敛。 在采样空间上保证一定的准确率； ppo根据奖励和KL散度惩罚来调整对应生成字符的概率分布，所以准确率很重要； reward model的泛化性很重要，因为随着ppo的训练，采样分布一直在变； KL惩罚因子一定程度上保证了rm在采样空间上的准确率。 SFT采样空间衡量 因为ppo是从sft开始训练的，并且其过程依赖于采样的多样性，所以衡量采样空间的大小能从感官上预测ppo模型的训练效果。\n实践中可用的指标包括以下5个：\nentropy of next token probability; 下一个字符概率分布的熵 distribution of reward score; 采样分布上奖励值的分布 maximum next token probability; 下一个字符的最大概率值 sampled token probability; 采样token的概率值 number of possible token; 可采样token的数量 正则化reward score\n上图中记录了一次ppo训练过程中的多个reward score分布，几乎看不到有任何的变化。\n因为reward score分布其实是2个变量的叠加分布，即：不同case上得到的reward score的分布 + 同一case上生成不同case的reward分布。我们希望用同一case上生成不同case的reward分布来反应采样空间的大小，所以采用以下正则方法。\n正则方法：reward norm = reward - mean reward from same prompt + global mean reward","title":"PPO实践经验"},{"content":" Polya said it well: “When you have satisfied yourself that the theorem is true, you start proving it.”\n开头的话意思是，“当你坚信一个理论是正确的时候，再开始尝试证明它“。这句话里隐藏了一个很重要的观点，数学理论在很多时候的作用是去证明你已有的想法，而不是通过理论推导获取到新的想法。\n今天我们要说的旋转位置编码（RoPE, Rotary Position Embedding），以及它的前导工作复数位置编码（Complex Position Embedding），或许就是这种观点的2个实践例子。如果你首先看到的是它们发表出来的数学公式，你可能会没有耐心看完，所幸它们的代码实现并不难，就算弄清楚它们的原理对实际使用并没有什么帮助。但可惜的是，你也会失去了2次为精妙的idea拍手称赞的机会。\nRoPE包括复数位置编码，这2者背后的想法都是非常简单且直观的，但是它们相关的理论推导又是平凡且枯燥的。这也正是数学的奇妙之处，论抽象，没有什么事物能比得过它。但学习数学的精髓，就是掌握它的这种抽象，如果数学只是死记硬背的公式，不好意思，它并不是什么神秘的咒语，不会给你带来一丝丝魔力。所以我们今天用简单的语言说明一下它们背后的观点。\n什么是复数 因为这2个工作都是建立在复数理论之上，所以我们要耐着性子看一下复数的本质。还好，虽然复数的名字是“复杂的数（Complex number）”，但它的本质是将事情变得简单，不得不说是一次起名上的重大失误。\n在有理数还只有正数的年代（1700s），人们并不会理解负数有什么实际意义。今天人们对复数也有着同样的误会，它的本质是旋转。试想有一个有理数的数轴上，1乘以-1表示在数轴上逆时针翻转180°，那么有没有一个数能让$1\\times x \\times x=-1$，即施加2次使得1进行翻转呢，那就是逆时针翻转90°，这就是$i$的直观理解。\n顺着这个想法，正常的指数增长是指固定增长率下的持续增长，那么复指数表示固定速率下的持续旋转。欧拉公式$e^{i\\pi}=-1$表示将一个数持续旋转弧度$\\pi$的情况下，它将指向相反的方向。\n在物理中复指数还用来表示正弦信号，因为正弦信号的来源也是旋转运动。\n复数位置编码 不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。\n来自 让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces (kexue.fm)\n以上内容概括了位置编码的作用，以及2大类：绝对位置编码、相对位置编码。但是总体来说都是在一个框架里，即将绝对位置或者相对位置当做一个token，对应1个固定的多维向量，即位置编码。\n复数位置编码使用了一种巧妙的视角将文字编码和位置编码融为一体，即将文字向量的每一个维度看成1个正弦信号，所以每个文字的embedding就是由N个幅度、初始相位、频率各不相同的信号组成，信号的横轴就是文字所处的位置编号。虽然单一的正弦信号是有周期的，但是N个正弦信号的组合可以使公共周期变得非常大，也就是这种信号可以表示非常长的距离信息。\n用信号集合表示位置编码还有2个显而易见的好处：\n位置无关的平移特性：任意2个位置之间的转换，即每个信号以各自的速率转动相同的时间，这个转换本身与文字当前所处的位置无关； 有界性：正弦信号是上下界的，这点对网络的训练稳定至关重要。 需要说明的是，把信号当做复数位置编码的背后逻辑是我个人的理解，原论文中只有数据理论与证明。\n旋转位置编码 “两个二维向量的内积，等于把它们当复数看时，一个复数与另一个复数的共轭的乘积实部。”\n来自 让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces (kexue.fm)\n旋转位置编码的理解需要基于一个抽象：如果将二维向量看做复数，那么向量内积相当于一个复数的角度减去另一个复数的角度，并将它们的模相乘。\n上述抽象基于2个事实：\n复数相乘的含义：1. 模相乘；2. 角度相加； 复数的共轭：指模相同，但是角度相反的复数。 这2个向量在Attention计算中分别代表了Q和K，首先在内积之前对这2个向量进行旋转，旋转角度大小与位置成正比关系。那么在做self-attention的时候，基于前面所说的抽象本质，内积的信息里面包含了Q和K的旋转角度之差，这个差是只依赖于位置差的，所以满足位置无关的平移特性。\n那么在多维的情况下，可以把embedding看作多组复数的组合，这种类比依然适用。\n总结 虽然不能说RoPE是从复数位置编码衍生出来的，因为设置更加巧妙更加简洁，但是这种近乎于直觉的想象力+严密的推理 似乎是它们共同的风格。\n数学一定要有直观的意义吗，我认为是的。虽然并不是所有的数学发现都是从实际出发而得来的，但是最终它们一定会用来解决实际的问题。如果没有了这种直观的想象力，那么仅仅从公式推理去做研究，就如同水木失去本源难以发展，又如空中楼阁难以稳固。\n","permalink":"https://dawson-chen.github.io/posts/rope-mechanism/","summary":"Polya said it well: “When you have satisfied yourself that the theorem is true, you start proving it.”\n开头的话意思是，“当你坚信一个理论是正确的时候，再开始尝试证明它“。这句话里隐藏了一个很重要的观点，数学理论在很多时候的作用是去证明你已有的想法，而不是通过理论推导获取到新的想法。\n今天我们要说的旋转位置编码（RoPE, Rotary Position Embedding），以及它的前导工作复数位置编码（Complex Position Embedding），或许就是这种观点的2个实践例子。如果你首先看到的是它们发表出来的数学公式，你可能会没有耐心看完，所幸它们的代码实现并不难，就算弄清楚它们的原理对实际使用并没有什么帮助。但可惜的是，你也会失去了2次为精妙的idea拍手称赞的机会。\nRoPE包括复数位置编码，这2者背后的想法都是非常简单且直观的，但是它们相关的理论推导又是平凡且枯燥的。这也正是数学的奇妙之处，论抽象，没有什么事物能比得过它。但学习数学的精髓，就是掌握它的这种抽象，如果数学只是死记硬背的公式，不好意思，它并不是什么神秘的咒语，不会给你带来一丝丝魔力。所以我们今天用简单的语言说明一下它们背后的观点。\n什么是复数 因为这2个工作都是建立在复数理论之上，所以我们要耐着性子看一下复数的本质。还好，虽然复数的名字是“复杂的数（Complex number）”，但它的本质是将事情变得简单，不得不说是一次起名上的重大失误。\n在有理数还只有正数的年代（1700s），人们并不会理解负数有什么实际意义。今天人们对复数也有着同样的误会，它的本质是旋转。试想有一个有理数的数轴上，1乘以-1表示在数轴上逆时针翻转180°，那么有没有一个数能让$1\\times x \\times x=-1$，即施加2次使得1进行翻转呢，那就是逆时针翻转90°，这就是$i$的直观理解。\n顺着这个想法，正常的指数增长是指固定增长率下的持续增长，那么复指数表示固定速率下的持续旋转。欧拉公式$e^{i\\pi}=-1$表示将一个数持续旋转弧度$\\pi$的情况下，它将指向相反的方向。\n在物理中复指数还用来表示正弦信号，因为正弦信号的来源也是旋转运动。\n复数位置编码 不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。\n来自 让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces (kexue.fm)\n以上内容概括了位置编码的作用，以及2大类：绝对位置编码、相对位置编码。但是总体来说都是在一个框架里，即将绝对位置或者相对位置当做一个token，对应1个固定的多维向量，即位置编码。\n复数位置编码使用了一种巧妙的视角将文字编码和位置编码融为一体，即将文字向量的每一个维度看成1个正弦信号，所以每个文字的embedding就是由N个幅度、初始相位、频率各不相同的信号组成，信号的横轴就是文字所处的位置编号。虽然单一的正弦信号是有周期的，但是N个正弦信号的组合可以使公共周期变得非常大，也就是这种信号可以表示非常长的距离信息。\n用信号集合表示位置编码还有2个显而易见的好处：\n位置无关的平移特性：任意2个位置之间的转换，即每个信号以各自的速率转动相同的时间，这个转换本身与文字当前所处的位置无关； 有界性：正弦信号是上下界的，这点对网络的训练稳定至关重要。 需要说明的是，把信号当做复数位置编码的背后逻辑是我个人的理解，原论文中只有数据理论与证明。\n旋转位置编码 “两个二维向量的内积，等于把它们当复数看时，一个复数与另一个复数的共轭的乘积实部。”\n来自 让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces (kexue.fm)\n旋转位置编码的理解需要基于一个抽象：如果将二维向量看做复数，那么向量内积相当于一个复数的角度减去另一个复数的角度，并将它们的模相乘。\n上述抽象基于2个事实：\n复数相乘的含义：1. 模相乘；2. 角度相加； 复数的共轭：指模相同，但是角度相反的复数。 这2个向量在Attention计算中分别代表了Q和K，首先在内积之前对这2个向量进行旋转，旋转角度大小与位置成正比关系。那么在做self-attention的时候，基于前面所说的抽象本质，内积的信息里面包含了Q和K的旋转角度之差，这个差是只依赖于位置差的，所以满足位置无关的平移特性。\n那么在多维的情况下，可以把embedding看作多组复数的组合，这种类比依然适用。\n总结 虽然不能说RoPE是从复数位置编码衍生出来的，因为设置更加巧妙更加简洁，但是这种近乎于直觉的想象力+严密的推理 似乎是它们共同的风格。\n数学一定要有直观的意义吗，我认为是的。虽然并不是所有的数学发现都是从实际出发而得来的，但是最终它们一定会用来解决实际的问题。如果没有了这种直观的想象力，那么仅仅从公式推理去做研究，就如同水木失去本源难以发展，又如空中楼阁难以稳固。","title":"Rope背后的数学想象力"},{"content":"前言 介绍了一下DeepSpeed的架构，以及部分重点内容的原理。\n其实是看DeepSpeed源码时候随便写的一段笔记，没时间整理并且写的很潦草，所以不太想发，但是框架的代码读起来不容易，里面知识点确实花了一些时间才弄明白。\n另外，也看到DeepSpeed框架在工作中使用越来越多，所以发出来给想要了解DeepSpeed原理的人一个参考，欢迎批评指正，献丑了。\n正文 ","permalink":"https://dawson-chen.github.io/posts/deepspeed-drafts/","summary":"前言 介绍了一下DeepSpeed的架构，以及部分重点内容的原理。\n其实是看DeepSpeed源码时候随便写的一段笔记，没时间整理并且写的很潦草，所以不太想发，但是框架的代码读起来不容易，里面知识点确实花了一些时间才弄明白。\n另外，也看到DeepSpeed框架在工作中使用越来越多，所以发出来给想要了解DeepSpeed原理的人一个参考，欢迎批评指正，献丑了。\n正文 ","title":"Deepspeed原理（手写笔记）"},{"content":"神经网络的参数是用浮点精度表示的， 浮点精度的标准是IEEE 754 - Wikipedia，以下是一个FP16数值在内存中存储格式。\n随着神经网络模型规模越来越大，如何减少模型占用的内存并且缩短训练时间成为亟需解决的问题，混合精度训练就是其中之一的解决方案，并且几乎不会影响模型训练的效果。\n混合精度原理 想象一下，如果模型参数+loss+gradient都是用fp16保存的，fp16的最小值是$6.1\\times 10^{-5}$，小于最小值的gradient都会变成0，相当于浪费了一次梯度传播。或许小的gradient并没有很重要，但是积累多次就会变得不可忽略。当前大模型普遍较低的学习率也会加剧这个问题的影响。\n因此为了解决这个问题，就需要用更高精度fp32保存一份参数，在正常前向推理和反向传播时都用fp16，计算好得梯度先转换为fp32，再乘以学习率，然后更新到fp32存储得参数上，最终将fp32参数转换成fp16更新模型参数。\n整个流程如下如：\n这种用fp16和fp32共同训练模型得技术就叫做混合精度训练(MP, Mixed-Precision training)，显然MP并不能节省模型加载需要的内存，因为需要多存储一份fp16的参数和梯度，但是用fp16进行模型前向和后向计算，能够减少中间计算值存储需要的内存，这部分内存会随着sequence length和batch size增大而增大，所以只有在这部分中间值占用内存比重较高时才能带来一定的内存节约。\n虽然计算时间的影响不大，但是fp16训练时间的确会大大减少，通常是减少1.5~5.5倍。\n更多资料：\nfastai - Mixed precision training\nUnderstanding Mixed Precision Training | by Jonathan Davis | Towards Data Science\nLoss Scale 是不是混合精度训练就完全没有梯度损失了呢，并不是，在反向传播过程中其实已经有部分梯度因为精度原因丢失了（因为正常模型梯度都不会太大，所以我们主要考虑下溢出）。那么如何解决这部分问题呢，就要用到Loss Scale。\n原理是将Loss乘以一个比较大的数scale，因为Loss是用fp32存储的，所以scale的选值范围是比较大的。这样因为反向传播链式法则原理，梯度也会放大很多倍，原本下溢出的值也会保存下来。然后在梯度转换成fp32后除以scale，最后更新就与正常混合精度训练一致了。\n流程如下：\n一般在开始训练时scale会设定成一个比较大的值，如果计算过程中fp16梯度发生上溢出，会跳过当前步的参数更新，并将scale下调。训练log中会输出如下消息：\n⚠️ Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to… ","permalink":"https://dawson-chen.github.io/posts/mixture-training/","summary":"神经网络的参数是用浮点精度表示的， 浮点精度的标准是IEEE 754 - Wikipedia，以下是一个FP16数值在内存中存储格式。\n随着神经网络模型规模越来越大，如何减少模型占用的内存并且缩短训练时间成为亟需解决的问题，混合精度训练就是其中之一的解决方案，并且几乎不会影响模型训练的效果。\n混合精度原理 想象一下，如果模型参数+loss+gradient都是用fp16保存的，fp16的最小值是$6.1\\times 10^{-5}$，小于最小值的gradient都会变成0，相当于浪费了一次梯度传播。或许小的gradient并没有很重要，但是积累多次就会变得不可忽略。当前大模型普遍较低的学习率也会加剧这个问题的影响。\n因此为了解决这个问题，就需要用更高精度fp32保存一份参数，在正常前向推理和反向传播时都用fp16，计算好得梯度先转换为fp32，再乘以学习率，然后更新到fp32存储得参数上，最终将fp32参数转换成fp16更新模型参数。\n整个流程如下如：\n这种用fp16和fp32共同训练模型得技术就叫做混合精度训练(MP, Mixed-Precision training)，显然MP并不能节省模型加载需要的内存，因为需要多存储一份fp16的参数和梯度，但是用fp16进行模型前向和后向计算，能够减少中间计算值存储需要的内存，这部分内存会随着sequence length和batch size增大而增大，所以只有在这部分中间值占用内存比重较高时才能带来一定的内存节约。\n虽然计算时间的影响不大，但是fp16训练时间的确会大大减少，通常是减少1.5~5.5倍。\n更多资料：\nfastai - Mixed precision training\nUnderstanding Mixed Precision Training | by Jonathan Davis | Towards Data Science\nLoss Scale 是不是混合精度训练就完全没有梯度损失了呢，并不是，在反向传播过程中其实已经有部分梯度因为精度原因丢失了（因为正常模型梯度都不会太大，所以我们主要考虑下溢出）。那么如何解决这部分问题呢，就要用到Loss Scale。\n原理是将Loss乘以一个比较大的数scale，因为Loss是用fp32存储的，所以scale的选值范围是比较大的。这样因为反向传播链式法则原理，梯度也会放大很多倍，原本下溢出的值也会保存下来。然后在梯度转换成fp32后除以scale，最后更新就与正常混合精度训练一致了。\n流程如下：\n一般在开始训练时scale会设定成一个比较大的值，如果计算过程中fp16梯度发生上溢出，会跳过当前步的参数更新，并将scale下调。训练log中会输出如下消息：\n⚠️ Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to… ","title":"混合精度训练"},{"content":"背景 让我们回顾以下过去的半个月里重要的AI发展。\n事件 时间 介绍 公司 Visual ChatGPT 3-12 可以通过文本和图片聊天，甚至修改图片内容。 Microsoft GPT4发布 3-13 更大的ChatGPT模型，部分专业能力达到人类水平，可以接收图片输入。 OpenAI 365 Copilot 3-16 智能办公大杀器。 Microsoft 文心一言 3-16 中国版的ChatGPT Baidu ChatGPT plugin 3-23 可以使用工具的ChatGPT OpenAI HuggingGPT 3-30 可以使用HuggingFace中模型能力的ChatGPT Microsoft 很多评价说过去的几周是AI发展的Crazy Week，这种速度疯狂到甚至让人们开始担心AI会影响到社会和人类，并在公开信中呼吁暂停AI的研究。造成这种现象的原因可以理解为，一是基于ChatGPT的成功，二是行业内大量的关注。\n个人认为，这其中ChatGPT plugin可以认为是对行业应用最有影响力的一个技术，也是继ChatGPT发布以来OpenAI发布的最重要的更新，可以简单的理解为OpenAI发布了对应ChatGPT的应用商店。对未来人工智能应用的形态也有一定启发，以前的AI模型的定位更多的是充当的一个单一的智能工具，具体到某个任务上，还需要人工协同才能完成；但是有了plugin这项技术，那么AI模型可以代替之前人工的部分，自主使用工具，从而端到端的完成某一项任务。这也是为什么一些基础的工作岗位很有可能会被新一代AI技术取代。\n在网上已经有很多对ChatGPT plugin如何使用的介绍，但是比较少有对其实现原理进行解析的内容。这篇文章里我们主要分析一下它的原理，以及可能造成的影响。\n必要性 首先说为什么语言模型要使用插件？随着语言模型的规模不断变大，各种涌现能力被相继发现，从而衍生出各种关于模型能力的研究。但谈到语言模型的应用，始终绕不开一个问题，就是模型无法获取外界的信息。也就是，一旦模型训练完成，后续的所有输出都来自于训练数据中学习到的知识。\n大语言模型存在的问题可以总结为以下2点：\n缺少最新数据的补充；\n在不同的应用场景，对数据的需求也是不同的。在开放问答领域，可以是维基百科一类的数据。在特定业务领域，可能是公司内部的一些私人数据集。\n缺少专业的能力；\n大型语言模型对通用逻辑的理解是比较好的，比方说写一篇文章，与人聊天。但是涉及到特殊的专业，比方说作数学题、求公式的解，这类型问题对模型来说是有点难的。\n虽然GPT4号称用了更大的模型，可以在一些专业领域得到类似于人类的效果甚至超越。但是从本质上来看，语言模型所采用的文字接龙训练方式，对于这类问题是非常不友好的。\n或许随着模型变大，训练时间更长可以得到更好的效果，但是花费巨大训练出的GPT3在计算能力上远远达不到1970年代出现的计算器，本身就可以说明大模型技术是不足以解决专业推理问题的。\n了解了以上模型存在的问题，就可以理解教模型使用插件的必要性了。PS：使用插件、使用工具，在不同的地方有不同的说法，但是是一件事情。\n模型使用工具技术发展 在GPT3发布以后，就有一些AI模型使用插件的技术研究陆续出现，甚至有一些开源的框架在github上收获不错的关注。\n想法的提出：MRKL System MRKL System（全名是Modular Reasoning, Knowledge and Language，论文链接，博客链接）由以色列的一家人工智能公司AI21推出，可以被认为是语言模型使用工具系统想法的提出者。虽然在此之前有WebGPT这类教模型使用浏览器的工作，但它是第一个提出将模型作为中枢，接入各种不同类型的插件来完成任务的工作。\n从工作流程上来看，MRKL已经完全接近于ChatGPT plugin。MRKL认为这是一种跨越神经学派和符号学派的架构（neuro-symbolic architecture），各种插件可以被认为是符号系统，由神经学派的语言模型进行统一调用。\n这篇论文中以使用计算器为例子，主要描述了如何将自然语言中的内容转换为API所需要的参数，文中提出语言模型few-shot在复杂的问题上性能有限，所以用Prompt tuning这种轻量化的微调技术提升转换的准确率。Prompt tuning技术是用特定训练好的非自然语言prompt来控制模型在特定任务中的生成表现，对应到MRKL中那就是每一个插件都需要训练一个特定的Prompt，虽然说有一定训练成本，但也算是一种比较好的解决思路。\n可是文中对于最重要的问题：”怎么决定调用插件？“，这块的细节并没有太多的描述，也引出了关于大模型推理技术的发展。\nReasoning技术：ReACT 为了教会模型实用工具，一种方法是首先让模型具备推理的能力，从而能够模拟人使用工具的过程。应该说语言模型的训练方式和推理是不沾边的，但是语言模型的美妙之处就在于，当模型大小足够大的时候，它会诞生出很多出乎意料的能力，比方说推理能力。\n大语言模型的推理能力通过Chain-of-thought体现出来，但是这种推理能力需要显式的Prompt进行引导。根据引导方式的不同产生出各种不同的技术，其本质上是对不同思维方式的模拟，这里我们只介绍比较典型的ReACT技术。\nReACT用强化学习的方式建模推理的过程，agent认为是一个可以使用各种工具的智能体，environment为所有可用插件构成的工具箱集合，action为可以使用的插件功能集合。而控制策略为语言模型中学习到的知识。一个典型的推理的流程如下图所示：\nReACT推理流程可以分为Thought→Action→Observation→Thought 这样的循环，具体如何实现在本文的后续内容中会进行分析。\n使用工具的语言模型：Toolformer 与利用推理能力使用工具的思路不同，Toolformer是在训练语言模型过程中，使模型学习在适当位置调用相关API，并用API结果辅助后续的文本生成。在Toolformer训练过程中，数据是Pittsburgh is also known as [QA(What …?→ Steel City)] the Steel City.这种格式，如果是人去标注数据，首先需要找到API的放置位置，判断标准是API结果对后续文本生成有帮助，并且上文中有API需要的参数；然后是将API的标识、输入、输出以[QA(What …?→ Steel City)]这种形式插入到训练文本中。\n注意，模型训练仍然采用典型的文字接龙方式，所以对原本语言模型的能力并没有损失。论文中提出一种利用LLM去自动标注这种数据的方式，和远程监督类似，步骤如下图：\n工具提出：LangChain LangChain差不多是在2022年底提出的，那时候也是LLMs技术急剧发展的阶段。其核心是做一个基于LLMs的工具，基本上所有需要用LLMs实现的功能都可以在里面找到对应的工具。其中一个主要的能力，就是教会模型使用工具，并且接入方式和扩展性都非常好。除此之外还有很多好用的工具，比如：Prompt管理、Memory。名字中的Chain表示其核心设计思路是将不同的模块链接在一起。\n详细的文档见链接。\nhttps://github.com/hwchase17/langchain\nLangChain中有很多有用的工具，包括各种搜索引擎Bing、Google、SerpAPI（google问答）、wiki等。还有一个更有趣的是Human as a tool插件，可以使语言模型必要时询问人类，从而模拟各种各样的功能。\n在原理部分我们会介绍它的工作流程。\nChatGPT plugin的原理 ChatGPT plugin是作为一个产品发布的，并且功能还没有完全开放，因此其实现原理细节也不是很清楚。但是由于LangChain中已经实现了类似的功能，并且2者的发布时间比较相近，所以有理由相信2者在原理上是有相似的。\n下面我们分2部分，首先分析LangChain中使用工具的原理；第二部分通过比较2者的区别，得出一些关于ChatGPT plugin原理的猜想。\nLangChain的工作流程 首先让我们看一个例子：\nfrom langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.llms import OpenAI llm = OpenAI(temperature=0) tools = load_tools([\u0026#34;serpapi\u0026#34;, \u0026#34;llm-math\u0026#34;], llm=llm) agent = initialize_agent(tools, llm, agent=\u0026#34;zero-shot-react-description\u0026#34;, verbose=True) response = agent.run(\u0026#34;Who is Leo DiCaprio\u0026#39;s girlfriend? What is her current age raised to the 0.43 power?\u0026#34;) 代码里涉及到的关键概念：\nllm: BaseLLM\nLangChain中对一系列开源模型接口的封装，其主要作用是统一不同的模型API，使接口更易使用，包括像提供缓存等一些基础功能。\ntools: List[BaseTool]\n对工具的封装，LangChain中对工具的封装是比较简单的，因此保证了比较高的自由度，唯一的要求是输入输出必须是文字形式，def run(self, tool_input: str) -\u0026gt; str。\n自定义Tool只需要3个参数:\nname：工具的标识名称； description: 工具的自然语言描述； func: 功能执行函数，输入输出都为单个的文本。 agent: Agent\n内部使用了一个LLM决定使用什么工具，LangChain中agent的实现有2种，一种是ReACT类型，一种是self-Ask类型，因为后者只能使用qa类型的工具，如果任务涉及不同类型的工具，最好用ReACT类型。其中比较常用的是zero-shot-react-description，其中zero-shot表示推理引导Prompt里不包括示例，description表示LLM在决定调用什么工具的信息都来自于工具的description字段。\n注意，针对特定的任务可以设计针对性的few-shot提升agent的效果。\n介绍完上面的概念，让我们看这个例子是怎么工作的。首先根据提供的工具，agent会生成引导Prompt，对于上面的例子，prompt是下面的样子：\n其中{input}为用户Query的占位符号，{agent_scratchpad}为模型生成填充的位置。下面说明一个循环Thought→Action→Observation→Thought的详细步骤：\n生成Thought，对应的Prompt（只显示Begin!之后的部分）：\nQuestion: Who is Leo DiCaprio\u0026rsquo;s girlfriend? What is her current age raised to the 0.43 power?\nThought:\nLLM输出：\nI need to find out who Leo DiCaprio\u0026rsquo;s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: \u0026ldquo;Leo DiCaprio girlfriend\u0026rdquo;\nObservation:\n其中Observation: 为语言模型生成的终止符。\n根据模型选择的Action，调用Search[ \u0026ldquo;Leo DiCaprio girlfriend\u0026rdquo;]得到结果：\nLeonardo DiCaprio has split from girlfriend Camila Morrone. Getty. The Titanic actor hasn\u0026rsquo;t been in a relationship with a woman over the age of \u0026hellip;\n第二次生成Thought，对应的Prompt如下：\nQuestion: Who is Leo DiCaprio\u0026rsquo;s girlfriend? What is her current age raised to the 0.43 power?Thought: I need to find out who Leo DiCaprio\u0026rsquo;s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: \u0026ldquo;Leo DiCaprio girlfriend\u0026rdquo; Observation: Leonardo DiCaprio has split from girlfriend Camila Morrone. Getty. The Titanic actor hasn\u0026rsquo;t been in a relationship with a woman over the age of \u0026hellip; Thought:\n继续这个循环直到输出最终结果，或者超过最大循环次数。\n最后，完整的推理过程如下：\nChatGPT plugin的原理猜想（未完） 根据OpenAI官方的介绍，ChatGPT plugin在设计上要比LangChain精细的多，主要体现：\n每个插件可以有多个API接口； 接口可以定义参数类型和格式； 描述的长度更大； 并且按照描述，对于自定义的新插件使用起来也是zero-shot的方式，所以其实现难度要更高。根据一些相关文献，可以猜想出以下可能的实现方式：\n待补充…\n讨论 应用场景 手机行业\n机器人行业\n对劳动力的影响 Zippia. \u0026ldquo;23+ Artificial Intelligence And Job Loss Statistics [2023]: How Job Automation Impacts the Workforce\u0026rdquo; Zippia.com. Feb. 7, 2023, https://www.zippia.com/advice/ai-job-loss-statistics/\n这段内容摘抄自上面这篇博客，作者是一家求职网站的创始人。文中用到的并非严格的统计方法，因此数字有夸大的嫌疑，这里过滤掉一些数字表示的结论，总结出一些未来可能的趋势，供大家参考。\n被AI技术淘汰掉旧的劳动力，不太可能找到更高薪的工作； 从长远来看，AI技术发展对全球经济是有促进作用的。但如果造成大规模的失业潮，就另当别论； 大多数公司会使用不同程度的AI技术来提升效率，也会是AI技术发展最直接的受益者； 当AI技术、机器人替代掉大多数的工作，很多人会没有工作，从而需要政府救济维持生活； 最有可能被取代的工作类型：客服、会计、前台接待、制造类、零售接待、数据分析 等； 最难被取代的工作类型：HR、作家、律师、管理者、科学家、人文类 等； 参考 Welcome to LangChain — 🦜🔗 LangChain 0.0.132 MRKL System 2205.00445.pdf (arxiv.org) REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS Toolformer：*https://arxiv.org/pdf/2302.04761.pdf* ReACT：https://arxiv.org/pdf/2210.03629.pdf YouTube@aiadvantage ","permalink":"https://dawson-chen.github.io/posts/chatgpt-plugin/","summary":"背景 让我们回顾以下过去的半个月里重要的AI发展。\n事件 时间 介绍 公司 Visual ChatGPT 3-12 可以通过文本和图片聊天，甚至修改图片内容。 Microsoft GPT4发布 3-13 更大的ChatGPT模型，部分专业能力达到人类水平，可以接收图片输入。 OpenAI 365 Copilot 3-16 智能办公大杀器。 Microsoft 文心一言 3-16 中国版的ChatGPT Baidu ChatGPT plugin 3-23 可以使用工具的ChatGPT OpenAI HuggingGPT 3-30 可以使用HuggingFace中模型能力的ChatGPT Microsoft 很多评价说过去的几周是AI发展的Crazy Week，这种速度疯狂到甚至让人们开始担心AI会影响到社会和人类，并在公开信中呼吁暂停AI的研究。造成这种现象的原因可以理解为，一是基于ChatGPT的成功，二是行业内大量的关注。\n个人认为，这其中ChatGPT plugin可以认为是对行业应用最有影响力的一个技术，也是继ChatGPT发布以来OpenAI发布的最重要的更新，可以简单的理解为OpenAI发布了对应ChatGPT的应用商店。对未来人工智能应用的形态也有一定启发，以前的AI模型的定位更多的是充当的一个单一的智能工具，具体到某个任务上，还需要人工协同才能完成；但是有了plugin这项技术，那么AI模型可以代替之前人工的部分，自主使用工具，从而端到端的完成某一项任务。这也是为什么一些基础的工作岗位很有可能会被新一代AI技术取代。\n在网上已经有很多对ChatGPT plugin如何使用的介绍，但是比较少有对其实现原理进行解析的内容。这篇文章里我们主要分析一下它的原理，以及可能造成的影响。\n必要性 首先说为什么语言模型要使用插件？随着语言模型的规模不断变大，各种涌现能力被相继发现，从而衍生出各种关于模型能力的研究。但谈到语言模型的应用，始终绕不开一个问题，就是模型无法获取外界的信息。也就是，一旦模型训练完成，后续的所有输出都来自于训练数据中学习到的知识。\n大语言模型存在的问题可以总结为以下2点：\n缺少最新数据的补充；\n在不同的应用场景，对数据的需求也是不同的。在开放问答领域，可以是维基百科一类的数据。在特定业务领域，可能是公司内部的一些私人数据集。\n缺少专业的能力；\n大型语言模型对通用逻辑的理解是比较好的，比方说写一篇文章，与人聊天。但是涉及到特殊的专业，比方说作数学题、求公式的解，这类型问题对模型来说是有点难的。\n虽然GPT4号称用了更大的模型，可以在一些专业领域得到类似于人类的效果甚至超越。但是从本质上来看，语言模型所采用的文字接龙训练方式，对于这类问题是非常不友好的。\n或许随着模型变大，训练时间更长可以得到更好的效果，但是花费巨大训练出的GPT3在计算能力上远远达不到1970年代出现的计算器，本身就可以说明大模型技术是不足以解决专业推理问题的。\n了解了以上模型存在的问题，就可以理解教模型使用插件的必要性了。PS：使用插件、使用工具，在不同的地方有不同的说法，但是是一件事情。\n模型使用工具技术发展 在GPT3发布以后，就有一些AI模型使用插件的技术研究陆续出现，甚至有一些开源的框架在github上收获不错的关注。\n想法的提出：MRKL System MRKL System（全名是Modular Reasoning, Knowledge and Language，论文链接，博客链接）由以色列的一家人工智能公司AI21推出，可以被认为是语言模型使用工具系统想法的提出者。虽然在此之前有WebGPT这类教模型使用浏览器的工作，但它是第一个提出将模型作为中枢，接入各种不同类型的插件来完成任务的工作。\n从工作流程上来看，MRKL已经完全接近于ChatGPT plugin。MRKL认为这是一种跨越神经学派和符号学派的架构（neuro-symbolic architecture），各种插件可以被认为是符号系统，由神经学派的语言模型进行统一调用。\n这篇论文中以使用计算器为例子，主要描述了如何将自然语言中的内容转换为API所需要的参数，文中提出语言模型few-shot在复杂的问题上性能有限，所以用Prompt tuning这种轻量化的微调技术提升转换的准确率。Prompt tuning技术是用特定训练好的非自然语言prompt来控制模型在特定任务中的生成表现，对应到MRKL中那就是每一个插件都需要训练一个特定的Prompt，虽然说有一定训练成本，但也算是一种比较好的解决思路。\n可是文中对于最重要的问题：”怎么决定调用插件？“，这块的细节并没有太多的描述，也引出了关于大模型推理技术的发展。\nReasoning技术：ReACT 为了教会模型实用工具，一种方法是首先让模型具备推理的能力，从而能够模拟人使用工具的过程。应该说语言模型的训练方式和推理是不沾边的，但是语言模型的美妙之处就在于，当模型大小足够大的时候，它会诞生出很多出乎意料的能力，比方说推理能力。\n大语言模型的推理能力通过Chain-of-thought体现出来，但是这种推理能力需要显式的Prompt进行引导。根据引导方式的不同产生出各种不同的技术，其本质上是对不同思维方式的模拟，这里我们只介绍比较典型的ReACT技术。\nReACT用强化学习的方式建模推理的过程，agent认为是一个可以使用各种工具的智能体，environment为所有可用插件构成的工具箱集合，action为可以使用的插件功能集合。而控制策略为语言模型中学习到的知识。一个典型的推理的流程如下图所示：","title":"ChatGPT Plugins原理介绍和讨论"}]